{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment1",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRJJ4tzaC852"
      },
      "source": [
        "# Preliminary Steps\n",
        "These are some preliminary steps before addressing the task. Import some basic libraries and set a variable that will be used in multiple steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCTmlk1C0Sm"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# typing\n",
        "from typing import List, Callable, Dict"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vspH9UzptOFT"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPThN5GWAZl"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsjdOVHZtSnp"
      },
      "source": [
        "## Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR9RtRITt72b"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJbpdftCt8Gl"
      },
      "source": [
        "## Train, validation and test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXgbC4TAuJ81"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Co7hfK2uKD8"
      },
      "source": [
        "# GloVe "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tRukt6Kxx4R"
      },
      "source": [
        "## OOV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-uwA9DYuNtr"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MypraiXCuOIb"
      },
      "source": [
        "# Models\n",
        "This section is used for creating different models, going from a baseline to slightly more complicated ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7EPmCLyuZVB"
      },
      "source": [
        "## Constants and utilities\n",
        "First of all, define some constants, parameter dictionaries and methods that will be reused by each architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__iSg0qcABw"
      },
      "source": [
        "# TODO: all the following constants are temporary \n",
        "N_CLASSES = 20  # this must be equal to the number of tags\n",
        "VOCABULARY_SIZE = 1000  # this must be obtained from the dataset\n",
        "EMBEDDING_SIZE = 64  # hyper-parameter to properly set\n",
        "MAX_SEQUENCE_SIZE = 100  # this must be obtained from the dataset\n",
        "\n",
        "BATCH_SIZE = 128  # hyper-parameter to properly set\n",
        "EPOCHS = 5\n",
        "\n",
        "\n",
        "# Model common compile information\n",
        "# Use sparse_categorical_crossentropy because labels are one hot encoded\n",
        "model_compile_info = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': 'sparse_categorical_crossentropy',\n",
        "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
        "}\n",
        "\n",
        "# Model common training information\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience=10,\n",
        "                                                restore_best_weights=True)]\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq-mTGQRL-uo"
      },
      "source": [
        "# This tensor should contain the weights obtained by GloVe\n",
        "embedding_weights = np.zeros(shape=(VOCABULARY_SIZE, EMBEDDING_SIZE))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stq44tS8CJSy"
      },
      "source": [
        "Define utility methods that will be used to **create**, **train** and **test** the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE7Mvm9sZ8Pf"
      },
      "source": [
        "def create_model(name,\n",
        "                 layers, \n",
        "                 compile_info, \n",
        "                 show_summary=True) -> keras.Model:\n",
        "    \"\"\"\n",
        "    Create the model using the layers passed as parameters.\n",
        "    After the creation, the model is compiled and its summary is possibly \n",
        "    printed to console.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers : array\n",
        "        Array that contains a list of layers that must be added \n",
        "        to the model.\n",
        "    compile_info: Dictionary\n",
        "        Contains information required for compiling the model.\n",
        "    show_summary: bool\n",
        "        If true, then the summary of the model will be printed to console\n",
        "    \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : keras.Model\n",
        "        The keras model.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential(name=name)\n",
        "    \n",
        "    for idx, layer in enumerate(layers):\n",
        "\n",
        "        # Sanity checks for being sure that the last layer has been \n",
        "        # correctly set\n",
        "        if idx == len(layers) - 1:\n",
        "            assert layer.activation == keras.activations.softmax, 'Wrong activation function'\n",
        "            assert layer.units == N_CLASSES, 'Wrong number of units'\n",
        "\n",
        "        model.add(layer)\n",
        "\n",
        "    # Compile\n",
        "    model.compile(**compile_info)\n",
        "\n",
        "    # Print model summary\n",
        "    if show_summary:\n",
        "        model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(model: keras.Model,\n",
        "                x_train: np.ndarray,\n",
        "                y_train: np.ndarray,\n",
        "                x_val: np.ndarray,\n",
        "                y_val: np.ndarray,\n",
        "                training_info: dict):\n",
        "    \"\"\"\n",
        "    Training routine for the Keras model.\n",
        "    At the end of the training, retrieved History data is shown.\n",
        "\n",
        "    :param model: Keras built model\n",
        "    :param x_train: training data in np.ndarray format\n",
        "    :param y_train: training labels in np.ndarray format\n",
        "    :param x_val: validation data in np.ndarray format\n",
        "    :param y_val: validation labels in np.ndarray format\n",
        "    :param training_info: dictionary storing model fit() argument information\n",
        "\n",
        "    :return\n",
        "        model: trained Keras model\n",
        "    \"\"\"\n",
        "    print(\"Start training! \\nParameters: {}\".format(training_info))\n",
        "    history = model.fit(x=x_train, y=y_train,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        **training_info)\n",
        "    print(\"Training completed! Showing history...\")\n",
        "\n",
        "    show_history(history)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_data(model: keras.Model,\n",
        "                 x: np.ndarray,\n",
        "                 prediction_info: dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Inference routine of a given input set of examples\n",
        "\n",
        "    :param model: Keras built and possibly trained model\n",
        "    :param x: input set of examples in np.ndarray format\n",
        "    :param prediction_info: dictionary storing model predict() argument information\n",
        "\n",
        "    :return\n",
        "        predictions: predicted labels in np.ndarray format\n",
        "    \"\"\"\n",
        "\n",
        "    print('Starting prediction: \\n{}'.format(prediction_info))\n",
        "    print('Predicting on {} samples'.format(x.shape[0]))\n",
        "\n",
        "    predictions = model.predict(x, **prediction_info)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def evaluate_predictions(predictions: np.ndarray,\n",
        "                         y: np.ndarray,\n",
        "                         metrics: List[Callable],\n",
        "                         metric_names: List[str]):\n",
        "    \"\"\"\n",
        "    Evaluates given model predictions on a list of metric functions\n",
        "\n",
        "    :param predictions: model predictions in np.ndarray format\n",
        "    :param y: ground-truth labels in np.ndarray format\n",
        "    :param metrics: list of metric functions\n",
        "    :param metric_names: list of metric names\n",
        "\n",
        "    :return\n",
        "        metric_info: dictionary containing metric values for each input metric\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(metrics) == len(metric_names)\n",
        "\n",
        "    print(\"Evaluating predictions! Total samples: \", y.shape[0])\n",
        "\n",
        "    metric_info = {}\n",
        "\n",
        "    for metric, metric_name in zip(metrics, metric_names):\n",
        "        metric_value = metric(y_pred=predictions, y_true=y)\n",
        "        metric_info[metric_name] = metric_value\n",
        "\n",
        "    return metric_info\n",
        "\n",
        "def model_sanity_check(model: keras.Model):\n",
        "    \"\"\"\n",
        "    Create a random input_tensor and try to pass through the model.\n",
        "    This method should be used in order to check if the model is \n",
        "    working as expected.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : keras.Model\n",
        "        The model that must be tried.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f'Sanity check for the model with name: {model.name}')\n",
        "    # Model sanity check for seeing if it runs correctly\n",
        "    input_tensor = np.random.uniform(size=(BATCH_SIZE, MAX_SEQUENCE_SIZE))\n",
        "    print(f'Input tensor shape: {input_tensor.shape}')\n",
        "    output_tensor = model(input_tensor)\n",
        "    print(f'Output tensor shape: {output_tensor.shape}')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPDiCsbl9ea"
      },
      "source": [
        "Define utility methods for **creating layers** in order to: \n",
        "* reduce the code verbosity.\n",
        "* be sure to always create different architectures with the same layer structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rCmJJgpmBRY"
      },
      "source": [
        "# EMBEDDING\n",
        "def embedding_layer(embedding_weights: np.array) -> layers.Embedding:\n",
        "    \"\"\"\n",
        "    Create an embedding layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embedding_weights : np.array\n",
        "        The weights for the embedding layer.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Embedding\n",
        "        The created embedding layer.\n",
        "    \"\"\"\n",
        "    layer = layers.Embedding(\n",
        "        input_dim=VOCABULARY_SIZE, \n",
        "        output_dim=EMBEDDING_SIZE, \n",
        "        input_length=MAX_SEQUENCE_SIZE,\n",
        "        weights=[embedding_weights],\n",
        "        mask_zero=True\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# RNN (LSTM and GRU)\n",
        "def _rnn_size(layer_depth: int) -> int:\n",
        "    \"\"\"\n",
        "    Simple logic used for assigning the number of units \n",
        "    to the rnn layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    size : int\n",
        "        The number units.\n",
        "    \"\"\"\n",
        "    size = 64\n",
        "    if layer_depth > 1:\n",
        "        size = 128\n",
        "    return size\n",
        "\n",
        "def bilstm_layer(layer_depth: int) -> layers.Bidirectional:\n",
        "    \"\"\"\n",
        "    Create a bidirectional lstm layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Bidirectional\n",
        "        The created bidirectional lstm layer.\n",
        "    \"\"\"\n",
        "    size = _rnn_size(layer_depth)\n",
        "    layer = layers.Bidirectional(\n",
        "        layers.LSTM(size, return_sequences=True, activation='relu')\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "def bigru_layer(layer_depth: int) -> layers.Bidirectional:\n",
        "    \"\"\"\n",
        "    Create a bidirectional gru layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Bidirectional\n",
        "        The created bidirectional gru layer.\n",
        "    \"\"\"\n",
        "    size = _rnn_size(layer_depth)\n",
        "    layer = layers.Bidirectional(\n",
        "        layers.GRU(size, return_sequences=True, activation='relu')\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# DENSE\n",
        "def _dense_size(last_layer:bool) -> int:\n",
        "    \"\"\"\n",
        "    Simple logic for assigning the size of the dense layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    size : int\n",
        "        The size of the dense layer.\n",
        "    \"\"\"\n",
        "    size = N_CLASSES\n",
        "    if not last_layer:\n",
        "        size = 256\n",
        "    return size\n",
        "\n",
        "def _dense_activation(last_layer:bool) -> str:\n",
        "    \"\"\"\n",
        "    Simple logic for assigning the activation function of the dense layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    activation : str\n",
        "        The activation function of the layer.\n",
        "    \"\"\"\n",
        "    activation = 'relu'\n",
        "    if last_layer:\n",
        "        activation = 'softmax'\n",
        "    return activation\n",
        "\n",
        "def dense_layer(last_layer:bool) -> layers.Dense:\n",
        "    \"\"\"\n",
        "    Create a dense layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Dense\n",
        "        The created dense layer.\n",
        "    \"\"\"\n",
        "    size = _dense_size(last_layer)\n",
        "    activation = _dense_activation(last_layer)\n",
        "    \n",
        "    return layers.Dense(size, activation=activation)\n",
        "\n",
        "# MODEL SANITY CHECK\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOhK3xp2FCUu"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoxVUVMap5Ma",
        "outputId": "800a4186-5984-4ef1-ad36-7233421b5d90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create layers\n",
        "baseline_layers = [\n",
        "                embedding_layer(embedding_weights=embedding_weights),\n",
        "                bilstm_layer(layer_depth=1),\n",
        "                dense_layer(last_layer=True)\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_model = create_model('baseline', \n",
        "                              baseline_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 100, 64)           64000     \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 100, 128)         66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100, 20)           2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 132,628\n",
            "Trainable params: 132,628\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline\n",
            "Input tensor shape: (128, 100)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LGdswmdFg6x"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4l5n888vLql"
      },
      "source": [
        "## Variations\n",
        "What follows is the implementation of small variations to the baseline architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMvk3AQFukOh"
      },
      "source": [
        "### GRU\n",
        "Change the LSTM layer with the GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "856hGaR9eDRb",
        "outputId": "cf4f158f-7697-453b-e0a6-34b01af3696f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create layers\n",
        "baseline_var1_layers = [\n",
        "                embedding_layer(embedding_weights=embedding_weights),\n",
        "                bigru_layer(layer_depth=1),\n",
        "                dense_layer(last_layer=True)\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var1_model = create_model('baseline_var1', \n",
        "                              baseline_var1_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var1_model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline_var1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 100, 64)           64000     \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 100, 128)         49920     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 100, 20)           2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 116,500\n",
            "Trainable params: 116,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline_var1\n",
            "Input tensor shape: (128, 100)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9We6j_USupzN"
      },
      "source": [
        "### Additional LSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLHH94CtqZ7c",
        "outputId": "a0dd637c-5154-4424-e9b7-dafe8724be92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create layers\n",
        "baseline_var2_layers = [\n",
        "                embedding_layer(embedding_weights=embedding_weights),\n",
        "                bilstm_layer(layer_depth=1),\n",
        "                bilstm_layer(layer_depth=2),\n",
        "                dense_layer(last_layer=True)\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var2_model = create_model('baseline_var2', \n",
        "                              baseline_var2_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var2_model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline_var2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 100, 64)           64000     \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirecti  (None, 100, 128)         66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirecti  (None, 100, 256)         263168    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100, 20)           5140      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 398,356\n",
            "Trainable params: 398,356\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline_var2\n",
            "Input tensor shape: (128, 100)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RN6ySWyu13J"
      },
      "source": [
        "### Additional Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujfM5mekqma8",
        "outputId": "bc0c6723-1eeb-4972-c803-8a08d68c9c0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create layers\n",
        "baseline_var3_layers = [\n",
        "                embedding_layer(embedding_weights=embedding_weights),\n",
        "                bilstm_layer(layer_depth=1),\n",
        "                dense_layer(last_layer=False),\n",
        "                dense_layer(last_layer=True)\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var3_model = create_model('baseline_var3', \n",
        "                              baseline_var3_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var3_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline_var3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 100, 64)           64000     \n",
            "                                                                 \n",
            " bidirectional_12 (Bidirecti  (None, 100, 128)         66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 100, 256)          33024     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 100, 20)           5140      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 168,212\n",
            "Trainable params: 168,212\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline_var3\n",
            "Input tensor shape: (128, 100)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwOShFdwvaHN"
      },
      "source": [
        "# Training and Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1SYhjS1vvzF"
      },
      "source": [
        "# Disussion and Error Analysis"
      ]
    }
  ]
}
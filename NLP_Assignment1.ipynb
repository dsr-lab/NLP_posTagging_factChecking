{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di NLP_Assignment1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRJJ4tzaC852"
      },
      "source": [
        "# Preliminary Steps\n",
        "These are some preliminary steps before addressing the task. Import some basic libraries and set a variable that will be used in multiple steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCTmlk1C0Sm"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# typing\n",
        "from typing import List, Callable, Dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vspH9UzptOFT"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPThN5GWAZl"
      },
      "source": [
        "dataset_name = \"dependency_treebank\"\n",
        "dataset_folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\")\n",
        "\n",
        "#Create directory to download dataset\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsjdOVHZtSnp"
      },
      "source": [
        "## Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR9RtRITt72b"
      },
      "source": [
        "#URL to the dataset\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
        "\n",
        "#Download the dataset\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByKCopDN_Rb-"
      },
      "source": [
        "## Dataset Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz8xnlmy_XvE"
      },
      "source": [
        "#Extract the dataset\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "\n",
        "print(\"Successful extraction\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJbpdftCt8Gl"
      },
      "source": [
        "## Cleaning of text and dataframe creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXgbC4TAuJ81"
      },
      "source": [
        "#The symbols allowed in the tokens\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "def prepreocess(token : str) -> str:\n",
        "\t\"\"\"\n",
        "\tCalls the function that cleans the text\n",
        "\tInput: the string to process\n",
        "\tOutput: the processed string\n",
        "\t\"\"\"\n",
        "\ttoken = text_to_lower(token)\n",
        "\ttoken = filter_out_uncommon_symbols(token)\n",
        "\ttoken = strip_text(token)\n",
        "\treturn token\n",
        "\n",
        "def text_to_lower(text: str) -> str:\n",
        "\t\"\"\"\n",
        "\tReturns the string in lower character\n",
        "\tInput: the string to process\n",
        "\tOutput: the processed string\n",
        "\t\"\"\"\n",
        "\treturn text.lower();\n",
        "\n",
        "def filter_out_uncommon_symbols(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "dataframe_rows = []\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Original\", dataset_name)\n",
        "#Iterate along the files\n",
        "for filename in os.listdir(folder):\n",
        "\tfile_path = os.path.join(folder, filename)\n",
        "\tprint(filename)\n",
        "\t#Ignore useless files\n",
        "\tif filename == \".DS_Store\":\n",
        "\t\tcontinue\n",
        "\ttry:\n",
        "\t\tif os.path.isfile(file_path):\n",
        "\t\t\t# open the file\n",
        "\t\t\twith open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "\t\t\t\t#Fetch the filenumber from the filename\n",
        "\t\t\t\tnumber_of_file = int(filename.split(\"_\")[1].split(\".\")[0])\n",
        "\t\t\t\t#Split in different groups\n",
        "\t\t\t\tif number_of_file < 101:\n",
        "\t\t\t\t\tsplit = \"train\"\n",
        "\t\t\t\telif number_of_file < 151:\n",
        "\t\t\t\t\tsplit = \"validation\"\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tsplit = \"test\"\n",
        "\t\t\t\t#Stop at end of file\n",
        "\t\t\t\tfor line in text_file:\n",
        "\t\t\t\t\tif line == \"\\n\":\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\tcolumns = line.split()\n",
        "\t\t\t\t\ttoken = columns[0]\n",
        "\t\t\t\t\ttoken = prepreocess(token)\n",
        "\t\t\t\t\tlabel = columns[1]\n",
        "\t\t\t\t\tdataframe_row = {\"split\": split, \"token\": token, \"label\": label}\n",
        "\t\t\t\t\tdataframe_rows.append(dataframe_row)\n",
        "\texcept Exception as e:\n",
        "                print('Failed to process %s. Reason: %s' % (file_path, e))\n",
        "                sys.exit(0)\n",
        "#Create processed dataframe folder\n",
        "folder = os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", dataset_name)\n",
        "if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "# transform the list of rows in a proper dataframe\n",
        "dataframe = pd.DataFrame(dataframe_rows)\n",
        "dataframe = dataframe[[\"split\", \"token\", \"label\"]]\n",
        "dataframe_path = os.path.join(folder, dataset_name + \".pkl\")\n",
        "dataframe.to_pickle(dataframe_path)\n",
        "\n",
        "dataframe_path = os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", dataset_name, dataset_name + \".pkl\")\n",
        "df = pd.read_pickle(dataframe_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Co7hfK2uKD8"
      },
      "source": [
        "# GloVe\n",
        "This section is the one responsible for the implementation of the GloVe embedding system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8qLxw1kcTto"
      },
      "source": [
        "## Constants and utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqG3dXiBcdeA"
      },
      "source": [
        "#Requested variables from the pre-processing methods:\n",
        "VOCAB_SIZE = 1000 #Temporary, it should be defined above\n",
        "words_to_tokens = {} #Dictionary that associates each word from the training+validation+test set with a token\n",
        "training_word = [] #List of unique words from the training set (or a method to get it)\n",
        "validation_words = [] #List of unique words from the validation set (or a method to get it)\n",
        "test_words = [] #List of unique words from the test set (or a method to get it)\n",
        "training_sentences = [] #List of sentences/documents from the training set (or a method to get it)\n",
        "validation_sentences = [] #List of sentences/documents from the validation set (or a method to get it)\n",
        "test_sentences = [] #List of sentences/documents from the test set (or a method to get it)\n",
        "#NB: a sentence or a document should be a List of words\n",
        "#---------------------------------------------------\n",
        "\n",
        "URL_BASE = \"https://nlp.stanford.edu/data\" #Location of the pre-trained GloVe's files\n",
        "GLOVE_VERSION = \"6B\"\n",
        "\n",
        "EMBEDDING_SIZE = 50 #The dimensionality of the embeddings; to be tested\n",
        "\n",
        "#List of paths to download and extract GloVe's files\n",
        "PATHS = {\n",
        "    \"url\": URL_BASE + \"/glove.\" + GLOVE_VERSION + \".zip\",\n",
        "    \"glove_path\": os.path.join(os.getcwd(),\"Glove\",GLOVE_VERSION),\n",
        "    \"glove_zip\": os.path.join(os.getcwd(),\"Glove\", GLOVE_VERSION, \"glove.\"+GLOVE_VERSION+\".zip\"),\n",
        "    \"glove_file\": os.path.join(os.getcwd(),\"Glove\", GLOVE_VERSION, \"glove.\"+GLOVE_VERSION+\".\"+str(EMBEDDING_SIZE)+\"d.txt\")\n",
        "}\n",
        "\n",
        "OOV_METHOD = \"Mean\" #Determine which OOV method to adopt; choose one between \"Mean\", \"Random\" and \"Placeholder\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIY-9Qf_bOU5"
      },
      "source": [
        "## Download\n",
        "In this part the presence of the GloVe file is checked. In case of a negative response, it will be downloaded and extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r14RFpFCbFn2"
      },
      "source": [
        "def setup_files():\n",
        "\n",
        "  '''\n",
        "  Create the folder if it does not exist.\n",
        "  Then download the zip file from the web archive if it does not exist.\n",
        "  Finally exctract the zip file of the GloVe txt file does not exist in the folder.\n",
        "  '''\n",
        "\n",
        "  if not os.path.exists(PATHS[\"glove_path\"]):\n",
        "    os.makedirs(PATHS[\"glove_path\"])\n",
        "\n",
        "  if not os.path.exists(PATHS[\"glove_file\"]):\n",
        "    if not os.path.exists(PATHS[\"glove_zip\"]):\n",
        "      download_glove(PATHS[\"url\"])\n",
        "\n",
        "    extract_glove(PATHS[\"glove_zip\"],PATHS[\"glove_path\"])\n",
        "\n",
        "def download_glove(url: str):\n",
        "\n",
        "    '''\n",
        "    Download GloVe's zip file from the web.\n",
        "    '''\n",
        "\n",
        "    urllib.request.urlretrieve(url, PATHS['glove_zip'])\n",
        "    print(\"Successful download\")\n",
        "\n",
        "def extract_glove(zip_file: str,\n",
        "                  glove_path: str):\n",
        "  \n",
        "    '''\n",
        "    Extract GloVe's zip file.\n",
        "    '''\n",
        "  \n",
        "    with zipfile.ZipFile(PATHS[\"glove_zip\"], 'r') as zip_ref:\n",
        "      zip_ref.extractall(path=PATHS[\"glove_path\"])\n",
        "      print(\"Successful extraction\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYhDkgNQejX3"
      },
      "source": [
        "## Initialization\n",
        "In this step, the downloaded GloVe file is loaded into an embedding vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfLDOt7le78v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de1b075-78e2-4a52-e85e-bdefd09cd7a7"
      },
      "source": [
        "def load_model(glove_file: str) ->Dict:\n",
        "\n",
        "  '''\n",
        "  Open GloVe's txt file and store each of its contained words\n",
        "  into a dictionary along with their correspondent embedding weights.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  glove_file : str\n",
        "      GloVe's txt file path.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  vocabulary: Dict\n",
        "      GloVe's vocabulary\n",
        "\n",
        "  '''\n",
        "\n",
        "  print(\"Loading GloVe Model...\")\n",
        "\n",
        "  with open(glove_file, encoding=\"utf8\" ) as f: #Open the txt file\n",
        "      lines = f.readlines() #Read the file line by line\n",
        "\n",
        "  vocabulary = {}\n",
        "  for line in lines:\n",
        "      splits = line.split()\n",
        "      #Save the first part of the line (word) as the dictionary's key and the second part (the embedding) as the key\n",
        "      vocabulary[splits[0]] = np.array([float(val) for val in splits[1:]])\n",
        "\n",
        "  print(\"GloVe model loaded\")\n",
        "\n",
        "  return vocabulary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe Model...\n",
            "GloVe model loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tRukt6Kxx4R"
      },
      "source": [
        "## OOV\n",
        "In this section, some possible \"Out Of Vocabulary\" handling methods are implemented, along with other OOV-related functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-uwA9DYuNtr"
      },
      "source": [
        "#OOV-handling: possible methods\n",
        "def random_embedding(embedding_size: int) ->np.array:\n",
        "  '''\n",
        "  Return a numpy array with random values sampled from a uniform distribution\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  embedding_size: int\n",
        "    The embedding size that is used as the size of the numpy array.\n",
        "\n",
        "  Results:\n",
        "  -------\n",
        "  np.array\n",
        "  A randomized numpy array.\n",
        "  '''\n",
        "\n",
        "  return np.random.uniform(low=-0.05, high=0.05, size=embedding_size)\n",
        "\n",
        "def placeholder_embedding(embedding_size: int) ->np.ndarray:\n",
        "  '''\n",
        "    Return a numpy array with all zeros\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    embedding_size: int\n",
        "      The embedding size that is used as the size of the numpy array.\n",
        "\n",
        "    Results:\n",
        "    -------\n",
        "    np.array\n",
        "    A numpy array filled with zeros.\n",
        "    '''\n",
        "\n",
        "  return np.zeros(shape=(embedding_size))\n",
        "\n",
        "def neighbours_mean_embedding(word: str,\n",
        "                              glove_embedding: Dict[str,int],\n",
        "                              sentences: List[List[str]]):\n",
        "  \n",
        "  '''\n",
        "  Compute the embedding of an OOV word by taking the mean\n",
        "  of its neighbours.\n",
        "\n",
        "  Parameters:\n",
        "  ---------\n",
        "  word: str\n",
        "      The OOV that needs to be embedded.\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding.\n",
        "  sentences: List[List[str]]\n",
        "      A list of all the sentences (lists of words) in the current set.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  mean: int\n",
        "      The mean of the embedding values of OOV-word's neighbours.\n",
        "  '''\n",
        "  \n",
        "  neighbours = {}\n",
        "\n",
        "  for sentence in sentences:\n",
        "    if word in sentence:\n",
        "      index = sentence.index(word)\n",
        "      if index!=0:\n",
        "        left_neighbour = sentence[index-1]\n",
        "        if left_neighbour in glove_embedding: #Consider only words that are not OOV\n",
        "          neighbours.add(left_neighbour)\n",
        "      if index!=len(sentence-1):\n",
        "        right_neighbour = sentence[index+1]\n",
        "        if right_neighbour in glove_embedding: #Consider only words that are not OOV\n",
        "          neighbours.add(right_neighbour)\n",
        "\n",
        "  neighbours_embeddings = np.array([glove_embedding[neighbour] for neighbour in neighbours])\n",
        "  return np.mean(neighbours_embeddings)\n",
        "\n",
        "\n",
        "#Others\n",
        "def get_oov_list(words: List[str],\n",
        "                 glove_embedding: Dict[str, int]) ->List[str]:\n",
        "\n",
        "  '''\n",
        "  Return a list of all the words that are not part of the GloVe embedding\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  words: List[str]\n",
        "      A list of unique words from a set of documents.\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  oov: List[str]\n",
        "      A list of all the OOV terms.\n",
        "  '''\n",
        "\n",
        "  embedding_vocabulary = set(glove_embedding.keys())\n",
        "  oov = set(words).difference(embedding_vocabulary)\n",
        "  return list(oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emKY7vJ-z6Od"
      },
      "source": [
        "##Embedding matrix\n",
        "Now, having opted for an OOV method, it is possible to create the embedding matrix, which associates the embedding to the correspondent word for the entire vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m33pLFGG0ysd"
      },
      "source": [
        "def update_embeddings(glove_embedding: Dict[str, int],\n",
        "                     new_embeddings: Dict[str, int]):\n",
        "  \n",
        "  '''\n",
        "  Update the GloVe's embeddings by adding the new embeddings of\n",
        "  the previous OOV words.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding.\n",
        "  new_embeddings: Dict[str, int]\n",
        "      A dictionary containing the new embeddings\n",
        "      for the analyzed OOV words.\n",
        "  '''\n",
        "  \n",
        "  #Merge GloVe's embeddings with the new discoveries\n",
        "  glove_embedding = glove_embedding | new_embeddings\n",
        "\n",
        "def update_embedding_matrix(starting_embedding_matrix: np.ndarray,\n",
        "                            glove_embedding: Dict[str, int],\n",
        "                            embedding_size: int,\n",
        "                            words: List[str],\n",
        "                            words_to_tokens: Dict[str,int]\n",
        "                            oov_method: str,\n",
        "                            sentences: List[List[str]]) ->np.ndarray\n",
        "\n",
        "  '''\n",
        "  Build an embedding matrix updating the values of a previous matrix based on\n",
        "  a new set of sentences and an updated GloVe embedding.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  starting_embedding_matrix: np.ndarray\n",
        "      The starting embedding matrix.\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding, eventually updated in the previous step with the\n",
        "      previous OOV terms.\n",
        "  embedding_size: int\n",
        "      The dimensions of the embedding.\n",
        "  words: List[str]\n",
        "      A list of unique words in the set of sentences.\n",
        "  words_to_tokens: Dict[str, int]\n",
        "      A dictionary that associates each word with a token.\n",
        "  oov_method: str\n",
        "      The name of the OOV method to use to handle OOV cases.\n",
        "  sentences: List[List[str]]\n",
        "      A list of all the sentences (lists of words) in the current set.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  embedding_matrix: np.ndarray\n",
        "      The updated embedded matrix that associates each word of the vocabulary\n",
        "      with its corresponding embedding.\n",
        "  '''\n",
        "\n",
        "  oov_terms = get_oov_list(words,glove_embedding)\n",
        "  #A copy of the original matrix is returned\n",
        "  embedding_matrix = np.copy(starting_embedding_matrix)\n",
        "  discovered_embeddings = {}\n",
        "\n",
        "  for word in tqdm(words):\n",
        "\n",
        "    token = words_to_tokens[word]\n",
        "    if np.all((embedding_matrix[token] == 0)):\n",
        "\n",
        "      if word in oov_terms: #Hanlde the OOV case with one of the methods\n",
        "        if oov_method == \"Random\":\n",
        "          embedding_vector = random_embedding(embedding_size)\n",
        "        elif oov_method == \"Placeholder\":\n",
        "          embedding_vector = placeholder_embedding(embedding_size)\n",
        "        elif oov_method == \"Mean\":\n",
        "          embedding_vector = neighbours_mean_embedding(word, glove_embedding, sentences)\n",
        "        else:\n",
        "          raise \"Invalid OOV method\"\n",
        "        \n",
        "        discovered_embeddings[word] = embedding_vector\n",
        "\n",
        "      else:\n",
        "        embedding_vector = glove_embedding[word]\n",
        "\n",
        "      embedding_matrix[token] = embedding_vector #Update the embedding matrix\n",
        "\n",
        "  #The computed values for the OOV words update the GloVe embeddings at the end of the process.\n",
        "  #Updating these values at runtime affects the \"Mean\" OOV method.\n",
        "  update_embeddings(glove_embedding, discovered_embeddings)\n",
        "\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo7AfoSdCfYO"
      },
      "source": [
        "##Train, validation and test vocabularies\n",
        "Here all the previous methods defined in the above sections are exploited to create three different vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1BxcRw4CzJo"
      },
      "source": [
        "setup_files() #Create a path, download and extract the files, if necessary\n",
        "glove_vocab = load_model(PATHS[\"glove_file\"]) #Load the GloVe model\n",
        "\n",
        "embedding_matrix_0 = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE), dtype=np.float32) #Create an empty embedding matrix\n",
        "\n",
        "#Build the embedding matrix with the training set data\n",
        "embedding_matrix_training = update_embedding_matrix(embedding_matrix_0,\n",
        "                                                    glove_embedding,\n",
        "                                                    EMBEDDING_SIZE,\n",
        "                                                    training_words,\n",
        "                                                    words_to_tokens,\n",
        "                                                    OOV_METHOD,\n",
        "                                                    training_sentences)\n",
        "\n",
        "#Get an updated version of the embedding matrix with the validation set data\n",
        "embedding_matrix_validation = update_embedding_matrix(embedding_matrix_training,\n",
        "                                                    glove_embedding,\n",
        "                                                    EMBEDDING_SIZE,\n",
        "                                                    validation_words,\n",
        "                                                    words_to_tokens,\n",
        "                                                    OOV_METHOD,\n",
        "                                                    validation_sentences)\n",
        "\n",
        "#Get an updated version of the embedding matrix with the test set data\n",
        "embedding_matrix_test = update_embedding_matrix(embedding_matrix_validation,\n",
        "                                                    glove_embedding,\n",
        "                                                    EMBEDDING_SIZE,\n",
        "                                                    test_words,\n",
        "                                                    words_to_tokens,\n",
        "                                                    OOV_METHOD,\n",
        "                                                    test_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MypraiXCuOIb"
      },
      "source": [
        "# Models\n",
        "This section is used for creating different models, going from a baseline to slightly more complicated ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7EPmCLyuZVB"
      },
      "source": [
        "## Constants and utilities\n",
        "First of all, define some constants, parameter dictionaries and methods that will be reused by each architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__iSg0qcABw"
      },
      "source": [
        "# TODO: all the following constants are temporary \n",
        "N_CLASSES = 20  # this must be equal to the number of tags\n",
        "VOCABULARY_SIZE = 1000  # this must be obtained from the dataset\n",
        "EMBEDDING_SIZE = 64  # hyper-parameter to properly set [Defined in GloVe/Constants and Utilities]\n",
        "MAX_SEQUENCE_SIZE = 100  # this must be obtained from the dataset\n",
        "\n",
        "BATCH_SIZE = 128  # hyper-parameter to properly set\n",
        "EPOCHS = 5\n",
        "\n",
        "\n",
        "# Model common compile information\n",
        "# Use sparse_categorical_crossentropy because labels are one hot encoded\n",
        "model_compile_info = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': 'sparse_categorical_crossentropy',\n",
        "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
        "}\n",
        "\n",
        "# Model common training information\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience=10,\n",
        "                                                restore_best_weights=True)]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq-mTGQRL-uo"
      },
      "source": [
        "# This tensor should contain the weights obtained by GloVe\n",
        "embedding_weights = np.zeros(shape=(VOCABULARY_SIZE, EMBEDDING_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stq44tS8CJSy"
      },
      "source": [
        "Define utility methods that will be used to **create**, **train** and **test** the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE7Mvm9sZ8Pf"
      },
      "source": [
        "def create_model(name,\n",
        "                 layers, \n",
        "                 compile_info, \n",
        "                 show_summary=True) -> keras.Model:\n",
        "    \"\"\"\n",
        "    Create the model using the layers passed as parameters.\n",
        "    After the creation, the model is compiled and its summary is possibly \n",
        "    printed to console.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers : array\n",
        "        Array that contains a list of layers that must be added \n",
        "        to the model.\n",
        "    compile_info: Dictionary\n",
        "        Contains information required for compiling the model.\n",
        "    show_summary: bool\n",
        "        If true, then the summary of the model will be printed to console\n",
        "    \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : keras.Model\n",
        "        The keras model.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential(name=name)\n",
        "    \n",
        "    for idx, layer in enumerate(layers):\n",
        "\n",
        "        # Sanity checks for being sure that the last layer has been \n",
        "        # correctly set\n",
        "        if idx == len(layers) - 1:\n",
        "            assert layer.activation == keras.activations.softmax, 'Wrong activation function'\n",
        "            assert layer.units == N_CLASSES, 'Wrong number of units'\n",
        "\n",
        "        model.add(layer)\n",
        "\n",
        "    # Compile\n",
        "    model.compile(**compile_info)\n",
        "\n",
        "    # Print model summary\n",
        "    if show_summary:\n",
        "        model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(model: keras.Model,\n",
        "                x_train: np.ndarray,\n",
        "                y_train: np.ndarray,\n",
        "                x_val: np.ndarray,\n",
        "                y_val: np.ndarray,\n",
        "                training_info: dict):\n",
        "    \"\"\"\n",
        "    Training routine for the Keras model.\n",
        "    At the end of the training, retrieved History data is shown.\n",
        "\n",
        "    :param model: Keras built model\n",
        "    :param x_train: training data in np.ndarray format\n",
        "    :param y_train: training labels in np.ndarray format\n",
        "    :param x_val: validation data in np.ndarray format\n",
        "    :param y_val: validation labels in np.ndarray format\n",
        "    :param training_info: dictionary storing model fit() argument information\n",
        "\n",
        "    :return\n",
        "        model: trained Keras model\n",
        "    \"\"\"\n",
        "    print(\"Start training! \\nParameters: {}\".format(training_info))\n",
        "    history = model.fit(x=x_train, y=y_train,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        **training_info)\n",
        "    print(\"Training completed! Showing history...\")\n",
        "\n",
        "    show_history(history)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_data(model: keras.Model,\n",
        "                 x: np.ndarray,\n",
        "                 prediction_info: dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Inference routine of a given input set of examples\n",
        "\n",
        "    :param model: Keras built and possibly trained model\n",
        "    :param x: input set of examples in np.ndarray format\n",
        "    :param prediction_info: dictionary storing model predict() argument information\n",
        "\n",
        "    :return\n",
        "        predictions: predicted labels in np.ndarray format\n",
        "    \"\"\"\n",
        "\n",
        "    print('Starting prediction: \\n{}'.format(prediction_info))\n",
        "    print('Predicting on {} samples'.format(x.shape[0]))\n",
        "\n",
        "    predictions = model.predict(x, **prediction_info)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def evaluate_predictions(predictions: np.ndarray,\n",
        "                         y: np.ndarray,\n",
        "                         metrics: List[Callable],\n",
        "                         metric_names: List[str]):\n",
        "    \"\"\"\n",
        "    Evaluates given model predictions on a list of metric functions\n",
        "\n",
        "    :param predictions: model predictions in np.ndarray format\n",
        "    :param y: ground-truth labels in np.ndarray format\n",
        "    :param metrics: list of metric functions\n",
        "    :param metric_names: list of metric names\n",
        "\n",
        "    :return\n",
        "        metric_info: dictionary containing metric values for each input metric\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(metrics) == len(metric_names)\n",
        "\n",
        "    print(\"Evaluating predictions! Total samples: \", y.shape[0])\n",
        "\n",
        "    metric_info = {}\n",
        "\n",
        "    for metric, metric_name in zip(metrics, metric_names):\n",
        "        metric_value = metric(y_pred=predictions, y_true=y)\n",
        "        metric_info[metric_name] = metric_value\n",
        "\n",
        "    return metric_info\n",
        "\n",
        "def model_sanity_check(model: keras.Model):\n",
        "    \"\"\"\n",
        "    Create a random input_tensor and try to pass through the model.\n",
        "    This method should be used in order to check if the model is \n",
        "    working as expected.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : keras.Model\n",
        "        The model that must be tried.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f'Sanity check for the model with name: {model.name}')\n",
        "    # Model sanity check for seeing if it runs correctly\n",
        "    input_tensor = np.random.uniform(size=(BATCH_SIZE, MAX_SEQUENCE_SIZE))\n",
        "    print(f'Input tensor shape: {input_tensor.shape}')\n",
        "    output_tensor = model(input_tensor)\n",
        "    print(f'Output tensor shape: {output_tensor.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPDiCsbl9ea"
      },
      "source": [
        "Define utility methods for **creating layers** in order to: \n",
        "* reduce the code verbosity.\n",
        "* be sure to always create different architectures with the same layer structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rCmJJgpmBRY"
      },
      "source": [
        "# EMBEDDING\n",
        "def embedding_layer(embedding_weights: np.array) -> layers.Embedding:\n",
        "    \"\"\"\n",
        "    Create an embedding layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embedding_weights : np.array\n",
        "        The weights for the embedding layer.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Embedding\n",
        "        The created embedding layer.\n",
        "    \"\"\"\n",
        "    layer = layers.Embedding(\n",
        "        input_dim=VOCABULARY_SIZE, \n",
        "        output_dim=EMBEDDING_SIZE, \n",
        "        input_length=MAX_SEQUENCE_SIZE,\n",
        "        weights=[embedding_weights],\n",
        "        mask_zero=True\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# RNN (LSTM and GRU)\n",
        "def _rnn_size(layer_depth: int) -> int:\n",
        "    \"\"\"\n",
        "    Simple logic used for assigning the number of units \n",
        "    to the rnn layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    size : int\n",
        "        The number units.\n",
        "    \"\"\"\n",
        "    size = 64\n",
        "    if layer_depth > 1:\n",
        "        size = 128\n",
        "    return size\n",
        "\n",
        "def bilstm_layer(layer_depth: int) -> layers.Bidirectional:\n",
        "    \"\"\"\n",
        "    Create a bidirectional lstm layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Bidirectional\n",
        "        The created bidirectional lstm layer.\n",
        "    \"\"\"\n",
        "    size = _rnn_size(layer_depth)\n",
        "    layer = layers.Bidirectional(\n",
        "        layers.LSTM(size, return_sequences=True, activation='relu')\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "def bigru_layer(layer_depth: int) -> layers.Bidirectional:\n",
        "    \"\"\"\n",
        "    Create a bidirectional gru layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Bidirectional\n",
        "        The created bidirectional gru layer.\n",
        "    \"\"\"\n",
        "    size = _rnn_size(layer_depth)\n",
        "    layer = layers.Bidirectional(\n",
        "        layers.GRU(size, return_sequences=True, activation='relu')\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# DENSE\n",
        "def _dense_size(last_layer:bool) -> int:\n",
        "    \"\"\"\n",
        "    Simple logic for assigning the size of the dense layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    size : int\n",
        "        The size of the dense layer.\n",
        "    \"\"\"\n",
        "    size = N_CLASSES\n",
        "    if not last_layer:\n",
        "        size = 256\n",
        "    return size\n",
        "\n",
        "def _dense_activation(last_layer:bool) -> str:\n",
        "    \"\"\"\n",
        "    Simple logic for assigning the activation function of the dense layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    activation : str\n",
        "        The activation function of the layer.\n",
        "    \"\"\"\n",
        "    activation = 'relu'\n",
        "    if last_layer:\n",
        "        activation = 'softmax'\n",
        "    return activation\n",
        "\n",
        "def dense_layer(last_layer:bool) -> layers.Dense:\n",
        "    \"\"\"\n",
        "    Create a dense layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Dense\n",
        "        The created dense layer.\n",
        "    \"\"\"\n",
        "    size = _dense_size(last_layer)\n",
        "    activation = _dense_activation(last_layer)\n",
        "    \n",
        "    return layers.Dense(size, activation=activation)\n",
        "\n",
        "# MODEL SANITY CHECK\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOhK3xp2FCUu"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoxVUVMap5Ma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800a4186-5984-4ef1-ad36-7233421b5d90"
      },
      "source": [
        "# Create layers\n",
        "baseline_layers = [\n",
        "                embedding_layer(embedding_weights=embedding_weights),\n",
        "                bilstm_layer(layer_depth=1),\n",
        "                dense_layer(last_layer=True)\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_model = create_model('baseline', \n",
        "                              baseline_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 100, 64)           64000     \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 100, 128)         66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100, 20)           2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 132,628\n",
            "Trainable params: 132,628\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline\n",
            "Input tensor shape: (128, 100)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LGdswmdFg6x"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4l5n888vLql"
      },
      "source": [
        "## Variations\n",
        "What follows is the implementation of small variations to the baseline architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMvk3AQFukOh"
      },
      "source": [
        "### GRU\n",
        "Change the LSTM layer with the GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "856hGaR9eDRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4f158f-7697-453b-e0a6-34b01af3696f"
      },
      "source": [
        "# Create layers\n",
        "baseline_var1_layers = [\n",
        "                embedding_layer(embedding_weights=embedding_weights),\n",
        "                bigru_layer(layer_depth=1),\n",
        "                dense_layer(last_layer=True)\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var1_model = create_model('baseline_var1', \n",
        "                              baseline_var1_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var1_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline_var1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 100, 64)           64000     \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 100, 128)         49920     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 100, 20)           2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 116,500\n",
            "Trainable params: 116,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline_var1\n",
            "Input tensor shape: (128, 100)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9We6j_USupzN"
      },
      "source": [
        "### Additional LSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLHH94CtqZ7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0dd637c-5154-4424-e9b7-dafe8724be92"
      },
      "source": [
        "# Create layers\n",
        "baseline_var2_layers = [\n",
        "                embedding_layer(embedding_weights=embedding_weights),\n",
        "                bilstm_layer(layer_depth=1),\n",
        "                bilstm_layer(layer_depth=2),\n",
        "                dense_layer(last_layer=True)\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var2_model = create_model('baseline_var2', \n",
        "                              baseline_var2_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var2_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline_var2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 100, 64)           64000     \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirecti  (None, 100, 128)         66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirecti  (None, 100, 256)         263168    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100, 20)           5140      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 398,356\n",
            "Trainable params: 398,356\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline_var2\n",
            "Input tensor shape: (128, 100)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RN6ySWyu13J"
      },
      "source": [
        "### Additional Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujfM5mekqma8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0c6723-1eeb-4972-c803-8a08d68c9c0f"
      },
      "source": [
        "# Create layers\n",
        "baseline_var3_layers = [\n",
        "                embedding_layer(embedding_weights=embedding_weights),\n",
        "                bilstm_layer(layer_depth=1),\n",
        "                dense_layer(last_layer=False),\n",
        "                dense_layer(last_layer=True)\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var3_model = create_model('baseline_var3', \n",
        "                              baseline_var3_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var3_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline_var3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 100, 64)           64000     \n",
            "                                                                 \n",
            " bidirectional_12 (Bidirecti  (None, 100, 128)         66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 100, 256)          33024     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 100, 20)           5140      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 168,212\n",
            "Trainable params: 168,212\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline_var3\n",
            "Input tensor shape: (128, 100)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwOShFdwvaHN"
      },
      "source": [
        "# Training and Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1SYhjS1vvzF"
      },
      "source": [
        "# Disussion and Error Analysis"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRJJ4tzaC852"
      },
      "source": [
        "# Preliminary Steps\n",
        "These are some preliminary steps before addressing the task. Import some basic libraries and set a variable that will be used in multiple steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCTmlk1C0Sm"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Evaluation and plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# utils\n",
        "from functools import partial\n",
        "from typing import List, Callable, Dict\n",
        "\n",
        "np.random.seed(seed=100) #Define a seed for randomization, avoiding to get different placeholder or random embeddings each time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vspH9UzptOFT"
      },
      "source": [
        "# Dataset\n",
        "What follows are a series of methods and utilities for processing the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reOmnUrPjRzt"
      },
      "source": [
        "## Constant and utilities\n",
        "Defining global variables and constants used to build, hanlde and tokenize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBylTiJGjVZe"
      },
      "source": [
        "DATASET_NAME = \"dependency_treebank\" #Name of the folder that will be automatically created after extracting the dataset\n",
        "DOCUMENT_EXTENSION = \".dp\" #Extension of the document files to be read\n",
        "\n",
        "USE_DOCUMENTS = True #True=Use documents; False = Use sentences (split documents into sentences)\n",
        "file_end_name = \"_documents\" if USE_DOCUMENTS else \"_sentences\"\n",
        "\n",
        "#List of paths to handle the dataset\n",
        "DATASET_PATHS = {\n",
        "    \"url\" : 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip', # Url to dowload the dataset\n",
        "    \"dataset_folder\": os.path.join(os.getcwd(), \"Datasets\", \"Original\"), # Folder containing the original dataset data\n",
        "    \"dataset_path\" : os.path.join(os.getcwd(), \"Datasets\", \"Original\", \"dependency_treebank.zip\"), # Path to zipped dataset\n",
        "    \"documents_path\" : os.path.join(os.getcwd(), \"Datasets\", \"Original\", DATASET_NAME), # Folder containing extracted documents (NB: it is created automatically during the extraction)\n",
        "    \"dataframe_folder\" : os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", DATASET_NAME), # Folder containing the dataframe data\n",
        "    \"dataframe_path\" : os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", DATASET_NAME, DATASET_NAME + file_end_name + \".pkl\") # Path to pickle save of built dataframe\n",
        "}\n",
        "\n",
        "TRAINING_DOCS = 100 #N. of documents to use in training set\n",
        "VALIDATION_DOCS = 50 #N. of documents to use in validation set\n",
        "TEST_DOCS = 49 #N. of documents to use in test set\n",
        "\n",
        "PADDING = 0 #Value for padding\n",
        "QUANTILE = 0.99 #Quantile used to catch a good max_tokens_length without wasting memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr0llgPylm31"
      },
      "source": [
        "##Folders creation\n",
        "Create folders for the dataset and the dataframe files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPThN5GWAZl"
      },
      "source": [
        "def create_folders(paths: List[str]):\n",
        "  '''\n",
        "  Create the folders in paths list.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  paths: List[str]\n",
        "    A list of all the folders to create\n",
        "  '''\n",
        "\n",
        "  for path in paths:\n",
        "    if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "\n",
        "folders = [DATASET_PATHS[\"dataset_folder\"],\n",
        "           DATASET_PATHS[\"dataframe_folder\"]]\n",
        "           \n",
        "create_folders(folders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsjdOVHZtSnp"
      },
      "source": [
        "## Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR9RtRITt72b"
      },
      "source": [
        "def download_dataset(dataset_path: str):\n",
        "  '''\n",
        "  Download the dataset zip file into the specified path\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  dataset_path: str\n",
        "    Destination path\n",
        "  '''\n",
        "\n",
        "  if not os.path.exists(dataset_path):\n",
        "      urllib.request.urlretrieve(DATASET_PATHS[\"url\"], dataset_path)\n",
        "\n",
        "      print(\"Successful download\")\n",
        "\n",
        "download_dataset(DATASET_PATHS[\"dataset_path\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByKCopDN_Rb-"
      },
      "source": [
        "## Dataset extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz8xnlmy_XvE"
      },
      "source": [
        "def extract_dataset(dataset_path: str,\n",
        "                    dataset_folder: str,\n",
        "                    documents_path: str):\n",
        "  '''\n",
        "  Extract dataset zip file.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  dataset_path: str\n",
        "    Path of the zip file\n",
        "  dataset_folder: str\n",
        "    Path of the dataset\n",
        "  documents_path: str\n",
        "    Path containing the documents after extraction\n",
        "  '''\n",
        "  \n",
        "  expected_docs_number = TRAINING_DOCS + VALIDATION_DOCS + TEST_DOCS\n",
        "\n",
        "  #Extract only if there are less than 199 documents or the extraction's folder does not exist\n",
        "  if not os.path.exists(documents_path) or len(os.listdir(documents_path))<expected_docs_number:\n",
        "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_folder)\n",
        "\n",
        "    print(\"Successful extraction\")\n",
        "\n",
        "extract_dataset(DATASET_PATHS[\"dataset_path\"],DATASET_PATHS[\"dataset_folder\"],DATASET_PATHS[\"documents_path\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGVpE8IuCTIa"
      },
      "source": [
        "##Preprocess\n",
        "Definition of methods to clean the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ZbwHLECWPk"
      },
      "source": [
        "def preprocess(token: str) -> str:\n",
        "\t\"\"\"\n",
        "\tCalls the function that cleans the text\n",
        "\tInput: the string to process\n",
        "\tOutput: the processed string\n",
        "\t\"\"\"\n",
        "\ttoken = text_to_lower(token)\n",
        "\ttoken = strip_text(token)\n",
        "\treturn token\n",
        "\n",
        "def text_to_lower(text: str) -> str:\n",
        "\t\"\"\"\n",
        "\tReturns the string in lower character\n",
        "\tInput: the string to process\n",
        "\tOutput: the processed string\n",
        "\t\"\"\"\n",
        "\treturn text.lower();\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "\t\"\"\"\n",
        "\tRemoves any left or right spacing (including carriage return) from text.\n",
        "\tExample:\n",
        "\tInput: '  This assignment is cool\\n'\n",
        "\tOutput: 'This assignment is cool'\n",
        "\t\"\"\"\n",
        "\n",
        "\treturn text.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJbpdftCt8Gl"
      },
      "source": [
        "## Dataframe creation\n",
        "This section is responsible for building a dataframe based on the dataset; using the format (split, document/sentence, labels).\n",
        "\n",
        "Where:\n",
        "1. split is a string value of type \"train\", \"validation\" or \"test\";\n",
        "2. document/sentence is a a string containing tokens belonging to a full document or sentence separated by spaces;\n",
        "3. labels is a string containing the correspondent labels to the document/sentence tokens separated by spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXgbC4TAuJ81"
      },
      "source": [
        "TOKEN_SEPARATOR = \" \" #Character used to separate tokens in the dataframe\n",
        "SENTENCE_SEPARATOR = \"##\" #Characters to signal the end of a sentence (if USE_DOCUMENTS=False)\n",
        "WORDS_CONTAINER = \"document\" if USE_DOCUMENTS else \"sentence\"\n",
        "\n",
        "def list_to_string(_list: List[str])->str :\n",
        "\t'''\n",
        "\tConvert a list of strings into a single string using a separator.\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\t_list: List[str]\n",
        "\t\tA list of strings\n",
        "\t\n",
        "\tReturns:\n",
        "\t--------\n",
        "\tstr:\n",
        "\t\tThe union of the previous strings into a single one\n",
        "\t'''\n",
        "\n",
        "\tstring = \"\"\n",
        "\n",
        "\tfor index, value in enumerate(_list):\n",
        "\t\tstring+=value\n",
        "\t\tif index!=len(_list)-1:\n",
        "\t\t\tstring+=TOKEN_SEPARATOR #Separate each word in the string\n",
        "\t\n",
        "\treturn string\n",
        "\n",
        "def add_row_to_dataframe_rows(dataframe_rows: List,\n",
        "                              split: str,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdocument: str,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlabels: str):\n",
        "\t'''\n",
        "\tAdd the elements of a dataframe row into dataframe's rows.\n",
        "\n",
        "\tParameters:\n",
        "\t-----------\n",
        "\tdataframe_rows: List\n",
        "\t\tDataframe's rows\n",
        "\tsplit: str\n",
        "\t\tSplit value (train, validation or test)\n",
        "\tdocument: str\n",
        "\t\tDocument or sentence\n",
        "\tlabels: str\n",
        "\t\tString containing the labels of a document or sentence\n",
        "\t'''\n",
        "\t\t\n",
        "\tif USE_DOCUMENTS:\n",
        "\t\tdataframe_row = {\"split\": split, WORDS_CONTAINER: document, \"labels\": labels}\n",
        "\t\tdataframe_rows.append(dataframe_row)\n",
        "\t\n",
        "\telse:\n",
        "\t\tsentences = document.split(SENTENCE_SEPARATOR)\n",
        "\t\tsplit_labels = labels.split(SENTENCE_SEPARATOR)\n",
        "\t\t#Add each sentences separately in the dataframe if using sentences\n",
        "\t\tfor i in range(len(sentences)):\n",
        "\t\t\tdataframe_row = {\"split\": split, WORDS_CONTAINER: sentences[i], \"labels\": split_labels[i]}\n",
        "\t\t\tdataframe_rows.append(dataframe_row)\n",
        "\t\n",
        "def rows_to_dataframe(rows: List)->pd.DataFrame:\n",
        "\t'''\n",
        "\tConvert a list of rows into a pandas Dataframe\n",
        "\n",
        "\tParameters:\n",
        "\t-----------\n",
        "\trows: List\n",
        "\t\tList of rows of type (split,document/sentence,labels)\n",
        "\n",
        "\tReturns:\n",
        "\t--------\n",
        "\tdataframe: pd.DataFrame\n",
        "\t\tA dataframe containing the given rows.\n",
        "\t'''\n",
        "\n",
        "\tdataframe = pd.DataFrame(rows)\n",
        "\tdataframe = dataframe[[\"split\", WORDS_CONTAINER, \"labels\"]]\n",
        "\n",
        "\treturn dataframe\n",
        "\n",
        "def get_documents(path: str)->List[str]:\n",
        "\t'''\n",
        "\tGet a list of the document files inside the specified path.\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\tpath: str\n",
        "\t\tPath containing the documents\n",
        "\n",
        "\tReturns:\n",
        "\t--------\n",
        "\tdocuments: List[str]\n",
        "\t\tList of document files\n",
        "\t'''\n",
        "\n",
        "\tfiles = os.listdir(path)\n",
        "\tdocuments = filter(lambda name: (name.endswith(DOCUMENT_EXTENSION)), files) #Select only documents with the right extension\n",
        "\tdocuments = list(documents)\n",
        "\tdocuments.sort()\n",
        " \n",
        "\treturn documents\n",
        " \n",
        "def get_document_number(filename: str)->int:\n",
        "\t'''\n",
        "\tGet the number of the given document\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\tfilename: str\n",
        "\t\tFilename of the document\n",
        "\t\n",
        "\tReturns:\n",
        "\t--------\n",
        "\tint:\n",
        "\t\tDocument's number\n",
        "\t'''\n",
        "\n",
        "\treturn int(filename.split(\"_\")[1].split(\".\")[0])\n",
        " \n",
        "def extract_data_from_line(line: str)->(str,str):\n",
        "\t'''\n",
        "\tExtract token and label from a given line.\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\tline: str\n",
        "\t\tA document/sentence line\n",
        "\n",
        "\tReturns:\n",
        "\t--------\n",
        "\tstr, str:\n",
        "\t\t1. Token and label if line is not empty (\\n)\n",
        "\t\t2. None, None if line is empty and USE_DOCUMENTS=True\n",
        "\t\t3. A couple of sentence separators if line is empty and USE_DOCUMENTS=False\n",
        "\t\t\t to signal the end of a sentence\n",
        "\t'''\n",
        "\n",
        "\tif line != \"\\n\":\n",
        "\t\tcolumns = line.split()\n",
        "\t\ttoken = columns[0] #The token is the first element of a line\n",
        "\t\ttoken = preprocess(token)\n",
        "\t\tlabel = columns[1] #The label is the second element of a line\n",
        "\n",
        "\t\treturn token, label\n",
        "\n",
        "\telse:\n",
        "\t\tif USE_DOCUMENTS: #Ignore the empty line (\"\\n\") if using documents\n",
        "\t\t\treturn None, None\n",
        "\t\telse:\n",
        "\t\t\treturn SENTENCE_SEPARATOR, SENTENCE_SEPARATOR #Mark the empy line (\"\\n\") as the end of a sentence if using sentences\n",
        "\n",
        "def process_document(document: str,\n",
        "                     doc_number: int)->(str,List[str],List[str]):\n",
        "\t'''\n",
        "\tRead a document/sentence and extract split, tokens and labels data.\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\tdocument: str\n",
        "\t\tDocument to read\n",
        "    doc_number: int\n",
        "\t\tNumber of the document\n",
        "\n",
        "\tReturns:\n",
        "\t\tsplit, tokens and labels of the given document/sentence\n",
        "\t'''\n",
        "\n",
        "\ttokens = []\n",
        "\tlabels = []\n",
        "\tsplit = \"\"\n",
        "\n",
        "\ttry:\n",
        "\t\tif os.path.isfile(document):\n",
        "\t\t\t#Open the file\n",
        "\t\t\twith open(document, mode='r', encoding='utf-8') as text_file:\n",
        "\n",
        "\t\t\t\t#Split in different groups\n",
        "\t\t\t\tif doc_number <= TRAINING_DOCS:\n",
        "\t\t\t\t\tsplit = \"train\"\n",
        "\t\t\t\telif doc_number <= TRAINING_DOCS+VALIDATION_DOCS:\n",
        "\t\t\t\t\tsplit = \"validation\"\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tsplit = \"test\"\n",
        "\n",
        "\t\t\t\t#Extract tokens and labels line by line\n",
        "\t\t\t\tfor line in text_file:\n",
        "\t\t\t\t\ttoken, label = extract_data_from_line(line)\n",
        "\n",
        "\t\t\t\t\tif token!=None and label!=None:\n",
        "\t\t\t\t\t\ttokens.append(token)\n",
        "\t\t\t\t\t\tlabels.append(label)\n",
        "\t\t \n",
        "\texcept Exception as e:\n",
        "                print('Failed to process %s. Reason: %s' % (document, e))\n",
        "                sys.exit(0)\n",
        "\n",
        "\treturn split, list_to_string(tokens), list_to_string(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQECWv5YD28l"
      },
      "source": [
        "###Build/Load Dataframe\n",
        "Use the previous methods to build and save the dataframe or load the dataframe if already saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8ePHxt0D74q"
      },
      "source": [
        "def build_dataframe(documents_path: str,\n",
        "                    dataframe_path: str,\n",
        "\t\t\t\t\t\t\t\t\t\tsave=True)->pd.DataFrame:\n",
        "\t'''\n",
        "\tBuild a dataframe and optionally save it.\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\tdocuments_path: str\n",
        "\t\tPath containing the documents\n",
        "    dataframe_path: str\n",
        "\t\tPath to save the dataframe in\n",
        "\tsave: bool\n",
        "\t\tFlag to decide on saving or not the built dataframe\n",
        "\n",
        "\tReturns:\n",
        "\t--------\n",
        "\tdataframe: pd.DataFrame\n",
        "\t\tThe built dataframe\n",
        "\t'''\n",
        "\n",
        "\tdocuments = get_documents(documents_path)\n",
        "\tdataframe_rows = []\n",
        "\n",
        "\t#Iterate along the files\n",
        "\tfor filename in documents:\n",
        "\t\tdocument = os.path.join(documents_path, filename)\n",
        "\t\tdoc_number = get_document_number(filename)\n",
        "\n",
        "\t\tsplit, tokens, labels = process_document(document, doc_number)\n",
        "\t\tadd_row_to_dataframe_rows(dataframe_rows,split,tokens,labels)\n",
        "\n",
        "\t#Transform the list of rows in a proper dataframe\n",
        "\tdataframe = rows_to_dataframe(dataframe_rows)\n",
        "\tprint(\"Dataframe built successfully\")\n",
        "\t\n",
        "\t#Save the dataframe\n",
        "\tif save:\n",
        "\t\tdataframe.to_pickle(dataframe_path)\n",
        "\t\tprint(\"Dataframe saved successfully\")\n",
        " \n",
        "\treturn dataframe\n",
        "\n",
        "def load_dataframe(documents_path: str,\n",
        "                   dataframe_path: str,\n",
        "\t\t\t\t\t\t\t\t\t force_rebuild = False)->pd.DataFrame:\n",
        "\t'''\n",
        "\tLoad the dataframe from memory it was saved, or build a new one.\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\tdocuments_path: str\n",
        "\t\tPath containing the documents\n",
        "    dataframe_path: str\n",
        "\t\tPath to load the dataframe from\n",
        "\tforce_rebuild: bool\n",
        "\t\tFlag to force the rebuild of the dataframe even if a saved copy exists\n",
        "\n",
        "\tReturns:\n",
        "\t--------\n",
        "\tpd.DataFrame\n",
        "\t\tThe built or loaded dataframe\n",
        "\t'''\n",
        "\n",
        "\tif not os.path.exists(dataframe_path) or force_rebuild:\n",
        "\t\treturn build_dataframe(documents_path, dataframe_path)\n",
        "\telse:\n",
        "\t\treturn pd.read_pickle(dataframe_path) #Load dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uIhDkgTkRVy"
      },
      "source": [
        "##Split dataset\n",
        "Split dataset into train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMM5D7ZTeBGJ"
      },
      "source": [
        "df = load_dataframe(DATASET_PATHS[\"documents_path\"],DATASET_PATHS[\"dataframe_path\"], True)\n",
        "\n",
        "#Test\n",
        "#print(df.iloc[1][WORDS_CONTAINER])\n",
        "#print(df.iloc[1][\"labels\"])\n",
        "\n",
        "training_set = df.loc[df[\"split\"] == \"train\"]\n",
        "validation_set = df.loc[df[\"split\"] == \"validation\"]\n",
        "test_set = df.loc[df[\"split\"] == \"test\"]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_dict = {}\n",
        "for label in test_set['labels']:\n",
        "    label_len = len(label)\n",
        "    print(label_len)\n",
        "    if label_len in res_dict:\n",
        "        curr_val = res_dict[label_len]\n",
        "        curr_val += 1\n",
        "        res_dict[label_len] = curr_val\n",
        "    else:\n",
        "        res_dict[label_len] = 1\n",
        "\n",
        "#res_dict"
      ],
      "metadata": {
        "id": "bNelSs3nhlEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TmFlty5oJh4"
      },
      "source": [
        "##Tokenization and vocabularies\n",
        "Here each set is passed through a tokenization process, which also allows to define the vocabulary of each set and also their vocabulary size. Furthermore the maximum length of a token sequence is defined and the labels are exteacted from the sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xmzxl78oesT"
      },
      "source": [
        "STARTING_TOKEN = 1 #First value to start the tokenization on (0 is already used as padding value)\n",
        "\n",
        "def get_tokenizer(corpus: List[str],\n",
        "                  starting_dict=None)->Dict[str,int]:\n",
        "  '''\n",
        "  Create or expand (given an existing dictionary) a tokenization dictionary\n",
        "  that associates an integer to each word.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  corpus: List[str]\n",
        "    Text to examine searching for new words to add into the dictionary\n",
        "  starting_dict: Dict[str,int]\n",
        "    An already filled dictionary to further expand (optional)\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    1. A filled dictionary that associates an integer to each word (if starting_dict=None);\n",
        "    2. An expanded dictionary that associates an integer to each new word (if starting_dict is not None)\n",
        "  '''\n",
        "\n",
        "  #Copy the original dictionary to keep it save from updates\n",
        "  words_to_tokens = {} if starting_dict==None else starting_dict.copy()\n",
        "\n",
        "  for text in corpus:\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "      if not word in words_to_tokens:\n",
        "        words_to_tokens[word] = len(words_to_tokens)+STARTING_TOKEN\n",
        "\n",
        "  return words_to_tokens\n",
        "\n",
        "def tokenize(word: str,\n",
        "             words_to_tokens: Dict[str,int])->int:\n",
        "  '''\n",
        "  Get the integer value of a given token.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  word: str\n",
        "    Token\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  int:\n",
        "    Value associated to the token\n",
        "  '''\n",
        "\n",
        "  return words_to_tokens[word]\n",
        "\n",
        "def detokenize(token:int,\n",
        "               words_to_tokens: Dict[str,int])->str:\n",
        "  '''\n",
        "  Get the token-word of a given token-value.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  token: int\n",
        "    Tokenized word\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  str:\n",
        "    Word associated to the token-value\n",
        "  '''\n",
        "\n",
        "  val_list = list(words_to_tokens.values())\n",
        "  key_list = list(words_to_tokens.keys())\n",
        "\n",
        "  position = val_list.index(token)\n",
        "\n",
        "  return key_list[position]\n",
        "\n",
        "  #return words_to_tokens.index(token)\n",
        "\n",
        "def tokenize_string(string: str,\n",
        "                    words_to_tokens: Dict[str,int],\n",
        "                    max_length: int)->List[int]:\n",
        "\n",
        "  '''\n",
        "  Get the tokenized sequence of a string of separated tokens (document/sentence).\n",
        "\n",
        "  Parameters:\n",
        "  string: str\n",
        "    String of separated tokens (document or sentence)\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "  max_length: int\n",
        "    Tokenization length\n",
        "\n",
        "  Returns:\n",
        "    List[int]:\n",
        "      A list of token-values where each one is the tokenized value of a token\n",
        "      int the input-string.\n",
        "      The list is padded if its length is below the max_length.\n",
        "      The list is truncated if its length is above the max_length.\n",
        "  '''\n",
        "\n",
        "  tokens = string.split()\n",
        "  tokenized_sequence = [tokenize(token, words_to_tokens)  for token in tokens]\n",
        "  length_diff = max_length-len(tokenized_sequence)\n",
        "\n",
        "  if length_diff==0: # Return the same sequence if it has the requested size\n",
        "    return tokenized_sequence\n",
        "  elif length_diff<0: # Return the truncated sequence if it exceeds the requested size\n",
        "    return tokenized_sequence[0:max_length]\n",
        "  else: # Return the padded sequence if it has an inferior size than the expected one\n",
        "    return np.pad(tokenized_sequence, (PADDING, length_diff), 'constant').tolist()\n",
        "\n",
        "#Define corpus\n",
        "train_text = training_set[WORDS_CONTAINER].tolist()\n",
        "val_text = validation_set[WORDS_CONTAINER].tolist()\n",
        "test_text = test_set[WORDS_CONTAINER].tolist()\n",
        "\n",
        "#Define labels\n",
        "train_labels = training_set[\"labels\"].tolist()\n",
        "val_labels = validation_set[\"labels\"].tolist()\n",
        "test_labels = test_set[\"labels\"].tolist()\n",
        "\n",
        "#Token dictionaries\n",
        "train_tokens = get_tokenizer(train_text)\n",
        "val_tokens = get_tokenizer(val_text, starting_dict = train_tokens)\n",
        "test_tokens = get_tokenizer(test_text, starting_dict = val_tokens)\n",
        "\n",
        "#Vocabularies\n",
        "train_vocab = train_tokens.keys()\n",
        "val_vocab = val_tokens.keys()\n",
        "test_vocab = test_tokens.keys()\n",
        "\n",
        "#Vocab sizes\n",
        "train_vocab_size = len(train_vocab)\n",
        "val_vocab_size = len(val_vocab)\n",
        "test_vocab_size = len(test_vocab)\n",
        "\n",
        "#Max lenght of a token sequence\n",
        "corpus = train_text+val_text+test_text\n",
        "n_tokens = [len(doc.split()) for doc in corpus]\n",
        "max_length = int(np.quantile(n_tokens,QUANTILE))\n",
        "print(max_length)\n",
        "\n",
        "#Tokenized sets\n",
        "train_tokenized = np.array(list(map(lambda string: tokenize_string(string, train_tokens,max_length),train_text)))\n",
        "val_tokenized = np.array(list(map(lambda string: tokenize_string(string, val_tokens,max_length),val_text)))\n",
        "test_tokenized = np.array(list(map(lambda string: tokenize_string(string, test_tokens,max_length),test_text)))\n",
        "\n",
        "# Debug\n",
        "#with np.printoptions(threshold=np.inf):\n",
        "#    print(train_tokenized[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWhsQLQPdbQ"
      },
      "source": [
        "##Labels encoding\n",
        "Functions for encoding and decoding of labels from string representation to a numerical one are implemented here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qSrRRAbPhzo"
      },
      "source": [
        "def get_categorical_encoding(labels_list: List[str])->Dict[str,int]:\n",
        "  '''\n",
        "  Create the categorical encoding of a list of labels.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  labels_list: List[str]\n",
        "    List of all labels\n",
        "  \n",
        "  Returns:\n",
        "  --------\n",
        "  encoding: Dict[str,int]\n",
        "    A dictionary that associates a unique integer value to each possible\n",
        "    different label\n",
        "  '''\n",
        "\n",
        "  encoding = {}\n",
        "\n",
        "  for label_group in labels_list:\n",
        "    labels = label_group.split()\n",
        "    for label in labels:\n",
        "      if not label in encoding:\n",
        "        encoding[label] = len(encoding) + 1 # 0 is padding reserved\n",
        "\n",
        "  return encoding\n",
        "\n",
        "def encode_label(label: str,\n",
        "                 encoding: Dict[str,int])->int:\n",
        "  '''\n",
        "  Encode a label with its corresponding encoding.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  label: str\n",
        "    Label to encode\n",
        "  encoding: Dict[str,int]\n",
        "    Encoding dictionary of all possible labels\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  int:\n",
        "    Encoding value of the given label\n",
        "  '''\n",
        "\n",
        "  return encoding[label]\n",
        "\n",
        "def decode_label(value,\n",
        "                 encoding: Dict[str,int])->str:\n",
        "  '''\n",
        "  Decode a label-encoding value with its corresponding label.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  value\n",
        "    Value to decode\n",
        "  encoding: Dict[str,int]\n",
        "    Encoding dictionary of all possible labels\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  str:\n",
        "    Corresponding decoded label\n",
        "  '''\n",
        "\n",
        "  val_list = list(encoding.values())\n",
        "  key_list = list(encoding.keys())\n",
        "\n",
        "  position = val_list.index(value)\n",
        "\n",
        "  return key_list[position]\n",
        "\n",
        "  #return encoding.index[value]\n",
        "\n",
        "def encode_string_of_labels(string:str,\n",
        "                            encoding:Dict[str,int], max_length:int)->np.array:\n",
        "  '''\n",
        "  Get the encoded sequence of a string of separated labels.\n",
        "\n",
        "  Parameters:\n",
        "  string: str\n",
        "    String of separated labels\n",
        "  encoding: Dict[str,int]\n",
        "    Encoding dictionary of all possible labels\n",
        "  max_length: int\n",
        "    Tokenization length\n",
        "\n",
        "  Returns:\n",
        "    np.array:\n",
        "      Numpy array of encoded labels where each one is the encoded version of the corresponding label\n",
        "      int the input-string.\n",
        "      The array is padded if its length is below the max_length.\n",
        "      The array is truncated if its length is above the max_length.\n",
        "  '''\n",
        "\n",
        "  labels = string.split()\n",
        "  encoded_sequence = [encode_label(label, encoding)  for label in labels]\n",
        "\n",
        "  length_diff = max_length-len(encoded_sequence)\n",
        "  if length_diff==0:\n",
        "    return np.array(encoded_sequence) # Return the same sequence if it has the requested size\n",
        "  elif length_diff<0:\n",
        "    return np.array(encoded_sequence[0:max_length]) # Return the truncated sequence if it exceeds the requested size\n",
        "  else:\n",
        "    return np.pad(encoded_sequence, (PADDING, length_diff), 'constant') # Return the padded sequence if it has an inferior size than the expected one\n",
        "\n",
        "def is_punctuation_label(label: str)->bool:\n",
        "  '''\n",
        "  Return True if the label is a punctuation label and False otherwise.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  label: str\n",
        "    Label to classify as a punctuation one or not\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  bool:\n",
        "    True if the label is a punctuation label, False otherwise\n",
        "  '''\n",
        "  punctuation_labels = ['``', '#', '$', \"''\", ',', '-LRB-', '-RRB-', '.', ':']\n",
        "\n",
        "  # Debug\n",
        "  #if label in punctuation_labels:\n",
        "  #    print(f'removing: {label}')\n",
        "  \n",
        "  return label in punctuation_labels\n",
        "\n",
        "labels_list = training_set[\"labels\"].tolist() + validation_set[\"labels\"].tolist() + test_set[\"labels\"].tolist()\n",
        "labels_encoding = get_categorical_encoding(labels_list) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Co7hfK2uKD8"
      },
      "source": [
        "# GloVe\n",
        "This section is the one responsible for the implementation of the GloVe embedding system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8qLxw1kcTto"
      },
      "source": [
        "## Constants and utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqG3dXiBcdeA"
      },
      "source": [
        "URL_BASE = \"https://nlp.stanford.edu/data\" #Location of the pre-trained GloVe's files\n",
        "GLOVE_VERSION = \"6B\"\n",
        "\n",
        "EMBEDDING_SIZE = 200 #The dimensionality of the embeddings; to be tested\n",
        "\n",
        "#List of paths to download and extract GloVe's files\n",
        "PATHS = {\n",
        "    \"url\": URL_BASE + \"/glove.\" + GLOVE_VERSION + \".zip\",\n",
        "    \"glove_path\": os.path.join(os.getcwd(),\"Glove\",GLOVE_VERSION),\n",
        "    \"glove_zip\": os.path.join(os.getcwd(),\"Glove\", GLOVE_VERSION, \"glove.\"+GLOVE_VERSION+\".zip\"),\n",
        "    \"glove_file\": os.path.join(os.getcwd(),\"Glove\", GLOVE_VERSION, \"glove.\"+GLOVE_VERSION+\".\"+str(EMBEDDING_SIZE)+\"d.txt\")\n",
        "}\n",
        "\n",
        "#Determine which OOV method to adopt; choose one between \"Mean\", \"Random\" and \"Placeholder\"\n",
        "OOV_METHOD = \"Random\" \n",
        "\n",
        "# Constant value used when OOV_METHOD = 'Placeholder'. Randomly initialized.\n",
        "PLACEHOLDER = np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIY-9Qf_bOU5"
      },
      "source": [
        "## Download\n",
        "In this part the presence of the GloVe file is checked. In case of a negative response, it will be downloaded and extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r14RFpFCbFn2"
      },
      "source": [
        "def setup_files():\n",
        "  '''\n",
        "  Create the folder if it does not exist.\n",
        "  Then download the zip file from the web archive if it does not exist.\n",
        "  Finally exctract the zip file of the GloVe txt file does not exist in the folder.\n",
        "  '''\n",
        "\n",
        "  if not os.path.exists(PATHS[\"glove_path\"]):\n",
        "    os.makedirs(PATHS[\"glove_path\"])\n",
        "\n",
        "  if not os.path.exists(PATHS[\"glove_file\"]):\n",
        "    if not os.path.exists(PATHS[\"glove_zip\"]):\n",
        "      download_glove(PATHS[\"url\"])\n",
        "\n",
        "    extract_glove(PATHS[\"glove_zip\"],PATHS[\"glove_path\"])\n",
        "\n",
        "def download_glove(url: str):\n",
        "    '''\n",
        "    Download GloVe's zip file from the web.\n",
        "    '''\n",
        "\n",
        "    urllib.request.urlretrieve(url, PATHS['glove_zip'])\n",
        "    print(\"Successful download\")\n",
        "\n",
        "def extract_glove(zip_file: str,\n",
        "                  glove_path: str):\n",
        "  \n",
        "    '''\n",
        "    Extract GloVe's zip file.\n",
        "    '''\n",
        "  \n",
        "    with zipfile.ZipFile(PATHS[\"glove_zip\"], 'r') as zip_ref:\n",
        "      zip_ref.extractall(path=PATHS[\"glove_path\"])\n",
        "      print(\"Successful extraction\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYhDkgNQejX3"
      },
      "source": [
        "## Initialization\n",
        "In this step, the downloaded GloVe file is loaded into an embedding vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfLDOt7le78v"
      },
      "source": [
        "def load_model(glove_file: str) ->Dict:\n",
        "  '''\n",
        "  Open GloVe's txt file and store each of its contained words\n",
        "  into a dictionary along with their correspondent embedding weights.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  glove_file : str\n",
        "      GloVe's txt file path.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  vocabulary: Dict\n",
        "      GloVe's vocabulary\n",
        "\n",
        "  '''\n",
        "\n",
        "  print(\"Loading GloVe Model...\")\n",
        "\n",
        "  with open(glove_file, encoding=\"utf8\" ) as f: #Open the txt file\n",
        "      lines = f.readlines() #Read the file line by line\n",
        "\n",
        "  vocabulary = {}\n",
        "  for line in lines:\n",
        "      splits = line.split()\n",
        "      #Save the first part of the line (word) as the dictionary's key and the second part (the embedding) as the key\n",
        "      vocabulary[splits[0]] = np.array([float(val) for val in splits[1:]])\n",
        "\n",
        "  print(\"GloVe model loaded\")\n",
        "\n",
        "  return vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tRukt6Kxx4R"
      },
      "source": [
        "## OOV\n",
        "In this section, some possible \"Out Of Vocabulary\" handling methods are implemented, along with other OOV-related functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-uwA9DYuNtr"
      },
      "source": [
        "#OOV-handling: possible methods\n",
        "\n",
        "def random_embedding(embedding_size: int) ->np.array:\n",
        "    '''\n",
        "    Return a numpy array with random values sampled from a uniform distribution.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    embedding_size: int\n",
        "        The embedding size that is used as the size of the numpy array.\n",
        "\n",
        "    Results:\n",
        "    -------\n",
        "    np.array\n",
        "    A randomized numpy array.\n",
        "    '''\n",
        "\n",
        "    return np.random.uniform(low=-0.05, high=0.05, size=embedding_size)\n",
        "\n",
        "def placeholder_embedding() ->np.ndarray:\n",
        "    '''\n",
        "    Return the placeholder embedding as the OOV embedding.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    np.ndarray:\n",
        "        Placeholder embedding\n",
        "    '''\n",
        "\n",
        "    return PLACEHOLDER\n",
        "\n",
        "def neighbours_mean_embedding(word: str,\n",
        "                              glove_embedding: Dict[str,int],\n",
        "                              sentences: List[str],\n",
        "                              labels_list: List[str])->np.array:\n",
        "  \n",
        "    '''\n",
        "    Compute the embedding of an OOV word by taking the mean\n",
        "    of its neighbours.\n",
        "\n",
        "    Parameters:\n",
        "    ---------\n",
        "    word: str\n",
        "        The OOV that needs to be embedded.\n",
        "    glove_embedding: Dict[str, int]\n",
        "        GloVe's embedding.\n",
        "    sentences: List[str]\n",
        "        A list of all the sentences/documents (strings of separated words) in the current set.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    mean: int\n",
        "        The mean of the embedding values of OOV-word's neighbours.\n",
        "    '''\n",
        "  \n",
        "    neighbours = set()\n",
        "\n",
        "    for sentence,labels_group in zip(sentences,labels_list):\n",
        "        tokens = sentence.split()\n",
        "        labels = labels_group.split()\n",
        "        for index, token in enumerate(tokens):\n",
        "            if word == token:\n",
        "                if index!=0: #Check if a left_neighbour exists\n",
        "                    left_neighbour = tokens[index-1]\n",
        "                if left_neighbour in glove_embedding and not is_punctuation_label(labels[index-1]): #Consider only words that are not OOV and not punctuation\n",
        "                    neighbours.add(left_neighbour)\n",
        "                if index!=len(tokens)-1: #Check if a right_neighbour exists\n",
        "                    right_neighbour = tokens[index+1]\n",
        "                if right_neighbour in glove_embedding and not is_punctuation_label(labels[index+1]): #Consider only words that are not OOV and not punctuation\n",
        "                    neighbours.add(right_neighbour)\n",
        "\n",
        "    neighbours_embeddings = np.array([glove_embedding[neighbour] for neighbour in neighbours])\n",
        "    #Return the mean of the neighbours; if there are no valid neighbours return a placeholder embedding\n",
        "    return np.mean(neighbours_embeddings) if len(neighbours)>0 else PLACEHOLDER\n",
        "\n",
        "\n",
        "#Others\n",
        "def get_oov_list(words: List[str],\n",
        "                 glove_embedding: Dict[str, int]) ->List[str]:\n",
        "    '''\n",
        "    Return a list of all the words that are not part of the GloVe embedding\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    words: List[str]\n",
        "        A list of unique words from a set of documents.\n",
        "    glove_embedding: Dict[str, int]\n",
        "        GloVe's embedding.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    oov: List[str]\n",
        "        A list of all the OOV terms.\n",
        "    '''\n",
        "\n",
        "    embedding_vocabulary = set(glove_embedding.keys())\n",
        "    oov = set(words).difference(embedding_vocabulary)\n",
        "\n",
        "    return list(oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emKY7vJ-z6Od"
      },
      "source": [
        "##Embedding matrix\n",
        "Now, having opted for an OOV method, it is possible to create the embedding matrix, which associates the embedding to the correspondent word for the entire vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m33pLFGG0ysd"
      },
      "source": [
        "def update_embeddings(glove_embedding: Dict[str, int],\n",
        "                     new_embeddings: Dict[str, int]):\n",
        "    '''\n",
        "    Update the GloVe's embeddings by adding the new embeddings of\n",
        "    the previous OOV words.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    glove_embedding: Dict[str, int]\n",
        "        GloVe's embedding.\n",
        "    new_embeddings: Dict[str, int]\n",
        "        A dictionary containing the new embeddings\n",
        "        for the analyzed OOV words.\n",
        "    '''\n",
        "    \n",
        "    #Merge GloVe's embeddings with the new discoveries\n",
        "    glove_embedding.update(new_embeddings)\n",
        "\n",
        "def build_embedding_matrix(vocab_size: int,\n",
        "                            glove_embedding: Dict[str, int],\n",
        "                            embedding_size: int,\n",
        "                            words_to_tokens: Dict[str,int],\n",
        "                            oov_method: str,\n",
        "                            sentences: List[str],\n",
        "                            labels: List[str]) ->np.ndarray:\n",
        "    '''\n",
        "    Get the embedding matrix of the given set of documents/sentences.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    vocab_size: int\n",
        "        Size of the set's vocabulary\n",
        "    glove_embedding: Dict[str, int]\n",
        "        GloVe's embedding\n",
        "    embedding_size: int\n",
        "        The embedding size for tokens' embeddings\n",
        "    words_to_tokens: Dict[str,int]\n",
        "        Tokenization dictionary of the given set\n",
        "    oov_method: str\n",
        "        Method to handle OOV terms\n",
        "    sentences: List[str]\n",
        "        Set (training, validation or test) of documents/labels\n",
        "    labels: List[str]\n",
        "        List of the labels for each document/sentence of the given set\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    embedding_matrix: np.ndarray\n",
        "        Created and filled embedding matrix for the given set of documents/sentences\n",
        "    '''\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_size), dtype=np.float32) #Create an empty embedding matrix\n",
        "\n",
        "    oov_terms = get_oov_list(words_to_tokens.keys(), glove_embedding)\n",
        "    discovered_embeddings = {}\n",
        "\n",
        "    for word, token in tqdm(words_to_tokens.items()):\n",
        "        if np.all((embedding_matrix[token-STARTING_TOKEN] == 0)):\n",
        "\n",
        "            if word in oov_terms: #Hanlde the OOV case with one of the methods\n",
        "                if oov_method == \"Random\":\n",
        "                    embedding_vector = random_embedding(embedding_size)\n",
        "                elif oov_method == \"Placeholder\":\n",
        "                    embedding_vector = placeholder_embedding()\n",
        "                elif oov_method == \"Mean\":\n",
        "                    embedding_vector = neighbours_mean_embedding(word, glove_embedding, sentences, labels)\n",
        "                else:\n",
        "                    raise \"Invalid OOV method\"\n",
        "                discovered_embeddings[word] = embedding_vector\n",
        "\n",
        "            else:\n",
        "                embedding_vector = glove_embedding[word]\n",
        "\n",
        "            embedding_matrix[token-STARTING_TOKEN] = embedding_vector #Update the embedding matrix\n",
        "\n",
        "    #The computed values for the OOV words update the GloVe embeddings at the end of the process.\n",
        "    #Updating these values at runtime affects the \"Mean\" OOV method.\n",
        "    update_embeddings(glove_embedding, discovered_embeddings)\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo7AfoSdCfYO"
      },
      "source": [
        "##Train, validation and test embedding matrices\n",
        "Here all the previous methods defined in the above sections are exploited to create three different vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1BxcRw4CzJo"
      },
      "source": [
        "setup_files() #Create a path, download and extract the files, if necessary\n",
        "glove_embedding = load_model(PATHS[\"glove_file\"]) #Load the GloVe model\n",
        "\n",
        "#Build the embedding matrix with the training set data\n",
        "train_embedding_matrix = build_embedding_matrix(train_vocab_size,\n",
        "                                                glove_embedding,\n",
        "                                                EMBEDDING_SIZE,\n",
        "                                                train_tokens,\n",
        "                                                OOV_METHOD,\n",
        "                                                train_text,\n",
        "                                                train_labels)\n",
        "\n",
        "#Get an updated version of the embedding matrix with the validation set data\n",
        "val_embedding_matrix = build_embedding_matrix(val_vocab_size,\n",
        "                                                glove_embedding,\n",
        "                                                EMBEDDING_SIZE,\n",
        "                                                val_tokens,\n",
        "                                                OOV_METHOD,\n",
        "                                                val_text,\n",
        "                                                val_labels)\n",
        "\n",
        "#Get an updated version of the embedding matrix with the test set data\n",
        "test_embedding_matrix = build_embedding_matrix(test_vocab_size,\n",
        "                                                glove_embedding,\n",
        "                                                EMBEDDING_SIZE,\n",
        "                                                test_tokens,\n",
        "                                                OOV_METHOD,\n",
        "                                                test_text,\n",
        "                                                test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1cXXWF_i1yz"
      },
      "source": [
        "## Models' input initialization\n",
        "The previous methods and variables are combined to compute the Input and Class values for all the sets, which will be used in the models' section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDS5yCuxjIcO"
      },
      "source": [
        "def build_input(embedding_matrix: np.array,\n",
        "                tokenized_sequence: List[List[int]],\n",
        "                embedding_size: int)->np.array:\n",
        "    '''\n",
        "    Build the input to be used on neural's models.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    embedding_matrix: np.array\n",
        "        Embedding matrix of the current set\n",
        "    tokenized_sequence: List[List[int]]\n",
        "        Tokenized sequences for the current set\n",
        "    embedding_size: int\n",
        "        Embedding size\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    input: np.array\n",
        "        The embedded and reshaped input to pass into the models\n",
        "    '''\n",
        "\n",
        "    n_docs = len(tokenized_sequence)\n",
        "    n_tokens = len(tokenized_sequence[0])\n",
        "\n",
        "    input = np.zeros((n_docs, n_tokens, embedding_size)) #Create a matrix of size (doc/sentences, n. tokens, embedding size)\n",
        "\n",
        "    for doc_index, tokens in enumerate(tokenized_sequence):\n",
        "        for token_index, token in enumerate(tokens):\n",
        "            if token!=PADDING:\n",
        "                input[doc_index][token_index] = embedding_matrix[token-STARTING_TOKEN] #Each cell of the matrix contains the embedding of the correspondent token\n",
        "            else:\n",
        "                input[doc_index][token_index] = np.zeros(embedding_size) #Set a null embedding for padding tokens\n",
        "\n",
        "    return input\n",
        "\n",
        "#Input values\n",
        "X_train = build_input(train_embedding_matrix, train_tokenized, EMBEDDING_SIZE)\n",
        "X_val = build_input(val_embedding_matrix, val_tokenized, EMBEDDING_SIZE)\n",
        "X_test = build_input(test_embedding_matrix, test_tokenized, EMBEDDING_SIZE)\n",
        "\n",
        "#Class values\n",
        "y_train = np.array(list(map(lambda string: encode_string_of_labels(string, labels_encoding, max_length),train_labels)))\n",
        "y_val = np.array(list(map(lambda string: encode_string_of_labels(string, labels_encoding, max_length),val_labels)))\n",
        "y_test = np.array(list(map(lambda string: encode_string_of_labels(string, labels_encoding, max_length),test_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MypraiXCuOIb"
      },
      "source": [
        "# Models\n",
        "This section is used for creating different models, going from a baseline to slightly more complicated ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7EPmCLyuZVB"
      },
      "source": [
        "## Constants and utilities\n",
        "First of all, define some constants, parameter dictionaries and methods that will be reused by each architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__iSg0qcABw"
      },
      "source": [
        "N_CLASSES = len(labels_encoding) + 1  # number of classes to predict\n",
        "MAX_SEQUENCE_SIZE = max_length  # max sequence length (obtained from dataset)\n",
        "\n",
        "BATCH_SIZE = 8 \n",
        "EPOCHS = 100\n",
        "\n",
        "# Model common compile information\n",
        "# Use sparse_categorical_crossentropy because labels are one hot encoded\n",
        "model_compile_info = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': 'sparse_categorical_crossentropy',\n",
        "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
        "}\n",
        "\n",
        "# Model common training information\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience=10,\n",
        "                                                restore_best_weights=True)]\n",
        "}\n",
        "\n",
        "# Model common prediction information\n",
        "prediction_info = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'verbose': 1\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stq44tS8CJSy"
      },
      "source": [
        "Define utility methods that will be used to **create**, **train** and **test** the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE7Mvm9sZ8Pf"
      },
      "source": [
        "def create_model(name,\n",
        "                 layer_list, \n",
        "                 compile_info, \n",
        "                 show_summary=True) -> keras.Model:\n",
        "    \"\"\"\n",
        "    Create the model using the layers passed as parameters.\n",
        "    After the creation, the model is compiled and its summary is possibly \n",
        "    printed to console.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers : array\n",
        "        Array that contains a list of layers that must be added \n",
        "        to the model.\n",
        "    compile_info: Dictionary\n",
        "        Contains information required for compiling the model.\n",
        "    show_summary: bool\n",
        "        If true, then the summary of the model will be printed to console\n",
        "    \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : keras.Model\n",
        "        The keras model.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential(name=name)\n",
        "    \n",
        "    for idx, layer in enumerate(layer_list):\n",
        "\n",
        "        # Sanity checks for being sure that the last layer has been \n",
        "        # correctly set\n",
        "        if idx == len(layer_list) - 1:\n",
        "            assert isinstance(layer, layers.TimeDistributed)\n",
        "            assert layer.layer.activation == keras.activations.softmax, 'Wrong activation function'\n",
        "            assert layer.layer.units == N_CLASSES, 'Wrong number of units'\n",
        "\n",
        "        model.add(layer)\n",
        "\n",
        "    # Compile\n",
        "    model.compile(**compile_info)\n",
        "\n",
        "    # Print model summary\n",
        "    if show_summary:\n",
        "        model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def train_model(model: keras.Model,\n",
        "                x_train: np.ndarray,\n",
        "                y_train: np.ndarray,\n",
        "                x_val: np.ndarray,\n",
        "                y_val: np.ndarray,\n",
        "                training_info: dict) -> keras.Model:\n",
        "    \"\"\"\n",
        "    Training routine for the Keras model.\n",
        "    At the end of the training, retrieved History data is shown.\n",
        "\n",
        "    :param model: Keras built model\n",
        "    :param x_train: training data in np.ndarray format\n",
        "    :param y_train: training labels in np.ndarray format\n",
        "    :param x_val: validation data in np.ndarray format\n",
        "    :param y_val: validation labels in np.ndarray format\n",
        "    :param training_info: dictionary storing model fit() argument information\n",
        "\n",
        "    :return\n",
        "        model: trained Keras model\n",
        "    \"\"\"\n",
        "    print(\"Start training! \\nParameters: {}\".format(training_info))\n",
        "    history = model.fit(x=x_train, y=y_train,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        **training_info)\n",
        "    print(\"Training completed! Showing history...\")\n",
        "\n",
        "    show_history(history)\n",
        "\n",
        "    return model\n",
        "\n",
        "def show_history(history: keras.callbacks.History):\n",
        "    \"\"\"\n",
        "    Shows training history data stored by the History Keras callback\n",
        "\n",
        "    :param history: History Keras callback\n",
        "    \"\"\"\n",
        "    print(history.history)\n",
        "    history_data = history.history\n",
        "    print(\"Displaying the following history keys: \", history_data.keys())\n",
        "\n",
        "    for key, value in history_data.items():\n",
        "        if not key.startswith('val'):\n",
        "            fig, ax = plt.subplots(1, 1)\n",
        "            ax.set_title(key)\n",
        "            ax.plot(value)\n",
        "            if 'val_{}'.format(key) in history_data:\n",
        "                ax.plot(history_data['val_{}'.format(key)])\n",
        "            else:\n",
        "                print(\"Couldn't find validation values for metric: \", key)\n",
        "\n",
        "            ax.set_ylabel(key)\n",
        "            ax.set_xlabel('epoch')\n",
        "            ax.legend(['train', 'val'], loc='best')\n",
        "\n",
        "    val_accuracies = history_data['val_sparse_categorical_accuracy']\n",
        "    best_val_epoch = np.argmax(val_accuracies)\n",
        "    best_val_acc = val_accuracies[best_val_epoch]\n",
        "\n",
        "    print(f'Best validation accuracy: {best_val_acc} obtained at epoch: {best_val_epoch}')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def test_model(model: keras.Model,\n",
        "               labels: np.array,\n",
        "               x_test: np.array,\n",
        "               y_test: np.array):\n",
        "        \n",
        "    test_predictions = predict_data(model=model, \n",
        "                                    x=x_test,\n",
        "                                    prediction_info=prediction_info)\n",
        "\n",
        "    # Retrieving labels from raw predictions\n",
        "    test_predictions = np.argmax(test_predictions, axis=-1)\n",
        "\n",
        "    \n",
        "    \n",
        "    # Evaluation\n",
        "    metrics = [\n",
        "        partial(f1_score, average='macro', labels=labels),\n",
        "        partial(f1_score, average='micro', labels=labels),\n",
        "        partial(f1_score, average='weighted', labels=labels)\n",
        "    ]\n",
        "\n",
        "    metric_names = [\n",
        "        \"macro_f1\",\n",
        "        \"micro_f1\",\n",
        "        \"weighted_f1\"\n",
        "    ]\n",
        "\n",
        "    metric_info, y_pred = evaluate_predictions(\n",
        "                                    predictions=test_predictions,\n",
        "                                    y=y_test,\n",
        "                                    metrics=metrics,\n",
        "                                    metric_names=metric_names)\n",
        "\n",
        "    return metric_info, y_pred\n",
        "\n",
        "def predict_data(model: keras.Model,\n",
        "                 x: np.ndarray,\n",
        "                 prediction_info: dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Inference routine of a given input set of examples\n",
        "\n",
        "    :param model: Keras built and possibly trained model\n",
        "    :param x: input set of examples in np.ndarray format\n",
        "    :param prediction_info: dictionary storing model predict() argument information\n",
        "\n",
        "    :return\n",
        "        predictions: predicted labels in np.ndarray format\n",
        "    \"\"\"\n",
        "\n",
        "    print('Starting prediction: \\n{}'.format(prediction_info))\n",
        "    print('Predicting on {} samples'.format(x.shape[0]))\n",
        "\n",
        "    predictions = model.predict(x, **prediction_info)\n",
        "    return predictions\n",
        "\n",
        "def evaluate_predictions(predictions: np.ndarray,\n",
        "                         y: np.ndarray,\n",
        "                         metrics: List[Callable],\n",
        "                         metric_names: List[str]):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Evaluates given model predictions on a list of metric functions.\n",
        "    Moreover, this function is responsible for clearning the labels\n",
        "    from punctuations.\n",
        "\n",
        "    :param predictions: model predictions in np.ndarray format\n",
        "    :param y: ground-truth labels in np.ndarray format\n",
        "    :param metrics: list of metric functions\n",
        "    :param metric_names: list of metric names\n",
        "\n",
        "    :return\n",
        "        metric_info: dictionary containing metric values for each input metric\n",
        "        y_no_punct: dictionary containing the y_true and y_pred without \n",
        "            punctuation\n",
        "    \"\"\"\n",
        "    \n",
        "    assert len(metrics) == len(metric_names)\n",
        "\n",
        "    # Flatten is required because:\n",
        "    # - each time step must be classified and has its own prediction\n",
        "    # - each time step has also its own true label\n",
        "    # - f1_score cannot be computed for multiclass-multioutput \n",
        "    y_pred_flattened = predictions.flatten()\n",
        "    y_true_flattened = y.flatten()\n",
        "\n",
        "    assert len(y_pred_flattened) == len(y_true_flattened)\n",
        "    \n",
        "    print(f'Prediction evaluation started...')\n",
        "\n",
        "    metric_info = {}\n",
        "    for metric, metric_name in zip(metrics, metric_names):\n",
        "        \n",
        "        metric_value = metric(y_pred=y_pred_flattened, y_true=y_true_flattened)\n",
        "        # metric_value = metric(y_true_no_punct, y_pred_no_punct)\n",
        "        metric_info[metric_name] = metric_value\n",
        "\n",
        "    return metric_info, predictions\n",
        "\n",
        "def model_sanity_check(model: keras.Model, \n",
        "                       use_embedding_layer: bool = False):\n",
        "    \"\"\"\n",
        "    Create a random input_tensor and try to pass through the model.\n",
        "    This method should be used in order to check if the model is \n",
        "    working as expected.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : keras.Model\n",
        "        The model that must be tested.\n",
        "    use_embedding_layer: depending on this flag the shape of the input must be \n",
        "        treated differently.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f'Sanity check for the model with name: {model.name}')\n",
        "    # Model sanity check for seeing if it runs correctly\n",
        "    if use_embedding_layer:\n",
        "        input_tensor = np.random.uniform(\n",
        "            size=(BATCH_SIZE, MAX_SEQUENCE_SIZE)\n",
        "            )\n",
        "    else:\n",
        "        input_tensor = np.random.uniform(\n",
        "            size=(BATCH_SIZE, MAX_SEQUENCE_SIZE, EMBEDDING_SIZE)\n",
        "            )\n",
        "    print(f'Input tensor shape: {input_tensor.shape}')\n",
        "    output_tensor = model(input_tensor)\n",
        "    print(f'Output tensor shape: {output_tensor.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPDiCsbl9ea"
      },
      "source": [
        "Define utility methods for **creating layers** in order to: \n",
        "* reduce the code verbosity.\n",
        "* be sure to always create different architectures with the same layer structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rCmJJgpmBRY"
      },
      "source": [
        "# EMBEDDING\n",
        "# NOTE: Actually this layer has not been used in the final models, \n",
        "# but it has been used for some experimentations\n",
        "def embedding_layer(embedding_weights: np.array,\n",
        "                    layer_name: str='embedding') -> layers.Embedding:\n",
        "    \"\"\"\n",
        "    Create an embedding layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embedding_weights : np.array\n",
        "        The weights for the embedding layer.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Embedding\n",
        "        The created embedding layer.\n",
        "    \"\"\"\n",
        "    layer = layers.Embedding(\n",
        "        input_dim=VOCABULARY_SIZE, \n",
        "        output_dim=EMBEDDING_SIZE, \n",
        "        input_length=MAX_SEQUENCE_SIZE,\n",
        "        weights=[embedding_weights],\n",
        "        mask_zero=True,\n",
        "        name=layer_name\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# MASKING\n",
        "def masking_layer(input_shape: tuple,\n",
        "                  mask_value: float=0.0,\n",
        "                  layer_name: str='masking') -> layers.Masking:\n",
        "    \"\"\"\n",
        "    Create a masking layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_shape : tuple\n",
        "        The weights for the embedding layer.\n",
        "    mask_value : the value to mask because it represents the padding \n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Masking\n",
        "        The created masking layer.\n",
        "    \"\"\"\n",
        "    layer = layers.Masking(\n",
        "        input_shape=input_shape, \n",
        "        mask_value=mask_value,\n",
        "        name=layer_name\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "#RNN (LSTM and GRU)\n",
        "def _rnn_size(layer_depth: int) -> int:\n",
        "    \"\"\"\n",
        "    Simple logic used for assigning the number of units \n",
        "    to the rnn layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    size : int\n",
        "        The number units.\n",
        "    \"\"\"\n",
        "    size = 64\n",
        "    if layer_depth > 1:\n",
        "        size = 128\n",
        "    return size\n",
        "\n",
        "def bilstm_layer(layer_depth: int,\n",
        "                 layer_name: str='bi-lstm') -> layers.Bidirectional:\n",
        "    \"\"\"\n",
        "    Create a bidirectional lstm layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Bidirectional\n",
        "        The created bidirectional lstm layer.\n",
        "    \"\"\"\n",
        "    size = _rnn_size(layer_depth)\n",
        "    layer = layers.Bidirectional(\n",
        "        layers.LSTM(size, \n",
        "                    return_sequences=True, \n",
        "                    activation='tanh'),\n",
        "                    name=layer_name,\n",
        "                    \n",
        "        )\n",
        "    return layer\n",
        "\n",
        "def bigru_layer(layer_depth: int,\n",
        "                layer_name: str='bi-gru') -> layers.Bidirectional:\n",
        "    \"\"\"\n",
        "    Create a bidirectional gru layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Bidirectional\n",
        "        The created bidirectional gru layer.\n",
        "    \"\"\"\n",
        "    size = _rnn_size(layer_depth)\n",
        "    layer = layers.Bidirectional(\n",
        "        layers.GRU(size, \n",
        "                   return_sequences=True, \n",
        "                   activation='tanh'),\n",
        "                   name=layer_name\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "#DENSE\n",
        "def _dense_size(last_layer:bool) -> int:\n",
        "    \"\"\"\n",
        "    Simple logic for assigning the size of the dense layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    size : int\n",
        "        The size of the dense layer.\n",
        "    \"\"\"\n",
        "    size = N_CLASSES\n",
        "    if not last_layer:\n",
        "        size = 256\n",
        "    return size\n",
        "\n",
        "def _dense_activation(last_layer:bool) -> str:\n",
        "    \"\"\"\n",
        "    Simple logic for assigning the activation function of the dense layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    activation : str\n",
        "        The activation function of the layer.\n",
        "    \"\"\"\n",
        "    activation = 'tanh'\n",
        "    if last_layer:\n",
        "        activation = 'softmax'\n",
        "    return activation\n",
        "\n",
        "def time_distributed_dense_layer(last_layer:bool,\n",
        "                layer_name: str='dense') -> layers.Dense:\n",
        "    \"\"\"\n",
        "    Create a dense layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Dense\n",
        "        The created dense layer.\n",
        "    \"\"\"\n",
        "    size = _dense_size(last_layer)\n",
        "    activation = _dense_activation(last_layer)\n",
        "\n",
        "    return layers.TimeDistributed(\n",
        "        layers.Dense(size, activation=activation),\n",
        "        name=layer_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGt1V4r8Ihda"
      },
      "source": [
        "## Creation and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOhK3xp2FCUu"
      },
      "source": [
        "### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoxVUVMap5Ma",
        "collapsed": true
      },
      "source": [
        "# Create layers\n",
        "baseline_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE), \n",
        "                              layer_name='masking_0'),\n",
        "                bilstm_layer(layer_depth=1, \n",
        "                             layer_name='bi-lstm_0'),\n",
        "                time_distributed_dense_layer(last_layer=True, \n",
        "                                             layer_name='dense_0')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_model = create_model('baseline', \n",
        "                              baseline_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run (used only for debug purpose)\n",
        "# model_sanity_check(baseline_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blvImsmFPiOY"
      },
      "source": [
        "baseline_model = train_model(baseline_model, X_train, y_train, X_val, y_val, training_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LGdswmdFg6x"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4l5n888vLql"
      },
      "source": [
        "### Variations\n",
        "What follows is the implementation of small variations to the baseline architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMvk3AQFukOh"
      },
      "source": [
        "#### GRU\n",
        "Change the LSTM layer with the GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "856hGaR9eDRb"
      },
      "source": [
        "# Create layers\n",
        "baseline_var1_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE),\n",
        "                              layer_name='masking_0'),\n",
        "                bigru_layer(layer_depth=1,\n",
        "                            layer_name='bi-gru_0'),\n",
        "                time_distributed_dense_layer(last_layer=True,\n",
        "                                             layer_name='dense_0')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var1_model = create_model('baseline_var1', \n",
        "                              baseline_var1_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run (used only for debug purpose)\n",
        "# model_sanity_check(baseline_var1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t86Neznrbml1"
      },
      "source": [
        "baseline_var1_model = train_model(baseline_var1_model, X_train, y_train, X_val, y_val, training_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9We6j_USupzN"
      },
      "source": [
        "#### Additional LSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLHH94CtqZ7c"
      },
      "source": [
        "# Create layers\n",
        "baseline_var2_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE),\n",
        "                              layer_name='masking_0'),\n",
        "                bilstm_layer(layer_depth=1,\n",
        "                             layer_name='bi-lstm_0'),\n",
        "                bilstm_layer(layer_depth=2,\n",
        "                             layer_name='bi-lstm_1'),\n",
        "                time_distributed_dense_layer(last_layer=True,\n",
        "                                             layer_name='dense_0')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var2_model = create_model('baseline_var2', \n",
        "                              baseline_var2_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run (used only for debug purpose)\n",
        "# model_sanity_check(baseline_var2_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juCpv5oxdCDZ"
      },
      "source": [
        "baseline_var2_model = train_model(baseline_var2_model, X_train, y_train, X_val, y_val, training_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RN6ySWyu13J"
      },
      "source": [
        "#### Additional Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujfM5mekqma8"
      },
      "source": [
        "# Create layers\n",
        "baseline_var3_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE),\n",
        "                              layer_name='masking_0'),\n",
        "                bilstm_layer(layer_depth=1,\n",
        "                             layer_name='bi-lstm_0'),\n",
        "                time_distributed_dense_layer(last_layer=False,\n",
        "                                             layer_name='dense_0'),\n",
        "                time_distributed_dense_layer(last_layer=True,\n",
        "                                             layer_name='dense_1')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var3_model = create_model('baseline_var3', \n",
        "                              baseline_var3_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run (used only for debug purpose)\n",
        "# model_sanity_check(baseline_var3_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSWlfkPHdMWE"
      },
      "source": [
        "baseline_var3_model = train_model(baseline_var3_model, X_train, y_train, X_val, y_val, training_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwOShFdwvaHN"
      },
      "source": [
        "## Test\n",
        "Now proceed with testing the two best models found so far.\n",
        "\n",
        "From our experiments, we have noticed that there isn't a big difference in the final accuracy:\n",
        "* the *baseline* model is the slower to train, but can reach eventually a good accuracy\n",
        "* the *baseline variations* give pretty similar results, and can reach higher accuracy in a less amount of time. \n",
        "* Those that gave us the best results are the models where we added an additional BI-LSTM layer and an additional Dense layer, respectively.\n",
        "\n",
        "What follow is a list of utility methods used for testing the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYdfufazIzHm"
      },
      "source": [
        "def create_testing_labels():\n",
        "    \"\"\"\n",
        "    Method for creating a list of labels indices and label descriptions \n",
        "    that must be used for testing.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    labels : np.array\n",
        "        List of labels (indices)\n",
        "    labels_name: np.array\n",
        "        Description of labels\n",
        "    \"\"\"\n",
        "\n",
        "    labels = []\n",
        "    labels_name = []\n",
        "    for word, token in tqdm(labels_encoding.items()):\n",
        "        if token != PADDING:\n",
        "            if not is_punctuation_label(word):\n",
        "                labels.append(token)\n",
        "                labels_name.append(decode_label(token, labels_encoding))\n",
        "    return labels, labels_name\n",
        "\n",
        "# labels (indices and descriptions) used for testing\n",
        "testing_labels, testing_label_names = create_testing_labels()\n",
        "\n",
        "def show_f1_scores(metric_info):\n",
        "    \"\"\"\n",
        "    Method for creating a list of labels that will be used for testing.\n",
        "    \n",
        "    Parameters\n",
        "    -------\n",
        "    metric_info : dict\n",
        "        Dictionary that contains the f1 scores\n",
        "    \n",
        "    \"\"\"\n",
        "    print()\n",
        "    print('F1 SCORES:')\n",
        "    print(f'  macro: {metric_info[\"macro_f1\"]}')\n",
        "    print(f'  micro: {metric_info[\"micro_f1\"]}')\n",
        "    print(f'  weighted: {metric_info[\"weighted_f1\"]}')\n",
        "    print()\n",
        "\n",
        "def show_classification_report(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Method that prints the classification report\n",
        "    \n",
        "    Parameters\n",
        "    -------\n",
        "    y_true : np.array\n",
        "        Array of true labels\n",
        "    y_pred : np.array\n",
        "        Array of predicted labels\n",
        "    \"\"\"\n",
        "\n",
        "    print(classification_report(\n",
        "        y_true, \n",
        "        y_pred, \n",
        "        labels=testing_labels,\n",
        "        target_names=testing_label_names\n",
        "        ))\n",
        "\n",
        "def show_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Method that shows the confusion matrix.\n",
        "    \n",
        "    Parameters\n",
        "    -------\n",
        "    y_true : np.array\n",
        "        Array of true labels\n",
        "    y_pred : np.array\n",
        "        Array of predicted labels\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(20,20))\n",
        "    ConfusionMatrixDisplay.from_predictions(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        normalize='true', \n",
        "        cmap=plt.cm.Blues, \n",
        "        values_format=\".2f\",\n",
        "        labels=testing_labels,\n",
        "        display_labels=testing_label_names,\n",
        "        xticks_rotation='vertical',\n",
        "        ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee-ftu64OChy"
      },
      "source": [
        "###Test model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dWDf07E1WUb"
      },
      "source": [
        "# Test first model and print results\n",
        "metric_info, y_pred = test_model(\n",
        "    baseline_model, \n",
        "    testing_labels,\n",
        "    X_test, \n",
        "    y_test)\n",
        "\n",
        "show_f1_scores(metric_info)\n",
        "show_classification_report(y_test.flatten(), y_pred.flatten())\n",
        "show_confusion_matrix(y_test.flatten(), y_pred.flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6tbpGjfOIWE"
      },
      "source": [
        "###Test model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvAjGfsiAhJZ"
      },
      "source": [
        "# Test second model and print results\n",
        "metric_info, y_pred = test_model(\n",
        "    baseline_var3_model, \n",
        "    testing_labels,\n",
        "    X_test, \n",
        "    y_test)\n",
        "\n",
        "show_f1_scores(metric_info)\n",
        "show_classification_report(y_test.flatten(), y_pred.flatten())\n",
        "show_confusion_matrix(y_test.flatten(), y_pred.flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q08POct6hvwe"
      },
      "source": [
        "# Discussion and Error Analysis\n",
        "In this part there is a small recap of all the tests that have been perfomed, and the explanation of some choices that have been made.\n",
        "\n",
        "Finally, there are some considerations about the models that we selected for the final testing, and a comparison with the results obtained on the validation set.\n",
        "\n",
        "## Hyper-parameters and tenchniques\n",
        "The quantity of possible hyper-parameters and tenchniques combination is huge, e.g., the number of rnn hidden layers, the number of units for dense layers, the batch size, the normalization techniques, etc...\n",
        "\n",
        "We fixed some of these based on the literature, and the we focused our attention mainly on the following ones:\n",
        "* activation function\n",
        "* dataset split: documents vs sentences\n",
        "* GloVe dimension\n",
        "* OOV strategies\n",
        "\n",
        "We tuned these hyper-parameters and tenchniques on the baseline model. Only once we've found the best combination, then we have implemented the model variations.\n",
        "\n",
        "### Activation function\n",
        "At the beginning we used the 'ReLu' activation functions in all layers, but the last Dense.\n",
        "We discovered that this choice had two problems:\n",
        "* ReLu is not supported from cuDNN kernel, which is the one that gives the higher computational speed\n",
        "* the number of time steps is pretty large with the dataset splitted into documents, and this was causing the network to become untrainable due to vanishing gradients.\n",
        "\n",
        "Eventually, we decided to use the **Tanh** activation function which was able to overcome all the problems mentioned above.\n",
        "\n",
        "### Dataset split: sentences vs documents\n",
        "In the code there is the flag 'USE_DOCUMENTS' that allows to switch easily from one split to another.\n",
        "\n",
        "When splitted into **sentences**, the model performs better.\n",
        "\n",
        "**Why?**\n",
        "\n",
        "When splitting the *train* dataset into **documents** we have remarked that only ~30% of documents is using the full max_length (computed through quantile technique seen during the tutorials), and then there is a lot of variability in the input length.\n",
        "\n",
        "Whereas, when we split the *train* dataset into **sentences**, the percentage of documents able to reach the full max_length is ~70%.\n",
        "\n",
        "Very similar reasonings can be done for the *test* set.\n",
        "\n",
        "Our intuition is that the model would require a larger dataset in order to better cope with very different input lengths."
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRJJ4tzaC852"
      },
      "source": [
        "# Preliminary Steps\n",
        "These are some preliminary steps before addressing the task. Import some basic libraries and set a variable that will be used in multiple steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCTmlk1C0Sm"
      },
      "source": [
        "import os, sys\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# typing\n",
        "from typing import List, Callable, Dict"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vspH9UzptOFT"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reOmnUrPjRzt"
      },
      "source": [
        "## Constant and utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBylTiJGjVZe"
      },
      "source": [
        "DATASET_NAME = \"dependency_treebank\"\n",
        "DOCUMENT_EXTENSION = \".dp\"\n",
        "\n",
        "USE_DOCUMENTS = True #True=Use documents; False = Use sentences\n",
        "file_end_name = \"_documents\" if USE_DOCUMENTS else \"_sentences\"\n",
        "\n",
        "#List of paths to handle the dataset\n",
        "DATASET_PATHS = {\n",
        "    \"url\" : 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip', #url to dowload the dataset\n",
        "    \"dataset_folder\": os.path.join(os.getcwd(), \"Datasets\", \"Original\"), #folder containing the original dataset data\n",
        "    \"dataset_path\" : os.path.join(os.getcwd(), \"Datasets\", \"Original\", \"dependency_treebank.zip\"), #path to zipped dataset\n",
        "    \"documents_path\" : os.path.join(os.getcwd(), \"Datasets\", \"Original\", DATASET_NAME), #folder containing extracted documents (NB: it is created automatically during the extraction)\n",
        "    \"dataframe_folder\" : os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", DATASET_NAME), #folder containing the dataframe data\n",
        "    \"dataframe_path\" : os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", DATASET_NAME, DATASET_NAME + file_end_name + \".pkl\") #path to pickle save of built dataframe\n",
        "}\n",
        "\n",
        "TRAINING_DOCS = 100\n",
        "VALIDATION_DOCS = 50\n",
        "TEST_DOCS = 49"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr0llgPylm31"
      },
      "source": [
        "##Folder creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPThN5GWAZl"
      },
      "source": [
        "def create_folders(paths):\n",
        "  for path in paths:\n",
        "    if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "\n",
        "folders = [DATASET_PATHS[\"dataset_folder\"],\n",
        "           DATASET_PATHS[\"dataframe_folder\"]]\n",
        "create_folders(folders)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsjdOVHZtSnp"
      },
      "source": [
        "## Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR9RtRITt72b"
      },
      "source": [
        "def download_dataset(dataset_path):\n",
        "  if not os.path.exists(dataset_path):\n",
        "      urllib.request.urlretrieve(url, dataset_path)\n",
        "      \n",
        "      print(\"Successful download\")\n",
        "\n",
        "download_dataset(DATASET_PATHS[\"dataset_path\"])"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByKCopDN_Rb-"
      },
      "source": [
        "## Dataset Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz8xnlmy_XvE"
      },
      "source": [
        "def extract_dataset(dataset_path, dataset_folder, documents_path):\n",
        "  expected_docs_number = TRAINING_DOCS + VALIDATION_DOCS + TEST_DOCS\n",
        "\n",
        "  if not os.path.exists(documents_path) or len(os.listdir(documents_path))<expected_docs_number:\n",
        "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_folder)\n",
        "\n",
        "    print(\"Successful extraction\")\n",
        "\n",
        "extract_dataset(DATASET_PATHS[\"dataset_path\"],DATASET_PATHS[\"dataset_folder\"],DATASET_PATHS[\"documents_path\"])"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGVpE8IuCTIa"
      },
      "source": [
        "##Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ZbwHLECWPk"
      },
      "source": [
        "def preprocess(token : str) -> str:\n",
        "\t\"\"\"\n",
        "\tCalls the function that cleans the text\n",
        "\tInput: the string to process\n",
        "\tOutput: the processed string\n",
        "\t\"\"\"\n",
        "\ttoken = text_to_lower(token)\n",
        "\ttoken = strip_text(token)\n",
        "\treturn token\n",
        "\n",
        "def text_to_lower(text: str) -> str:\n",
        "\t\"\"\"\n",
        "\tReturns the string in lower character\n",
        "\tInput: the string to process\n",
        "\tOutput: the processed string\n",
        "\t\"\"\"\n",
        "\treturn text.lower();\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "\t\"\"\"\n",
        "\tRemoves any left or right spacing (including carriage return) from text.\n",
        "\tExample:\n",
        "\tInput: '  This assignment is cool\\n'\n",
        "\tOutput: 'This assignment is cool'\n",
        "\t\"\"\"\n",
        "\n",
        "\treturn text.strip()"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJbpdftCt8Gl"
      },
      "source": [
        "## Dataframe creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXgbC4TAuJ81"
      },
      "source": [
        "TOKEN_SEPARATOR = \" \" #Character used to separate tokens in the dataframe\n",
        "SENTENCE_SEPARATOR = \"##\" #Characters to signal the end of a sentence (if USE_DOCUMENTS=False)\n",
        "WORDS_CONTAINER = \"document\" if USE_DOCUMENTS else \"sentence\"\n",
        "\n",
        "def list_to_string(_list):\n",
        "\tstring = \"\"\n",
        "\n",
        "\tfor index, value in enumerate(_list):\n",
        "\t\tstring+=value\n",
        "\t\tif index!=len(_list)-1:\n",
        "\t\t\tstring+=TOKEN_SEPARATOR\n",
        "\t\n",
        "\treturn string\n",
        "\n",
        "def add_row_to_dataframe_rows(dataframe_rows, split, document, labels):\n",
        "\tif USE_DOCUMENTS:\n",
        "\t\tdataframe_row = {\"split\": split, WORDS_CONTAINER: document, \"labels\": labels}\n",
        "\t\tdataframe_rows.append(dataframe_row)\n",
        "\t\n",
        "\telse:\n",
        "\t\tsentences = document.split(SENTENCE_SEPARATOR)\n",
        "\t\tsplit_labels = labels.split(SENTENCE_SEPARATOR)\n",
        "\t\tfor i in range(len(sentences)):\n",
        "\t\t\tdataframe_row = {\"split\": split, WORDS_CONTAINER: sentences[i], \"labels\": split_labels[i]}\n",
        "\t\t\tdataframe_rows.append(dataframe_row)\n",
        "\t\n",
        "def rows_to_dataframe(rows):\n",
        "\tdataframe = pd.DataFrame(rows)\n",
        "\tdataframe = dataframe[[\"split\", WORDS_CONTAINER, \"labels\"]]\n",
        "\n",
        "\treturn dataframe\n",
        "\n",
        "def get_documents(path):\n",
        "\tfiles = os.listdir(path)\n",
        "\tdocuments = filter(lambda name: (name.endswith(DOCUMENT_EXTENSION)), files)\n",
        "\tdocuments = list(documents)\n",
        "\tdocuments.sort()\n",
        " \n",
        "\treturn documents\n",
        " \n",
        "def get_document_number(filename):\n",
        "\treturn int(filename.split(\"_\")[1].split(\".\")[0])\n",
        " \n",
        "def extract_data_from_line(line):\n",
        "\tif line != \"\\n\":\n",
        "\t\tcolumns = line.split()\n",
        "\t\ttoken = columns[0]\n",
        "\t\ttoken = preprocess(token)\n",
        "\t\tlabel = columns[1]\n",
        "\n",
        "\t\treturn token, label\n",
        "\n",
        "\telse:\n",
        "\t\tif USE_DOCUMENTS:\n",
        "\t\t\treturn None, None\n",
        "\t\telse:\n",
        "\t\t\treturn SENTENCE_SEPARATOR, SENTENCE_SEPARATOR\n",
        "\n",
        "def process_document(document, doc_number):\n",
        "\ttokens = []\n",
        "\tlabels = []\n",
        "\tsplit = \"\"\n",
        "\n",
        "\ttry:\n",
        "\t\tif os.path.isfile(document):\n",
        "\t\t\t#Open the file\n",
        "\t\t\twith open(document, mode='r', encoding='utf-8') as text_file:\n",
        "\n",
        "\t\t\t\t#Split in different groups\n",
        "\t\t\t\tif doc_number <= TRAINING_DOCS:\n",
        "\t\t\t\t\tsplit = \"train\"\n",
        "\t\t\t\telif doc_number <= TRAINING_DOCS+VALIDATION_DOCS:\n",
        "\t\t\t\t\tsplit = \"validation\"\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tsplit = \"test\"\n",
        "\n",
        "\t\t\t\t#Stop at end of file\n",
        "\t\t\t\tfor line in text_file:\n",
        "\t\t\t\t\ttoken, label = extract_data_from_line(line)\n",
        "\n",
        "\t\t\t\t\tif token!=None and label!=None:\n",
        "\t\t\t\t\t\ttokens.append(token)\n",
        "\t\t\t\t\t\tlabels.append(label)\n",
        "\t\t \n",
        "\texcept Exception as e:\n",
        "                print('Failed to process %s. Reason: %s' % (document, e))\n",
        "                sys.exit(0)\n",
        "\n",
        "\treturn split, list_to_string(tokens), list_to_string(labels)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQECWv5YD28l"
      },
      "source": [
        "###Build/Load Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8ePHxt0D74q"
      },
      "source": [
        "def build_dataframe(documents_path,dataframe_path, save=True):\n",
        "\n",
        "\tdocuments = get_documents(documents_path)\n",
        "\tdataframe_rows = []\n",
        "\n",
        "\t#Iterate along the files\n",
        "\tfor filename in documents:\n",
        "\t\tdocument = os.path.join(documents_path, filename)\n",
        "\t\tdoc_number = get_document_number(filename)\n",
        "\n",
        "\t\tsplit, tokens, labels = process_document(document, doc_number)\n",
        "\t\tadd_row_to_dataframe_rows(dataframe_rows,split,tokens,labels)\n",
        "\n",
        "\t#Transform the list of rows in a proper dataframe\n",
        "\tdataframe = rows_to_dataframe(dataframe_rows)\n",
        "\tprint(\"Dataframe Built successfully\")\n",
        "\t\n",
        "\t#Save the dataframe\n",
        "\tif save:\n",
        "\t\tdataframe.to_pickle(dataframe_path)\n",
        "\t\tprint(\"Dataframe saved successfully\")\n",
        " \n",
        "\treturn dataframe\n",
        "\n",
        "def load_dataframe(documents_path, dataframe_path, force_rebuild = False):\n",
        "\tif not os.path.exists(dataframe_path) or force_rebuild:\n",
        "\t\treturn build_dataframe(documents_path, dataframe_path)\n",
        "\telse:\n",
        "\t\treturn pd.read_pickle(dataframe_path)"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMM5D7ZTeBGJ",
        "outputId": "e8e89273-ae84-40a2-abd7-588950435052"
      },
      "source": [
        "df = load_dataframe(DATASET_PATHS[\"documents_path\"],DATASET_PATHS[\"dataframe_path\"], True)\n",
        "#Test\n",
        "print(df.iloc[1][WORDS_CONTAINER])\n",
        "print(df.iloc[1][\"labels\"])"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe Built successfully\n",
            "Dataframe saved successfully\n",
            " mr. vinken is chairman of elsevier n.v. , the dutch publishing group .\n",
            " NNP NNP VBZ NN IN NNP NNP , DT NNP VBG NN .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Co7hfK2uKD8"
      },
      "source": [
        "# GloVe\n",
        "This section is the one responsible for the implementation of the GloVe embedding system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8qLxw1kcTto"
      },
      "source": [
        "## Constants and utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqG3dXiBcdeA"
      },
      "source": [
        "#Requested variables from the pre-processing methods:\n",
        "VOCAB_SIZE = 1000 #Temporary, it should be defined above\n",
        "words_to_tokens = {} #Dictionary that associates each word from the training+validation+test set with a token\n",
        "training_word = [] #List of unique words from the training set (or a method to get it)\n",
        "validation_words = [] #List of unique words from the validation set (or a method to get it)\n",
        "test_words = [] #List of unique words from the test set (or a method to get it)\n",
        "training_sentences = [] #List of sentences/documents from the training set (or a method to get it)\n",
        "validation_sentences = [] #List of sentences/documents from the validation set (or a method to get it)\n",
        "test_sentences = [] #List of sentences/documents from the test set (or a method to get it)\n",
        "#NB: a sentence or a document should be a List of words\n",
        "#---------------------------------------------------\n",
        "\n",
        "URL_BASE = \"https://nlp.stanford.edu/data\" #Location of the pre-trained GloVe's files\n",
        "GLOVE_VERSION = \"6B\"\n",
        "\n",
        "EMBEDDING_SIZE = 50 #The dimensionality of the embeddings; to be tested\n",
        "\n",
        "#List of paths to download and extract GloVe's files\n",
        "PATHS = {\n",
        "    \"url\": URL_BASE + \"/glove.\" + GLOVE_VERSION + \".zip\",\n",
        "    \"glove_path\": os.path.join(os.getcwd(),\"Glove\",GLOVE_VERSION),\n",
        "    \"glove_zip\": os.path.join(os.getcwd(),\"Glove\", GLOVE_VERSION, \"glove.\"+GLOVE_VERSION+\".zip\"),\n",
        "    \"glove_file\": os.path.join(os.getcwd(),\"Glove\", GLOVE_VERSION, \"glove.\"+GLOVE_VERSION+\".\"+str(EMBEDDING_SIZE)+\"d.txt\")\n",
        "}\n",
        "\n",
        "OOV_METHOD = \"Mean\" #Determine which OOV method to adopt; choose one between \"Mean\", \"Random\" and \"Placeholder\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIY-9Qf_bOU5"
      },
      "source": [
        "## Download\n",
        "In this part the presence of the GloVe file is checked. In case of a negative response, it will be downloaded and extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r14RFpFCbFn2"
      },
      "source": [
        "def setup_files():\n",
        "\n",
        "  '''\n",
        "  Create the folder if it does not exist.\n",
        "  Then download the zip file from the web archive if it does not exist.\n",
        "  Finally exctract the zip file of the GloVe txt file does not exist in the folder.\n",
        "  '''\n",
        "\n",
        "  if not os.path.exists(PATHS[\"glove_path\"]):\n",
        "    os.makedirs(PATHS[\"glove_path\"])\n",
        "\n",
        "  if not os.path.exists(PATHS[\"glove_file\"]):\n",
        "    if not os.path.exists(PATHS[\"glove_zip\"]):\n",
        "      download_glove(PATHS[\"url\"])\n",
        "\n",
        "    extract_glove(PATHS[\"glove_zip\"],PATHS[\"glove_path\"])\n",
        "\n",
        "def download_glove(url: str):\n",
        "\n",
        "    '''\n",
        "    Download GloVe's zip file from the web.\n",
        "    '''\n",
        "\n",
        "    urllib.request.urlretrieve(url, PATHS['glove_zip'])\n",
        "    print(\"Successful download\")\n",
        "\n",
        "def extract_glove(zip_file: str,\n",
        "                  glove_path: str):\n",
        "  \n",
        "    '''\n",
        "    Extract GloVe's zip file.\n",
        "    '''\n",
        "  \n",
        "    with zipfile.ZipFile(PATHS[\"glove_zip\"], 'r') as zip_ref:\n",
        "      zip_ref.extractall(path=PATHS[\"glove_path\"])\n",
        "      print(\"Successful extraction\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYhDkgNQejX3"
      },
      "source": [
        "## Initialization\n",
        "In this step, the downloaded GloVe file is loaded into an embedding vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfLDOt7le78v"
      },
      "source": [
        "def load_model(glove_file: str) ->Dict:\n",
        "\n",
        "  '''\n",
        "  Open GloVe's txt file and store each of its contained words\n",
        "  into a dictionary along with their correspondent embedding weights.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  glove_file : str\n",
        "      GloVe's txt file path.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  vocabulary: Dict\n",
        "      GloVe's vocabulary\n",
        "\n",
        "  '''\n",
        "\n",
        "  print(\"Loading GloVe Model...\")\n",
        "\n",
        "  with open(glove_file, encoding=\"utf8\" ) as f: #Open the txt file\n",
        "      lines = f.readlines() #Read the file line by line\n",
        "\n",
        "  vocabulary = {}\n",
        "  for line in lines:\n",
        "      splits = line.split()\n",
        "      #Save the first part of the line (word) as the dictionary's key and the second part (the embedding) as the key\n",
        "      vocabulary[splits[0]] = np.array([float(val) for val in splits[1:]])\n",
        "\n",
        "  print(\"GloVe model loaded\")\n",
        "\n",
        "  return vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tRukt6Kxx4R"
      },
      "source": [
        "## OOV\n",
        "In this section, some possible \"Out Of Vocabulary\" handling methods are implemented, along with other OOV-related functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-uwA9DYuNtr"
      },
      "source": [
        "#OOV-handling: possible methods\n",
        "def random_embedding(embedding_size: int) ->np.array:\n",
        "  '''\n",
        "  Return a numpy array with random values sampled from a uniform distribution\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  embedding_size: int\n",
        "    The embedding size that is used as the size of the numpy array.\n",
        "\n",
        "  Results:\n",
        "  -------\n",
        "  np.array\n",
        "  A randomized numpy array.\n",
        "  '''\n",
        "\n",
        "  return np.random.uniform(low=-0.05, high=0.05, size=embedding_size)\n",
        "\n",
        "def placeholder_embedding(embedding_size: int) ->np.ndarray:\n",
        "  '''\n",
        "    Return a numpy array with all zeros\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    embedding_size: int\n",
        "      The embedding size that is used as the size of the numpy array.\n",
        "\n",
        "    Results:\n",
        "    -------\n",
        "    np.array\n",
        "    A numpy array filled with zeros.\n",
        "    '''\n",
        "\n",
        "  return np.zeros(shape=(embedding_size))\n",
        "\n",
        "def neighbours_mean_embedding(word: str,\n",
        "                              glove_embedding: Dict[str,int],\n",
        "                              sentences: List[List[str]]):\n",
        "  \n",
        "  '''\n",
        "  Compute the embedding of an OOV word by taking the mean\n",
        "  of its neighbours.\n",
        "\n",
        "  Parameters:\n",
        "  ---------\n",
        "  word: str\n",
        "      The OOV that needs to be embedded.\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding.\n",
        "  sentences: List[List[str]]\n",
        "      A list of all the sentences (lists of words) in the current set.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  mean: int\n",
        "      The mean of the embedding values of OOV-word's neighbours.\n",
        "  '''\n",
        "  \n",
        "  neighbours = {}\n",
        "\n",
        "  for sentence in sentences:\n",
        "    if word in sentence:\n",
        "      index = sentence.index(word)\n",
        "      if index!=0:\n",
        "        left_neighbour = sentence[index-1]\n",
        "        if left_neighbour in glove_embedding: #Consider only words that are not OOV\n",
        "          neighbours.add(left_neighbour)\n",
        "      if index!=len(sentence-1):\n",
        "        right_neighbour = sentence[index+1]\n",
        "        if right_neighbour in glove_embedding: #Consider only words that are not OOV\n",
        "          neighbours.add(right_neighbour)\n",
        "\n",
        "  neighbours_embeddings = np.array([glove_embedding[neighbour] for neighbour in neighbours])\n",
        "  return np.mean(neighbours_embeddings)\n",
        "\n",
        "\n",
        "#Others\n",
        "def get_oov_list(words: List[str],\n",
        "                 glove_embedding: Dict[str, int]) ->List[str]:\n",
        "\n",
        "  '''\n",
        "  Return a list of all the words that are not part of the GloVe embedding\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  words: List[str]\n",
        "      A list of unique words from a set of documents.\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  oov: List[str]\n",
        "      A list of all the OOV terms.\n",
        "  '''\n",
        "\n",
        "  embedding_vocabulary = set(glove_embedding.keys())\n",
        "  oov = set(words).difference(embedding_vocabulary)\n",
        "  return list(oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emKY7vJ-z6Od"
      },
      "source": [
        "##Embedding matrix\n",
        "Now, having opted for an OOV method, it is possible to create the embedding matrix, which associates the embedding to the correspondent word for the entire vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m33pLFGG0ysd"
      },
      "source": [
        "def update_embeddings(glove_embedding: Dict[str, int],\n",
        "                     new_embeddings: Dict[str, int]):\n",
        "  \n",
        "  '''\n",
        "  Update the GloVe's embeddings by adding the new embeddings of\n",
        "  the previous OOV words.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding.\n",
        "  new_embeddings: Dict[str, int]\n",
        "      A dictionary containing the new embeddings\n",
        "      for the analyzed OOV words.\n",
        "  '''\n",
        "  \n",
        "  #Merge GloVe's embeddings with the new discoveries\n",
        "  glove_embedding = glove_embedding | new_embeddings\n",
        "\n",
        "def update_embedding_matrix(starting_embedding_matrix: np.ndarray,\n",
        "                            glove_embedding: Dict[str, int],\n",
        "                            embedding_size: int,\n",
        "                            words: List[str],\n",
        "                            words_to_tokens: Dict[str,int]\n",
        "                            oov_method: str,\n",
        "                            sentences: List[List[str]]) ->np.ndarray\n",
        "\n",
        "  '''\n",
        "  Build an embedding matrix updating the values of a previous matrix based on\n",
        "  a new set of sentences and an updated GloVe embedding.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  starting_embedding_matrix: np.ndarray\n",
        "      The starting embedding matrix.\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding, eventually updated in the previous step with the\n",
        "      previous OOV terms.\n",
        "  embedding_size: int\n",
        "      The dimensions of the embedding.\n",
        "  words: List[str]\n",
        "      A list of unique words in the set of sentences.\n",
        "  words_to_tokens: Dict[str, int]\n",
        "      A dictionary that associates each word with a token.\n",
        "  oov_method: str\n",
        "      The name of the OOV method to use to handle OOV cases.\n",
        "  sentences: List[List[str]]\n",
        "      A list of all the sentences (lists of words) in the current set.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  embedding_matrix: np.ndarray\n",
        "      The updated embedded matrix that associates each word of the vocabulary\n",
        "      with its corresponding embedding.\n",
        "  '''\n",
        "\n",
        "  oov_terms = get_oov_list(words,glove_embedding)\n",
        "  #A copy of the original matrix is returned\n",
        "  embedding_matrix = np.copy(starting_embedding_matrix)\n",
        "  discovered_embeddings = {}\n",
        "\n",
        "  for word in tqdm(words):\n",
        "\n",
        "    token = words_to_tokens[word]\n",
        "    if np.all((embedding_matrix[token] == 0)):\n",
        "\n",
        "      if word in oov_terms: #Hanlde the OOV case with one of the methods\n",
        "        if oov_method == \"Random\":\n",
        "          embedding_vector = random_embedding(embedding_size)\n",
        "        elif oov_method == \"Placeholder\":\n",
        "          embedding_vector = placeholder_embedding(embedding_size)\n",
        "        elif oov_method == \"Mean\":\n",
        "          embedding_vector = neighbours_mean_embedding(word, glove_embedding, sentences)\n",
        "        else:\n",
        "          raise \"Invalid OOV method\"\n",
        "        \n",
        "        discovered_embeddings[word] = embedding_vector\n",
        "\n",
        "      else:\n",
        "        embedding_vector = glove_embedding[word]\n",
        "\n",
        "      embedding_matrix[token] = embedding_vector #Update the embedding matrix\n",
        "\n",
        "  #The computed values for the OOV words update the GloVe embeddings at the end of the process.\n",
        "  #Updating these values at runtime affects the \"Mean\" OOV method.\n",
        "  update_embeddings(glove_embedding, discovered_embeddings)\n",
        "\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo7AfoSdCfYO"
      },
      "source": [
        "##Train, validation and test vocabularies\n",
        "Here all the previous methods defined in the above sections are exploited to create three different vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1BxcRw4CzJo"
      },
      "source": [
        "setup_files() #Create a path, download and extract the files, if necessary\n",
        "glove_vocab = load_model(PATHS[\"glove_file\"]) #Load the GloVe model\n",
        "\n",
        "embedding_matrix_0 = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE), dtype=np.float32) #Create an empty embedding matrix\n",
        "\n",
        "#Build the embedding matrix with the training set data\n",
        "embedding_matrix_training = update_embedding_matrix(embedding_matrix_0,\n",
        "                                                    glove_embedding,\n",
        "                                                    EMBEDDING_SIZE,\n",
        "                                                    training_words,\n",
        "                                                    words_to_tokens,\n",
        "                                                    OOV_METHOD,\n",
        "                                                    training_sentences)\n",
        "\n",
        "#Get an updated version of the embedding matrix with the validation set data\n",
        "embedding_matrix_validation = update_embedding_matrix(embedding_matrix_training,\n",
        "                                                    glove_embedding,\n",
        "                                                    EMBEDDING_SIZE,\n",
        "                                                    validation_words,\n",
        "                                                    words_to_tokens,\n",
        "                                                    OOV_METHOD,\n",
        "                                                    validation_sentences)\n",
        "\n",
        "#Get an updated version of the embedding matrix with the test set data\n",
        "embedding_matrix_test = update_embedding_matrix(embedding_matrix_validation,\n",
        "                                                    glove_embedding,\n",
        "                                                    EMBEDDING_SIZE,\n",
        "                                                    test_words,\n",
        "                                                    words_to_tokens,\n",
        "                                                    OOV_METHOD,\n",
        "                                                    test_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MypraiXCuOIb"
      },
      "source": [
        "# Models\n",
        "This section is used for creating different models, going from a baseline to slightly more complicated ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7EPmCLyuZVB"
      },
      "source": [
        "## Constants and utilities\n",
        "First of all, define some constants, parameter dictionaries and methods that will be reused by each architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__iSg0qcABw"
      },
      "source": [
        "# TODO: all the following constants are temporary \n",
        "N_CLASSES = 20  # this must be equal to the number of tags\n",
        "VOCABULARY_SIZE = 1000  # this must be obtained from the dataset\n",
        "MAX_SEQUENCE_SIZE = 100  # this must be obtained from the dataset\n",
        "\n",
        "BATCH_SIZE = 128  # hyper-parameter to properly set\n",
        "EPOCHS = 5\n",
        "\n",
        "\n",
        "# Model common compile information\n",
        "# Use sparse_categorical_crossentropy because labels are one hot encoded\n",
        "model_compile_info = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': 'sparse_categorical_crossentropy',\n",
        "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
        "}\n",
        "\n",
        "# Model common training information\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience=10,\n",
        "                                                restore_best_weights=True)]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq-mTGQRL-uo"
      },
      "source": [
        "# This tensor should contain the weights obtained by GloVe\n",
        "embedding_weights = np.zeros(shape=(VOCABULARY_SIZE, EMBEDDING_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stq44tS8CJSy"
      },
      "source": [
        "Define utility methods that will be used to **create**, **train** and **test** the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE7Mvm9sZ8Pf"
      },
      "source": [
        "def create_model(name,\n",
        "                 layers, \n",
        "                 compile_info, \n",
        "                 show_summary=True) -> keras.Model:\n",
        "    \"\"\"\n",
        "    Create the model using the layers passed as parameters.\n",
        "    After the creation, the model is compiled and its summary is possibly \n",
        "    printed to console.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers : array\n",
        "        Array that contains a list of layers that must be added \n",
        "        to the model.\n",
        "    compile_info: Dictionary\n",
        "        Contains information required for compiling the model.\n",
        "    show_summary: bool\n",
        "        If true, then the summary of the model will be printed to console\n",
        "    \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : keras.Model\n",
        "        The keras model.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential(name=name)\n",
        "    \n",
        "    for idx, layer in enumerate(layers):\n",
        "\n",
        "        # Sanity checks for being sure that the last layer has been \n",
        "        # correctly set\n",
        "        if idx == len(layers) - 1:\n",
        "            assert layer.activation == keras.activations.softmax, 'Wrong activation function'\n",
        "            assert layer.units == N_CLASSES, 'Wrong number of units'\n",
        "\n",
        "        model.add(layer)\n",
        "\n",
        "    # Compile\n",
        "    model.compile(**compile_info)\n",
        "\n",
        "    # Print model summary\n",
        "    if show_summary:\n",
        "        model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(model: keras.Model,\n",
        "                x_train: np.ndarray,\n",
        "                y_train: np.ndarray,\n",
        "                x_val: np.ndarray,\n",
        "                y_val: np.ndarray,\n",
        "                training_info: dict):\n",
        "    \"\"\"\n",
        "    Training routine for the Keras model.\n",
        "    At the end of the training, retrieved History data is shown.\n",
        "\n",
        "    :param model: Keras built model\n",
        "    :param x_train: training data in np.ndarray format\n",
        "    :param y_train: training labels in np.ndarray format\n",
        "    :param x_val: validation data in np.ndarray format\n",
        "    :param y_val: validation labels in np.ndarray format\n",
        "    :param training_info: dictionary storing model fit() argument information\n",
        "\n",
        "    :return\n",
        "        model: trained Keras model\n",
        "    \"\"\"\n",
        "    print(\"Start training! \\nParameters: {}\".format(training_info))\n",
        "    history = model.fit(x=x_train, y=y_train,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        **training_info)\n",
        "    print(\"Training completed! Showing history...\")\n",
        "\n",
        "    show_history(history)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_data(model: keras.Model,\n",
        "                 x: np.ndarray,\n",
        "                 prediction_info: dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Inference routine of a given input set of examples\n",
        "\n",
        "    :param model: Keras built and possibly trained model\n",
        "    :param x: input set of examples in np.ndarray format\n",
        "    :param prediction_info: dictionary storing model predict() argument information\n",
        "\n",
        "    :return\n",
        "        predictions: predicted labels in np.ndarray format\n",
        "    \"\"\"\n",
        "\n",
        "    print('Starting prediction: \\n{}'.format(prediction_info))\n",
        "    print('Predicting on {} samples'.format(x.shape[0]))\n",
        "\n",
        "    predictions = model.predict(x, **prediction_info)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def evaluate_predictions(predictions: np.ndarray,\n",
        "                         y: np.ndarray,\n",
        "                         metrics: List[Callable],\n",
        "                         metric_names: List[str]):\n",
        "    \"\"\"\n",
        "    Evaluates given model predictions on a list of metric functions\n",
        "\n",
        "    :param predictions: model predictions in np.ndarray format\n",
        "    :param y: ground-truth labels in np.ndarray format\n",
        "    :param metrics: list of metric functions\n",
        "    :param metric_names: list of metric names\n",
        "\n",
        "    :return\n",
        "        metric_info: dictionary containing metric values for each input metric\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(metrics) == len(metric_names)\n",
        "\n",
        "    print(\"Evaluating predictions! Total samples: \", y.shape[0])\n",
        "\n",
        "    metric_info = {}\n",
        "\n",
        "    for metric, metric_name in zip(metrics, metric_names):\n",
        "        metric_value = metric(y_pred=predictions, y_true=y)\n",
        "        metric_info[metric_name] = metric_value\n",
        "\n",
        "    return metric_info\n",
        "\n",
        "def model_sanity_check(model: keras.Model, \n",
        "                       use_embedding_layer: bool = False):\n",
        "    \"\"\"\n",
        "    Create a random input_tensor and try to pass through the model.\n",
        "    This method should be used in order to check if the model is \n",
        "    working as expected.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : keras.Model\n",
        "        The model that must be tested.\n",
        "    use_embedding_layer: depending on this flag the shape of the input must be \n",
        "        treated differently.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f'Sanity check for the model with name: {model.name}')\n",
        "    # Model sanity check for seeing if it runs correctly\n",
        "    if use_embedding_layer:\n",
        "        input_tensor = np.random.uniform(\n",
        "            size=(BATCH_SIZE, MAX_SEQUENCE_SIZE)\n",
        "            )\n",
        "    else:\n",
        "        input_tensor = np.random.uniform(\n",
        "            size=(BATCH_SIZE, MAX_SEQUENCE_SIZE, EMBEDDING_SIZE)\n",
        "            )\n",
        "    print(f'Input tensor shape: {input_tensor.shape}')\n",
        "    output_tensor = model(input_tensor)\n",
        "    print(f'Output tensor shape: {output_tensor.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPDiCsbl9ea"
      },
      "source": [
        "Define utility methods for **creating layers** in order to: \n",
        "* reduce the code verbosity.\n",
        "* be sure to always create different architectures with the same layer structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rCmJJgpmBRY"
      },
      "source": [
        "# EMBEDDING\n",
        "# NOTE: Actually this layer has not been used in the final models, \n",
        "# but it has been used for some experimentations\n",
        "def embedding_layer(embedding_weights: np.array,\n",
        "                    layer_name: str='embedding') -> layers.Embedding:\n",
        "    \"\"\"\n",
        "    Create an embedding layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embedding_weights : np.array\n",
        "        The weights for the embedding layer.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Embedding\n",
        "        The created embedding layer.\n",
        "    \"\"\"\n",
        "    layer = layers.Embedding(\n",
        "        input_dim=VOCABULARY_SIZE, \n",
        "        output_dim=EMBEDDING_SIZE, \n",
        "        input_length=MAX_SEQUENCE_SIZE,\n",
        "        weights=[embedding_weights],\n",
        "        mask_zero=True,\n",
        "        name=layer_name\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# MASKING\n",
        "def masking_layer(input_shape: tuple,\n",
        "                  mask_value: float=0.0,\n",
        "                  layer_name: str='masking') -> layers.Masking:\n",
        "    \"\"\"\n",
        "    Create a masking layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_shape : tuple\n",
        "        The weights for the embedding layer.\n",
        "    mask_value : the value to mask because it represents the padding \n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Masking\n",
        "        The created masking layer.\n",
        "    \"\"\"\n",
        "    layer = layers.Masking(\n",
        "        input_shape=input_shape, \n",
        "        mask_value=mask_value,\n",
        "        name=layer_name\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# RNN (LSTM and GRU)\n",
        "def _rnn_size(layer_depth: int) -> int:\n",
        "    \"\"\"\n",
        "    Simple logic used for assigning the number of units \n",
        "    to the rnn layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    size : int\n",
        "        The number units.\n",
        "    \"\"\"\n",
        "    size = 64\n",
        "    if layer_depth > 1:\n",
        "        size = 128\n",
        "    return size\n",
        "\n",
        "def bilstm_layer(layer_depth: int,\n",
        "                 layer_name: str='bi-lstm') -> layers.Bidirectional:\n",
        "    \"\"\"\n",
        "    Create a bidirectional lstm layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Bidirectional\n",
        "        The created bidirectional lstm layer.\n",
        "    \"\"\"\n",
        "    size = _rnn_size(layer_depth)\n",
        "    layer = layers.Bidirectional(\n",
        "        layers.LSTM(size, \n",
        "                    return_sequences=True, \n",
        "                    activation='relu'),\n",
        "                    name=layer_name,\n",
        "                    \n",
        "        )\n",
        "    return layer\n",
        "\n",
        "def bigru_layer(layer_depth: int,\n",
        "                layer_name: str='bi-gru') -> layers.Bidirectional:\n",
        "    \"\"\"\n",
        "    Create a bidirectional gru layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Bidirectional\n",
        "        The created bidirectional gru layer.\n",
        "    \"\"\"\n",
        "    size = _rnn_size(layer_depth)\n",
        "    layer = layers.Bidirectional(\n",
        "        layers.GRU(size, \n",
        "                   return_sequences=True, \n",
        "                   activation='relu'),\n",
        "                   name=layer_name\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# DENSE\n",
        "def _dense_size(last_layer:bool) -> int:\n",
        "    \"\"\"\n",
        "    Simple logic for assigning the size of the dense layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    size : int\n",
        "        The size of the dense layer.\n",
        "    \"\"\"\n",
        "    size = N_CLASSES\n",
        "    if not last_layer:\n",
        "        size = 256\n",
        "    return size\n",
        "\n",
        "def _dense_activation(last_layer:bool) -> str:\n",
        "    \"\"\"\n",
        "    Simple logic for assigning the activation function of the dense layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    activation : str\n",
        "        The activation function of the layer.\n",
        "    \"\"\"\n",
        "    activation = 'relu'\n",
        "    if last_layer:\n",
        "        activation = 'softmax'\n",
        "    return activation\n",
        "\n",
        "def dense_layer(last_layer:bool,\n",
        "                layer_name: str='dense') -> layers.Dense:\n",
        "    \"\"\"\n",
        "    Create a dense layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Dense\n",
        "        The created dense layer.\n",
        "    \"\"\"\n",
        "    size = _dense_size(last_layer)\n",
        "    activation = _dense_activation(last_layer)\n",
        "    \n",
        "    return layers.Dense(size, \n",
        "                        activation=activation, \n",
        "                        name=layer_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOhK3xp2FCUu"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoxVUVMap5Ma",
        "collapsed": true
      },
      "source": [
        "# Create layers\n",
        "baseline_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE), \n",
        "                              layer_name='masking_0'),\n",
        "                bilstm_layer(layer_depth=1, \n",
        "                             layer_name='bi-lstm_0'),\n",
        "                dense_layer(last_layer=True, \n",
        "                            layer_name='dense_0')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_model = create_model('baseline', \n",
        "                              baseline_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LGdswmdFg6x"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4l5n888vLql"
      },
      "source": [
        "## Variations\n",
        "What follows is the implementation of small variations to the baseline architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMvk3AQFukOh"
      },
      "source": [
        "### GRU\n",
        "Change the LSTM layer with the GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "856hGaR9eDRb"
      },
      "source": [
        "# Create layers\n",
        "baseline_var1_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE),\n",
        "                              layer_name='masking_0'),\n",
        "                bigru_layer(layer_depth=1,\n",
        "                            layer_name='bi-gru_0'),\n",
        "                dense_layer(last_layer=True,\n",
        "                            layer_name='dense_0')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var1_model = create_model('baseline_var1', \n",
        "                              baseline_var1_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var1_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9We6j_USupzN"
      },
      "source": [
        "### Additional LSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLHH94CtqZ7c"
      },
      "source": [
        "# Create layers\n",
        "baseline_var2_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE),\n",
        "                              layer_name='masking_0'),\n",
        "                bilstm_layer(layer_depth=1,\n",
        "                             layer_name='bi-lstm_0'),\n",
        "                bilstm_layer(layer_depth=2,\n",
        "                             layer_name='bi-lstm_1'),\n",
        "                dense_layer(last_layer=True,\n",
        "                            layer_name='dense_0')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var2_model = create_model('baseline_var2', \n",
        "                              baseline_var2_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var2_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RN6ySWyu13J"
      },
      "source": [
        "### Additional Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujfM5mekqma8"
      },
      "source": [
        "# Create layers\n",
        "baseline_var3_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE),\n",
        "                              layer_name='masking_0'),\n",
        "                bilstm_layer(layer_depth=1,\n",
        "                             layer_name='bi-lstm_0'),\n",
        "                dense_layer(last_layer=False,\n",
        "                            layer_name='dense_0'),\n",
        "                dense_layer(last_layer=True,\n",
        "                            layer_name='dense_1')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var3_model = create_model('baseline_var3', \n",
        "                              baseline_var3_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var3_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwOShFdwvaHN"
      },
      "source": [
        "# Training and Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1SYhjS1vvzF"
      },
      "source": [
        "# Disussion and Error Analysis"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment1",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRJJ4tzaC852"
      },
      "source": [
        "# Preliminary Steps\n",
        "These are some preliminary steps before addressing the task. Import some basic libraries and set a variable that will be used in multiple steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCTmlk1C0Sm"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# typing\n",
        "from typing import List, Callable, Dict"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vspH9UzptOFT"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPThN5GWAZl"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsjdOVHZtSnp"
      },
      "source": [
        "## Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR9RtRITt72b"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJbpdftCt8Gl"
      },
      "source": [
        "## Train, validation and test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXgbC4TAuJ81"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Co7hfK2uKD8"
      },
      "source": [
        "# GloVe "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tRukt6Kxx4R"
      },
      "source": [
        "## OOV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-uwA9DYuNtr"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MypraiXCuOIb"
      },
      "source": [
        "# Models\n",
        "This section is used for creating different models, going from a baseline to slightly more complicated ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7EPmCLyuZVB"
      },
      "source": [
        "## Constants and utilities\n",
        "First of all, define some constants, parameter dictionaries and methods that will be reused by each architecture.\n",
        "\n",
        "**Note**: the parameters defined below do not change across different architectures because the assignment is very precise, and it says that only layers must be either added or modified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__iSg0qcABw"
      },
      "source": [
        "# TODO: all the following constants are temporary \n",
        "N_CLASSES = 20  # this must be equal to the number of tags\n",
        "VOCABULARY_SIZE = 1000  # this must be obtained from the dataset\n",
        "EMBEDDING_SIZE = 64  # hyper-parameter to properly set\n",
        "MAX_SEQUENCE_SIZE = 100  # this must be obtained from the dataset\n",
        "\n",
        "BATCH_SIZE = 128  # hyper-parameter to properly set\n",
        "EPOCHS = 5\n",
        "\n",
        "# Use sparse_categorical_crossentropy because labels are one hot encoded\n",
        "model_compile_info = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': 'sparse_categorical_crossentropy',\n",
        "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
        "}\n",
        "\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience=10,\n",
        "                                                restore_best_weights=True)]\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq-mTGQRL-uo"
      },
      "source": [
        "# This tensor should contain the weights obtained by GloVe\n",
        "embedding_weights = np.zeros(shape=(VOCABULARY_SIZE, EMBEDDING_SIZE))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stq44tS8CJSy"
      },
      "source": [
        "Define utility methods that will be used to create, train and test the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE7Mvm9sZ8Pf"
      },
      "source": [
        "def create_and_compile_model(layers, \n",
        "                             compile_info, \n",
        "                             show_summary=True) -> keras.Model:\n",
        "    \"\"\"\n",
        "    Create the model using the layers passed as parameters.\n",
        "    After the creation, the model is compiled and its summary is possibly \n",
        "    printed to console.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers : array\n",
        "        Array that contains a list of layers that must be added \n",
        "        to the model.\n",
        "    compile_info: Dictionary\n",
        "        Contains information required for compiling the model.\n",
        "    show_summary: bool\n",
        "        If true, then the summary of the model will be printed to console\n",
        "    \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : keras.Model\n",
        "        The keras model.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    \n",
        "    for idx, layer in enumerate(layers):\n",
        "\n",
        "        # Sanity checks for being sure that the last layer has been \n",
        "        # correctly set\n",
        "        if idx == len(layers) - 1:\n",
        "            assert layer.activation == keras.activations.softmax, 'Wrong activation function'\n",
        "            assert layer.units == N_CLASSES, 'Wrong number of units'\n",
        "\n",
        "        model.add(layer)\n",
        "\n",
        "    # Compile\n",
        "    model.compile(**compile_info)\n",
        "\n",
        "    # Print model summary\n",
        "    if show_summary:\n",
        "        model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(model: keras.Model,\n",
        "                x_train: np.ndarray,\n",
        "                y_train: np.ndarray,\n",
        "                x_val: np.ndarray,\n",
        "                y_val: np.ndarray,\n",
        "                training_info: dict):\n",
        "    \"\"\"\n",
        "    Training routine for the Keras model.\n",
        "    At the end of the training, retrieved History data is shown.\n",
        "\n",
        "    :param model: Keras built model\n",
        "    :param x_train: training data in np.ndarray format\n",
        "    :param y_train: training labels in np.ndarray format\n",
        "    :param x_val: validation data in np.ndarray format\n",
        "    :param y_val: validation labels in np.ndarray format\n",
        "    :param training_info: dictionary storing model fit() argument information\n",
        "\n",
        "    :return\n",
        "        model: trained Keras model\n",
        "    \"\"\"\n",
        "    print(\"Start training! \\nParameters: {}\".format(training_info))\n",
        "    history = model.fit(x=x_train, y=y_train,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        **training_info)\n",
        "    print(\"Training completed! Showing history...\")\n",
        "\n",
        "    show_history(history)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_data(model: keras.Model,\n",
        "                 x: np.ndarray,\n",
        "                 prediction_info: dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Inference routine of a given input set of examples\n",
        "\n",
        "    :param model: Keras built and possibly trained model\n",
        "    :param x: input set of examples in np.ndarray format\n",
        "    :param prediction_info: dictionary storing model predict() argument information\n",
        "\n",
        "    :return\n",
        "        predictions: predicted labels in np.ndarray format\n",
        "    \"\"\"\n",
        "\n",
        "    print('Starting prediction: \\n{}'.format(prediction_info))\n",
        "    print('Predicting on {} samples'.format(x.shape[0]))\n",
        "\n",
        "    predictions = model.predict(x, **prediction_info)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def evaluate_predictions(predictions: np.ndarray,\n",
        "                         y: np.ndarray,\n",
        "                         metrics: List[Callable],\n",
        "                         metric_names: List[str]):\n",
        "    \"\"\"\n",
        "    Evaluates given model predictions on a list of metric functions\n",
        "\n",
        "    :param predictions: model predictions in np.ndarray format\n",
        "    :param y: ground-truth labels in np.ndarray format\n",
        "    :param metrics: list of metric functions\n",
        "    :param metric_names: list of metric names\n",
        "\n",
        "    :return\n",
        "        metric_info: dictionary containing metric values for each input metric\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(metrics) == len(metric_names)\n",
        "\n",
        "    print(\"Evaluating predictions! Total samples: \", y.shape[0])\n",
        "\n",
        "    metric_info = {}\n",
        "\n",
        "    for metric, metric_name in zip(metrics, metric_names):\n",
        "        metric_value = metric(y_pred=predictions, y_true=y)\n",
        "        metric_info[metric_name] = metric_value\n",
        "\n",
        "    return metric_info\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOhK3xp2FCUu"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqZ-XwiJT76C"
      },
      "source": [
        "def baseline_model(compile_info, embedding_weights=None) -> keras.Model:\n",
        "    \"\"\"\n",
        "    Define all the layers that must be contained in the baseline model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers : array\n",
        "        Array that contains a list of layers that must be added \n",
        "        to the model.\n",
        "    compile_info: Dictionary\n",
        "        Contains information required for compiling the model.\n",
        "    embedding_weights: np.array\n",
        "        Array containing the weights of for the embedding layer.\n",
        "    \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : keras.Model\n",
        "        The keras model.\n",
        "    \"\"\"\n",
        "    model_layers = []\n",
        "\n",
        "    # Embedding layer\n",
        "    model_layers.append(\n",
        "        layers.Embedding(\n",
        "            input_dim=VOCABULARY_SIZE, \n",
        "            output_dim=EMBEDDING_SIZE, \n",
        "            input_length=MAX_SEQUENCE_SIZE,\n",
        "            weights=[embedding_weights],\n",
        "            mask_zero=True\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    # TODO: try different activation functions??\n",
        "\n",
        "    # LSTM layer\n",
        "    model_layers.append(\n",
        "        layers.LSTM(128, return_sequences=True, activation='relu')\n",
        "        )\n",
        "\n",
        "    # Dense layer\n",
        "    model_layers.append(\n",
        "        layers.Dense(N_CLASSES, activation='softmax')\n",
        "        )\n",
        "\n",
        "    # Create the model\n",
        "    model = create_and_compile_model(model_layers, model_compile_info)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LGdswmdFg6x"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_S2EqCZkA60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f473b9-47b2-4573-c8b6-e0f49992eaa6"
      },
      "source": [
        "model_1 = baseline_model(model_compile_info, embedding_weights)\n",
        "\n",
        "# Model sanity check for seeing if it runs correctly\n",
        "input_tensor = np.random.uniform(size=(BATCH_SIZE, MAX_SEQUENCE_SIZE))\n",
        "print(f'Input tensor shape: {input_tensor.shape}')\n",
        "output_tensor = model_1(input_tensor)\n",
        "print(f'Output tensor shape: {output_tensor.shape}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 100, 64)           64000     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 100, 128)          98816     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100, 20)           2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 165,396\n",
            "Trainable params: 165,396\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Input tensor shape: (128, 100)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4l5n888vLql"
      },
      "source": [
        "## Variations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMvk3AQFukOh"
      },
      "source": [
        "### GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9We6j_USupzN"
      },
      "source": [
        "### Additional LSTM layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RN6ySWyu13J"
      },
      "source": [
        "### Additional Dense layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwOShFdwvaHN"
      },
      "source": [
        "# Training and Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1SYhjS1vvzF"
      },
      "source": [
        "# Disussion and Error Analysis"
      ]
    }
  ]
}
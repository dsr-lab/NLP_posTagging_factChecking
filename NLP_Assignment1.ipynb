{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRJJ4tzaC852"
      },
      "source": [
        "# Preliminary Steps\n",
        "These are some preliminary steps before addressing the task. Import some basic libraries and set a variable that will be used in multiple steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCTmlk1C0Sm"
      },
      "source": [
        "import os, sys\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tqdm import tqdm\n",
        "\n",
        "# typing\n",
        "from typing import List, Callable, Dict"
      ],
      "execution_count": 459,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vspH9UzptOFT"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reOmnUrPjRzt"
      },
      "source": [
        "## Constant and utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBylTiJGjVZe"
      },
      "source": [
        "DATASET_NAME = \"dependency_treebank\"\n",
        "DOCUMENT_EXTENSION = \".dp\"\n",
        "\n",
        "USE_DOCUMENTS = True #True=Use documents; False = Use sentences\n",
        "file_end_name = \"_documents\" if USE_DOCUMENTS else \"_sentences\"\n",
        "\n",
        "#List of paths to handle the dataset\n",
        "DATASET_PATHS = {\n",
        "    \"url\" : 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip', #url to dowload the dataset\n",
        "    \"dataset_folder\": os.path.join(os.getcwd(), \"Datasets\", \"Original\"), #folder containing the original dataset data\n",
        "    \"dataset_path\" : os.path.join(os.getcwd(), \"Datasets\", \"Original\", \"dependency_treebank.zip\"), #path to zipped dataset\n",
        "    \"documents_path\" : os.path.join(os.getcwd(), \"Datasets\", \"Original\", DATASET_NAME), #folder containing extracted documents (NB: it is created automatically during the extraction)\n",
        "    \"dataframe_folder\" : os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", DATASET_NAME), #folder containing the dataframe data\n",
        "    \"dataframe_path\" : os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", DATASET_NAME, DATASET_NAME + file_end_name + \".pkl\") #path to pickle save of built dataframe\n",
        "}\n",
        "\n",
        "TRAINING_DOCS = 100\n",
        "VALIDATION_DOCS = 50\n",
        "TEST_DOCS = 49\n",
        "\n",
        "PADDING = 0"
      ],
      "execution_count": 460,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr0llgPylm31"
      },
      "source": [
        "##Folders creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPThN5GWAZl"
      },
      "source": [
        "def create_folders(paths):\n",
        "  for path in paths:\n",
        "    if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "\n",
        "folders = [DATASET_PATHS[\"dataset_folder\"],\n",
        "           DATASET_PATHS[\"dataframe_folder\"]]\n",
        "           \n",
        "create_folders(folders)"
      ],
      "execution_count": 461,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsjdOVHZtSnp"
      },
      "source": [
        "## Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR9RtRITt72b"
      },
      "source": [
        "def download_dataset(dataset_path):\n",
        "  if not os.path.exists(dataset_path):\n",
        "      urllib.request.urlretrieve(DATASET_PATHS[\"url\"], dataset_path)\n",
        "\n",
        "      print(\"Successful download\")\n",
        "\n",
        "download_dataset(DATASET_PATHS[\"dataset_path\"])"
      ],
      "execution_count": 462,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByKCopDN_Rb-"
      },
      "source": [
        "## Dataset extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz8xnlmy_XvE"
      },
      "source": [
        "def extract_dataset(dataset_path, dataset_folder, documents_path):\n",
        "  expected_docs_number = TRAINING_DOCS + VALIDATION_DOCS + TEST_DOCS\n",
        "\n",
        "  if not os.path.exists(documents_path) or len(os.listdir(documents_path))<expected_docs_number:\n",
        "    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_folder)\n",
        "\n",
        "    print(\"Successful extraction\")\n",
        "\n",
        "extract_dataset(DATASET_PATHS[\"dataset_path\"],DATASET_PATHS[\"dataset_folder\"],DATASET_PATHS[\"documents_path\"])"
      ],
      "execution_count": 463,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGVpE8IuCTIa"
      },
      "source": [
        "##Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ZbwHLECWPk"
      },
      "source": [
        "def preprocess(token : str) -> str:\n",
        "\t\"\"\"\n",
        "\tCalls the function that cleans the text\n",
        "\tInput: the string to process\n",
        "\tOutput: the processed string\n",
        "\t\"\"\"\n",
        "\ttoken = text_to_lower(token)\n",
        "\ttoken = strip_text(token)\n",
        "\treturn token\n",
        "\n",
        "def text_to_lower(text: str) -> str:\n",
        "\t\"\"\"\n",
        "\tReturns the string in lower character\n",
        "\tInput: the string to process\n",
        "\tOutput: the processed string\n",
        "\t\"\"\"\n",
        "\treturn text.lower();\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "\t\"\"\"\n",
        "\tRemoves any left or right spacing (including carriage return) from text.\n",
        "\tExample:\n",
        "\tInput: '  This assignment is cool\\n'\n",
        "\tOutput: 'This assignment is cool'\n",
        "\t\"\"\"\n",
        "\n",
        "\treturn text.strip()"
      ],
      "execution_count": 464,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJbpdftCt8Gl"
      },
      "source": [
        "## Dataframe creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXgbC4TAuJ81"
      },
      "source": [
        "TOKEN_SEPARATOR = \" \" #Character used to separate tokens in the dataframe\n",
        "SENTENCE_SEPARATOR = \"##\" #Characters to signal the end of a sentence (if USE_DOCUMENTS=False)\n",
        "WORDS_CONTAINER = \"document\" if USE_DOCUMENTS else \"sentence\"\n",
        "\n",
        "def list_to_string(_list):\n",
        "\tstring = \"\"\n",
        "\n",
        "\tfor index, value in enumerate(_list):\n",
        "\t\tstring+=value\n",
        "\t\tif index!=len(_list)-1:\n",
        "\t\t\tstring+=TOKEN_SEPARATOR\n",
        "\t\n",
        "\treturn string\n",
        "\n",
        "def add_row_to_dataframe_rows(dataframe_rows, split, document, labels):\n",
        "\tif USE_DOCUMENTS:\n",
        "\t\tdataframe_row = {\"split\": split, WORDS_CONTAINER: document, \"labels\": labels}\n",
        "\t\tdataframe_rows.append(dataframe_row)\n",
        "\t\n",
        "\telse:\n",
        "\t\tsentences = document.split(SENTENCE_SEPARATOR)\n",
        "\t\tsplit_labels = labels.split(SENTENCE_SEPARATOR)\n",
        "\t\tfor i in range(len(sentences)):\n",
        "\t\t\tdataframe_row = {\"split\": split, WORDS_CONTAINER: sentences[i], \"labels\": split_labels[i]}\n",
        "\t\t\tdataframe_rows.append(dataframe_row)\n",
        "\t\n",
        "def rows_to_dataframe(rows):\n",
        "\tdataframe = pd.DataFrame(rows)\n",
        "\tdataframe = dataframe[[\"split\", WORDS_CONTAINER, \"labels\"]]\n",
        "\n",
        "\treturn dataframe\n",
        "\n",
        "def get_documents(path):\n",
        "\tfiles = os.listdir(path)\n",
        "\tdocuments = filter(lambda name: (name.endswith(DOCUMENT_EXTENSION)), files)\n",
        "\tdocuments = list(documents)\n",
        "\tdocuments.sort()\n",
        " \n",
        "\treturn documents\n",
        " \n",
        "def get_document_number(filename):\n",
        "\treturn int(filename.split(\"_\")[1].split(\".\")[0])\n",
        " \n",
        "def extract_data_from_line(line):\n",
        "\tif line != \"\\n\":\n",
        "\t\tcolumns = line.split()\n",
        "\t\ttoken = columns[0]\n",
        "\t\ttoken = preprocess(token)\n",
        "\t\tlabel = columns[1]\n",
        "\n",
        "\t\treturn token, label\n",
        "\n",
        "\telse:\n",
        "\t\tif USE_DOCUMENTS:\n",
        "\t\t\treturn None, None\n",
        "\t\telse:\n",
        "\t\t\treturn SENTENCE_SEPARATOR, SENTENCE_SEPARATOR\n",
        "\n",
        "def process_document(document, doc_number):\n",
        "\ttokens = []\n",
        "\tlabels = []\n",
        "\tsplit = \"\"\n",
        "\n",
        "\ttry:\n",
        "\t\tif os.path.isfile(document):\n",
        "\t\t\t#Open the file\n",
        "\t\t\twith open(document, mode='r', encoding='utf-8') as text_file:\n",
        "\n",
        "\t\t\t\t#Split in different groups\n",
        "\t\t\t\tif doc_number <= TRAINING_DOCS:\n",
        "\t\t\t\t\tsplit = \"train\"\n",
        "\t\t\t\telif doc_number <= TRAINING_DOCS+VALIDATION_DOCS:\n",
        "\t\t\t\t\tsplit = \"validation\"\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tsplit = \"test\"\n",
        "\n",
        "\t\t\t\t#Stop at end of file\n",
        "\t\t\t\tfor line in text_file:\n",
        "\t\t\t\t\ttoken, label = extract_data_from_line(line)\n",
        "\n",
        "\t\t\t\t\tif token!=None and label!=None:\n",
        "\t\t\t\t\t\ttokens.append(token)\n",
        "\t\t\t\t\t\tlabels.append(label)\n",
        "\t\t \n",
        "\texcept Exception as e:\n",
        "                print('Failed to process %s. Reason: %s' % (document, e))\n",
        "                sys.exit(0)\n",
        "\n",
        "\treturn split, list_to_string(tokens), list_to_string(labels)"
      ],
      "execution_count": 465,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQECWv5YD28l"
      },
      "source": [
        "###Build/Load Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8ePHxt0D74q"
      },
      "source": [
        "def build_dataframe(documents_path,dataframe_path, save=True):\n",
        "\n",
        "\tdocuments = get_documents(documents_path)\n",
        "\tdataframe_rows = []\n",
        "\n",
        "\t#Iterate along the files\n",
        "\tfor filename in documents:\n",
        "\t\tdocument = os.path.join(documents_path, filename)\n",
        "\t\tdoc_number = get_document_number(filename)\n",
        "\n",
        "\t\tsplit, tokens, labels = process_document(document, doc_number)\n",
        "\t\tadd_row_to_dataframe_rows(dataframe_rows,split,tokens,labels)\n",
        "\n",
        "\t#Transform the list of rows in a proper dataframe\n",
        "\tdataframe = rows_to_dataframe(dataframe_rows)\n",
        "\tprint(\"Dataframe built successfully\")\n",
        "\t\n",
        "\t#Save the dataframe\n",
        "\tif save:\n",
        "\t\tdataframe.to_pickle(dataframe_path)\n",
        "\t\tprint(\"Dataframe saved successfully\")\n",
        " \n",
        "\treturn dataframe\n",
        "\n",
        "def load_dataframe(documents_path, dataframe_path, force_rebuild = False):\n",
        "\tif not os.path.exists(dataframe_path) or force_rebuild:\n",
        "\t\treturn build_dataframe(documents_path, dataframe_path)\n",
        "\telse:\n",
        "\t\treturn pd.read_pickle(dataframe_path)"
      ],
      "execution_count": 466,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uIhDkgTkRVy"
      },
      "source": [
        "##Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMM5D7ZTeBGJ",
        "outputId": "7c4e89cd-6491-43f0-86ff-8b3b6efd1652"
      },
      "source": [
        "df = load_dataframe(DATASET_PATHS[\"documents_path\"],DATASET_PATHS[\"dataframe_path\"], True)\n",
        "\n",
        "#Test\n",
        "#print(df.iloc[1][WORDS_CONTAINER])\n",
        "#print(df.iloc[1][\"labels\"])\n",
        "\n",
        "training_set = df.loc[df[\"split\"] == \"train\"]\n",
        "validation_set = df.loc[df[\"split\"] == \"validation\"]\n",
        "test_set = df.loc[df[\"split\"] == \"test\"]"
      ],
      "execution_count": 467,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe built successfully\n",
            "Dataframe saved successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TmFlty5oJh4"
      },
      "source": [
        "##Tokenization and vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xmzxl78oesT"
      },
      "source": [
        "STARTING_TOKEN = 1\n",
        "\n",
        "def get_tokenizer(corpus, starting_dict=None):\n",
        "  words_to_tokens = {} if starting_dict==None else starting_dict.copy()\n",
        "\n",
        "  for text in corpus:\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "      if not word in words_to_tokens:\n",
        "        words_to_tokens[word] = len(words_to_tokens)+STARTING_TOKEN\n",
        "\n",
        "  return words_to_tokens\n",
        "\n",
        "def tokenize(word, words_to_tokens):\n",
        "  return words_to_tokens[word]\n",
        "\n",
        "def detokenize(token, words_to_tokens):\n",
        "  return words_to_tokens.index(token)\n",
        "\n",
        "def tokenize_string(string, words_to_tokens, max_lenght):\n",
        "  tokens = string.split()\n",
        "  tokenized_sequence = [tokenize(token, words_to_tokens)  for token in tokens]\n",
        "  padding = max_lenght-len(tokenized_sequence)\n",
        "  return np.pad(tokenized_sequence, (PADDING, padding), 'constant').tolist()\n",
        "\n",
        "#Define corpus\n",
        "train_text = training_set[WORDS_CONTAINER].tolist()\n",
        "val_text = validation_set[WORDS_CONTAINER].tolist()\n",
        "test_text = validation_set[WORDS_CONTAINER].tolist()\n",
        "\n",
        "#Define labels\n",
        "train_labels = training_set[\"labels\"].tolist()\n",
        "val_labels = validation_set[\"labels\"].tolist()\n",
        "test_labels = test_set[\"labels\"].tolist()\n",
        "\n",
        "#Token dictionaries\n",
        "train_tokens = get_tokenizer(train_text)\n",
        "val_tokens = get_tokenizer(val_text, starting_dict = train_tokens)\n",
        "test_tokens = get_tokenizer(test_text, starting_dict = val_tokens)\n",
        "\n",
        "#Vocabularies\n",
        "train_vocab = train_tokens.keys()\n",
        "val_vocab = val_tokens.keys()\n",
        "test_vocab = test_tokens.keys()\n",
        "\n",
        "#Vocab sizes\n",
        "train_vocab_size = len(train_vocab)\n",
        "val_vocab_size = len(val_vocab)\n",
        "test_vocab_size = len(test_vocab)\n",
        "\n",
        "#Max lenght of a token sequence\n",
        "corpus = train_text+val_text+test_text\n",
        "n_tokens = [len(doc.split()) for doc in corpus]\n",
        "max_length = max(n_tokens)\n",
        "\n",
        "#Tokenized sets\n",
        "train_tokenized = np.array(list(map(lambda string: tokenize_string(string, train_tokens,max_length),train_text)))\n",
        "val_tokenized = np.array(list(map(lambda string: tokenize_string(string, val_tokens,max_length),val_text)))\n",
        "test_tokenized = np.array(list(map(lambda string: tokenize_string(string, test_tokens,max_length),test_text)))"
      ],
      "execution_count": 468,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWhsQLQPdbQ"
      },
      "source": [
        "##Labels encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qSrRRAbPhzo"
      },
      "source": [
        "ONE_HOT = False # Determine which encoding method to use: True = One_hot encoding; False = Categorical encoding\n",
        "\n",
        "def get_categorical_encoding(labels_list):\n",
        "  encoding = {}\n",
        "\n",
        "  for label_group in labels_list:\n",
        "    labels = label_group.split()\n",
        "    for label in labels:\n",
        "      if not label in encoding:\n",
        "        encoding[label] = len(encoding)+1\n",
        "\n",
        "  return encoding\n",
        "\n",
        "def get_one_hot_encoding(categorical_encoding):\n",
        "  one_hot_encoding = {}\n",
        "\n",
        "  size = len(categorical_encoding)\n",
        "  labels = categorical_encoding.keys()\n",
        "\n",
        "  for index, label in enumerate(labels):\n",
        "    encoding = np.zeros(size, dtype=np.int32)\n",
        "    encoding[index] = 1\n",
        "    one_hot_encoding[label] = encoding\n",
        "\n",
        "  return one_hot_encoding\n",
        "\n",
        "def get_labels_encoding(labels_list, one_hot):\n",
        "  categorical_encoding = get_categorical_encoding(labels_list)\n",
        "\n",
        "  if one_hot:\n",
        "    return get_one_hot_encoding(categorical_encoding)\n",
        "\n",
        "  else:\n",
        "    return categorical_encoding\n",
        "\n",
        "def encode_label(label, encoding):\n",
        "  return encoding[label]\n",
        "\n",
        "def decode_label(value, encoding):\n",
        "  return encoding.index(value)\n",
        "\n",
        "def encode_string_of_labels(string, encoding, max_lenght):\n",
        "  labels = string.split()\n",
        "  encoded_sequence = [encode_label(label, encoding)  for label in labels]\n",
        "  padding = max_lenght-len(encoded_sequence)\n",
        "  return np.pad(encoded_sequence, (PADDING, padding), 'constant')\n",
        "\n",
        "def is_punctuation_label(label):\n",
        "  return len(label)==1\n",
        "\n",
        "labels_list = training_set[\"labels\"].tolist() + validation_set[\"labels\"].tolist() + test_set[\"labels\"].tolist()\n",
        "labels_encoding = get_labels_encoding(labels_list, ONE_HOT)"
      ],
      "execution_count": 469,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Co7hfK2uKD8"
      },
      "source": [
        "# GloVe\n",
        "This section is the one responsible for the implementation of the GloVe embedding system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8qLxw1kcTto"
      },
      "source": [
        "## Constants and utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqG3dXiBcdeA"
      },
      "source": [
        "URL_BASE = \"https://nlp.stanford.edu/data\" #Location of the pre-trained GloVe's files\n",
        "GLOVE_VERSION = \"6B\"\n",
        "\n",
        "EMBEDDING_SIZE = 50 #The dimensionality of the embeddings; to be tested\n",
        "\n",
        "#List of paths to download and extract GloVe's files\n",
        "PATHS = {\n",
        "    \"url\": URL_BASE + \"/glove.\" + GLOVE_VERSION + \".zip\",\n",
        "    \"glove_path\": os.path.join(os.getcwd(),\"Glove\",GLOVE_VERSION),\n",
        "    \"glove_zip\": os.path.join(os.getcwd(),\"Glove\", GLOVE_VERSION, \"glove.\"+GLOVE_VERSION+\".zip\"),\n",
        "    \"glove_file\": os.path.join(os.getcwd(),\"Glove\", GLOVE_VERSION, \"glove.\"+GLOVE_VERSION+\".\"+str(EMBEDDING_SIZE)+\"d.txt\")\n",
        "}\n",
        "\n",
        "OOV_METHOD = \"Mean\" #Determine which OOV method to adopt; choose one between \"Mean\", \"Random\" and \"Placeholder\""
      ],
      "execution_count": 470,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIY-9Qf_bOU5"
      },
      "source": [
        "## Download\n",
        "In this part the presence of the GloVe file is checked. In case of a negative response, it will be downloaded and extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r14RFpFCbFn2"
      },
      "source": [
        "def setup_files():\n",
        "\n",
        "  '''\n",
        "  Create the folder if it does not exist.\n",
        "  Then download the zip file from the web archive if it does not exist.\n",
        "  Finally exctract the zip file of the GloVe txt file does not exist in the folder.\n",
        "  '''\n",
        "\n",
        "  if not os.path.exists(PATHS[\"glove_path\"]):\n",
        "    os.makedirs(PATHS[\"glove_path\"])\n",
        "\n",
        "  if not os.path.exists(PATHS[\"glove_file\"]):\n",
        "    if not os.path.exists(PATHS[\"glove_zip\"]):\n",
        "      download_glove(PATHS[\"url\"])\n",
        "\n",
        "    extract_glove(PATHS[\"glove_zip\"],PATHS[\"glove_path\"])\n",
        "\n",
        "def download_glove(url: str):\n",
        "\n",
        "    '''\n",
        "    Download GloVe's zip file from the web.\n",
        "    '''\n",
        "\n",
        "    urllib.request.urlretrieve(url, PATHS['glove_zip'])\n",
        "    print(\"Successful download\")\n",
        "\n",
        "def extract_glove(zip_file: str,\n",
        "                  glove_path: str):\n",
        "  \n",
        "    '''\n",
        "    Extract GloVe's zip file.\n",
        "    '''\n",
        "  \n",
        "    with zipfile.ZipFile(PATHS[\"glove_zip\"], 'r') as zip_ref:\n",
        "      zip_ref.extractall(path=PATHS[\"glove_path\"])\n",
        "      print(\"Successful extraction\")"
      ],
      "execution_count": 471,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYhDkgNQejX3"
      },
      "source": [
        "## Initialization\n",
        "In this step, the downloaded GloVe file is loaded into an embedding vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfLDOt7le78v"
      },
      "source": [
        "def load_model(glove_file: str) ->Dict:\n",
        "\n",
        "  '''\n",
        "  Open GloVe's txt file and store each of its contained words\n",
        "  into a dictionary along with their correspondent embedding weights.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  glove_file : str\n",
        "      GloVe's txt file path.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  vocabulary: Dict\n",
        "      GloVe's vocabulary\n",
        "\n",
        "  '''\n",
        "\n",
        "  print(\"Loading GloVe Model...\")\n",
        "\n",
        "  with open(glove_file, encoding=\"utf8\" ) as f: #Open the txt file\n",
        "      lines = f.readlines() #Read the file line by line\n",
        "\n",
        "  vocabulary = {}\n",
        "  for line in lines:\n",
        "      splits = line.split()\n",
        "      #Save the first part of the line (word) as the dictionary's key and the second part (the embedding) as the key\n",
        "      vocabulary[splits[0]] = np.array([float(val) for val in splits[1:]])\n",
        "\n",
        "  print(\"GloVe model loaded\")\n",
        "\n",
        "  return vocabulary"
      ],
      "execution_count": 472,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tRukt6Kxx4R"
      },
      "source": [
        "## OOV\n",
        "In this section, some possible \"Out Of Vocabulary\" handling methods are implemented, along with other OOV-related functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-uwA9DYuNtr"
      },
      "source": [
        "#OOV-handling: possible methods\n",
        "\n",
        "PLACEHOLDER = np.random.uniform(low=-0.05, high=0.05, size=EMBEDDING_SIZE)\n",
        "\n",
        "def random_embedding(embedding_size: int) ->np.array:\n",
        "  '''\n",
        "  Return a numpy array with random values sampled from a uniform distribution\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  embedding_size: int\n",
        "    The embedding size that is used as the size of the numpy array.\n",
        "\n",
        "  Results:\n",
        "  -------\n",
        "  np.array\n",
        "  A randomized numpy array.\n",
        "  '''\n",
        "\n",
        "  return np.random.uniform(low=-0.05, high=0.05, size=embedding_size)\n",
        "\n",
        "def placeholder_embedding() ->np.ndarray:\n",
        "\n",
        "  return PLACEHOLDER\n",
        "\n",
        "def neighbours_mean_embedding(word: str,\n",
        "                              glove_embedding: Dict[str,int],\n",
        "                              sentences: List[str],\n",
        "                              labels_list: List[str]):\n",
        "  \n",
        "  '''\n",
        "  Compute the embedding of an OOV word by taking the mean\n",
        "  of its neighbours.\n",
        "\n",
        "  Parameters:\n",
        "  ---------\n",
        "  word: str\n",
        "      The OOV that needs to be embedded.\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding.\n",
        "  sentences: List[List[str]]\n",
        "      A list of all the sentences (lists of words) in the current set.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  mean: int\n",
        "      The mean of the embedding values of OOV-word's neighbours.\n",
        "  '''\n",
        "  \n",
        "  neighbours = set()\n",
        "\n",
        "  for sentence,labels_group in zip(sentences,labels_list):\n",
        "    tokens = sentence.split()\n",
        "    labels = labels_group.split()\n",
        "    for index, token in enumerate(tokens):\n",
        "      if word == token:\n",
        "        if index!=0:\n",
        "          left_neighbour = tokens[index-1]\n",
        "          if left_neighbour in glove_embedding and not is_punctuation_label(labels[index-1]): #Consider only words that are not OOV and not punctuation\n",
        "            neighbours.add(left_neighbour)\n",
        "        if index!=len(tokens)-1:\n",
        "          right_neighbour = tokens[index+1]\n",
        "          if right_neighbour in glove_embedding and not is_punctuation_label(labels[index+1]): #Consider only words that are not OOV and not punctuation\n",
        "            neighbours.add(right_neighbour)\n",
        "\n",
        "  neighbours_embeddings = np.array([glove_embedding[neighbour] for neighbour in neighbours])\n",
        "  return np.mean(neighbours_embeddings) if len(neighbours)>0 else PLACEHOLDER\n",
        "\n",
        "\n",
        "#Others\n",
        "def get_oov_list(words: List[str],\n",
        "                 glove_embedding: Dict[str, int]) ->List[str]:\n",
        "\n",
        "  '''\n",
        "  Return a list of all the words that are not part of the GloVe embedding\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  words: List[str]\n",
        "      A list of unique words from a set of documents.\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding.\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  oov: List[str]\n",
        "      A list of all the OOV terms.\n",
        "  '''\n",
        "\n",
        "  embedding_vocabulary = set(glove_embedding.keys())\n",
        "  oov = set(words).difference(embedding_vocabulary)\n",
        "  return list(oov)"
      ],
      "execution_count": 473,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emKY7vJ-z6Od"
      },
      "source": [
        "##Embedding matrix\n",
        "Now, having opted for an OOV method, it is possible to create the embedding matrix, which associates the embedding to the correspondent word for the entire vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m33pLFGG0ysd"
      },
      "source": [
        "def update_embeddings(glove_embedding: Dict[str, int],\n",
        "                     new_embeddings: Dict[str, int]):\n",
        "  \n",
        "  '''\n",
        "  Update the GloVe's embeddings by adding the new embeddings of\n",
        "  the previous OOV words.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  glove_embedding: Dict[str, int]\n",
        "      GloVe's embedding.\n",
        "  new_embeddings: Dict[str, int]\n",
        "      A dictionary containing the new embeddings\n",
        "      for the analyzed OOV words.\n",
        "  '''\n",
        "  \n",
        "  #Merge GloVe's embeddings with the new discoveries\n",
        "  glove_embedding.update(new_embeddings)\n",
        "\n",
        "def build_embedding_matrix(vocab_size: int,\n",
        "                            glove_embedding: Dict[str, int],\n",
        "                            embedding_size: int,\n",
        "                            words_to_tokens: Dict[str,int],\n",
        "                            oov_method: str,\n",
        "                            sentences: List[str],\n",
        "                            labels: List[str]) ->np.ndarray:\n",
        "\n",
        "  embedding_matrix = np.zeros((vocab_size, embedding_size), dtype=np.float32) #Create an empty embedding matrix\n",
        "\n",
        "  oov_terms = get_oov_list(words_to_tokens.keys(),glove_embedding)\n",
        "  discovered_embeddings = {}\n",
        "\n",
        "  for word, token in tqdm(words_to_tokens.items()):\n",
        "\n",
        "    if np.all((embedding_matrix[token-STARTING_TOKEN] == 0)):\n",
        "\n",
        "      if word in oov_terms: #Hanlde the OOV case with one of the methods\n",
        "        if oov_method == \"Random\":\n",
        "          embedding_vector = random_embedding(embedding_size)\n",
        "        elif oov_method == \"Placeholder\":\n",
        "          embedding_vector = placeholder_embedding()\n",
        "        elif oov_method == \"Mean\":\n",
        "          embedding_vector = neighbours_mean_embedding(word, glove_embedding, sentences, labels)\n",
        "        else:\n",
        "          raise \"Invalid OOV method\"\n",
        "        \n",
        "        discovered_embeddings[word] = embedding_vector\n",
        "\n",
        "      else:\n",
        "        embedding_vector = glove_embedding[word]\n",
        "\n",
        "      embedding_matrix[token-STARTING_TOKEN] = embedding_vector #Update the embedding matrix\n",
        "\n",
        "  #The computed values for the OOV words update the GloVe embeddings at the end of the process.\n",
        "  #Updating these values at runtime affects the \"Mean\" OOV method.\n",
        "  update_embeddings(glove_embedding, discovered_embeddings)\n",
        "\n",
        "  return embedding_matrix"
      ],
      "execution_count": 474,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo7AfoSdCfYO"
      },
      "source": [
        "##Train, validation and test embedding matrices\n",
        "Here all the previous methods defined in the above sections are exploited to create three different vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1BxcRw4CzJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fe6c36-baad-45fa-bf43-87d904ab10f3"
      },
      "source": [
        "setup_files() #Create a path, download and extract the files, if necessary\n",
        "glove_embedding = load_model(PATHS[\"glove_file\"]) #Load the GloVe model\n",
        "\n",
        "#Build the embedding matrix with the training set data\n",
        "train_embedding_matrix = build_embedding_matrix(train_vocab_size,\n",
        "                                                glove_embedding,\n",
        "                                                EMBEDDING_SIZE,\n",
        "                                                train_tokens,\n",
        "                                                OOV_METHOD,\n",
        "                                                train_text,\n",
        "                                                train_labels)\n",
        "\n",
        "#Get an updated version of the embedding matrix with the validation set data\n",
        "val_embedding_matrix = build_embedding_matrix(val_vocab_size,\n",
        "                                                glove_embedding,\n",
        "                                                EMBEDDING_SIZE,\n",
        "                                                val_tokens,\n",
        "                                                OOV_METHOD,\n",
        "                                                val_text,\n",
        "                                                val_labels)\n",
        "\n",
        "#Get an updated version of the embedding matrix with the test set data\n",
        "test_embedding_matrix = build_embedding_matrix(test_vocab_size,\n",
        "                                                glove_embedding,\n",
        "                                                EMBEDDING_SIZE,\n",
        "                                                test_tokens,\n",
        "                                                OOV_METHOD,\n",
        "                                                test_text,\n",
        "                                                test_labels)"
      ],
      "execution_count": 475,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe Model...\n",
            "GloVe model loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7404/7404 [00:03<00:00, 1982.11it/s]\n",
            "100%|██████████| 9901/9901 [00:01<00:00, 7957.08it/s] \n",
            "100%|██████████| 9901/9901 [00:00<00:00, 120762.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1cXXWF_i1yz"
      },
      "source": [
        "## Models' input initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDS5yCuxjIcO"
      },
      "source": [
        "def build_input(embedding_matrix, tokenized_sequence, embedding_size):\n",
        "  n_docs = len(tokenized_sequence)\n",
        "  n_tokens = len(tokenized_sequence[0])\n",
        "\n",
        "  input = np.zeros((n_docs, n_tokens, embedding_size))\n",
        "\n",
        "  for doc_index, tokens in enumerate(tokenized_sequence):\n",
        "    for token_index, token in enumerate(tokens):\n",
        "      if token!=PADDING:\n",
        "        input[doc_index][token_index] = embedding_matrix[token-STARTING_TOKEN]\n",
        "      else:\n",
        "        input[doc_index][token_index] = np.zeros(embedding_size)\n",
        "\n",
        "  return input\n",
        "\n",
        "#Input values\n",
        "X_train = build_input(train_embedding_matrix, train_tokenized, EMBEDDING_SIZE)\n",
        "X_val = build_input(val_embedding_matrix, val_tokenized, EMBEDDING_SIZE)\n",
        "X_test = build_input(test_embedding_matrix, test_tokenized, EMBEDDING_SIZE)\n",
        "\n",
        "#Class values\n",
        "y_train = np.array(list(map(lambda string: encode_string_of_labels(string, labels_encoding,max_length),train_labels)))\n",
        "y_val = np.array(list(map(lambda string: encode_string_of_labels(string, labels_encoding,max_length),val_labels)))\n",
        "y_test = np.array(list(map(lambda string: encode_string_of_labels(string, labels_encoding,max_length),test_labels)))"
      ],
      "execution_count": 476,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MypraiXCuOIb"
      },
      "source": [
        "# Models\n",
        "This section is used for creating different models, going from a baseline to slightly more complicated ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7EPmCLyuZVB"
      },
      "source": [
        "## Constants and utilities\n",
        "First of all, define some constants, parameter dictionaries and methods that will be reused by each architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__iSg0qcABw"
      },
      "source": [
        "# TODO: all the following constants are temporary \n",
        "N_CLASSES = 20  # this must be equal to the number of tags\n",
        "VOCABULARY_SIZE = 1000  # this must be obtained from the dataset\n",
        "MAX_SEQUENCE_SIZE = 100  # this must be obtained from the dataset\n",
        "\n",
        "BATCH_SIZE = 128  # hyper-parameter to properly set\n",
        "EPOCHS = 5\n",
        "\n",
        "\n",
        "# Model common compile information\n",
        "# Use sparse_categorical_crossentropy because labels are one hot encoded\n",
        "model_compile_info = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': 'sparse_categorical_crossentropy',\n",
        "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
        "}\n",
        "\n",
        "# Model common training information\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience=10,\n",
        "                                                restore_best_weights=True)]\n",
        "}"
      ],
      "execution_count": 477,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq-mTGQRL-uo"
      },
      "source": [
        "# This tensor should contain the weights obtained by GloVe\n",
        "embedding_weights = np.zeros(shape=(VOCABULARY_SIZE, EMBEDDING_SIZE))"
      ],
      "execution_count": 478,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stq44tS8CJSy"
      },
      "source": [
        "Define utility methods that will be used to **create**, **train** and **test** the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE7Mvm9sZ8Pf"
      },
      "source": [
        "def create_model(name,\n",
        "                 layers, \n",
        "                 compile_info, \n",
        "                 show_summary=True) -> keras.Model:\n",
        "    \"\"\"\n",
        "    Create the model using the layers passed as parameters.\n",
        "    After the creation, the model is compiled and its summary is possibly \n",
        "    printed to console.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers : array\n",
        "        Array that contains a list of layers that must be added \n",
        "        to the model.\n",
        "    compile_info: Dictionary\n",
        "        Contains information required for compiling the model.\n",
        "    show_summary: bool\n",
        "        If true, then the summary of the model will be printed to console\n",
        "    \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : keras.Model\n",
        "        The keras model.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential(name=name)\n",
        "    \n",
        "    for idx, layer in enumerate(layers):\n",
        "\n",
        "        # Sanity checks for being sure that the last layer has been \n",
        "        # correctly set\n",
        "        if idx == len(layers) - 1:\n",
        "            assert layer.activation == keras.activations.softmax, 'Wrong activation function'\n",
        "            assert layer.units == N_CLASSES, 'Wrong number of units'\n",
        "\n",
        "        model.add(layer)\n",
        "\n",
        "    # Compile\n",
        "    model.compile(**compile_info)\n",
        "\n",
        "    # Print model summary\n",
        "    if show_summary:\n",
        "        model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(model: keras.Model,\n",
        "                x_train: np.ndarray,\n",
        "                y_train: np.ndarray,\n",
        "                x_val: np.ndarray,\n",
        "                y_val: np.ndarray,\n",
        "                training_info: dict):\n",
        "    \"\"\"\n",
        "    Training routine for the Keras model.\n",
        "    At the end of the training, retrieved History data is shown.\n",
        "\n",
        "    :param model: Keras built model\n",
        "    :param x_train: training data in np.ndarray format\n",
        "    :param y_train: training labels in np.ndarray format\n",
        "    :param x_val: validation data in np.ndarray format\n",
        "    :param y_val: validation labels in np.ndarray format\n",
        "    :param training_info: dictionary storing model fit() argument information\n",
        "\n",
        "    :return\n",
        "        model: trained Keras model\n",
        "    \"\"\"\n",
        "    print(\"Start training! \\nParameters: {}\".format(training_info))\n",
        "    history = model.fit(x=x_train, y=y_train,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        **training_info)\n",
        "    print(\"Training completed! Showing history...\")\n",
        "\n",
        "    show_history(history)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_data(model: keras.Model,\n",
        "                 x: np.ndarray,\n",
        "                 prediction_info: dict) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Inference routine of a given input set of examples\n",
        "\n",
        "    :param model: Keras built and possibly trained model\n",
        "    :param x: input set of examples in np.ndarray format\n",
        "    :param prediction_info: dictionary storing model predict() argument information\n",
        "\n",
        "    :return\n",
        "        predictions: predicted labels in np.ndarray format\n",
        "    \"\"\"\n",
        "\n",
        "    print('Starting prediction: \\n{}'.format(prediction_info))\n",
        "    print('Predicting on {} samples'.format(x.shape[0]))\n",
        "\n",
        "    predictions = model.predict(x, **prediction_info)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def evaluate_predictions(predictions: np.ndarray,\n",
        "                         y: np.ndarray,\n",
        "                         metrics: List[Callable],\n",
        "                         metric_names: List[str]):\n",
        "    \"\"\"\n",
        "    Evaluates given model predictions on a list of metric functions\n",
        "\n",
        "    :param predictions: model predictions in np.ndarray format\n",
        "    :param y: ground-truth labels in np.ndarray format\n",
        "    :param metrics: list of metric functions\n",
        "    :param metric_names: list of metric names\n",
        "\n",
        "    :return\n",
        "        metric_info: dictionary containing metric values for each input metric\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(metrics) == len(metric_names)\n",
        "\n",
        "    print(\"Evaluating predictions! Total samples: \", y.shape[0])\n",
        "\n",
        "    metric_info = {}\n",
        "\n",
        "    for metric, metric_name in zip(metrics, metric_names):\n",
        "        metric_value = metric(y_pred=predictions, y_true=y)\n",
        "        metric_info[metric_name] = metric_value\n",
        "\n",
        "    return metric_info\n",
        "\n",
        "def model_sanity_check(model: keras.Model, \n",
        "                       use_embedding_layer: bool = False):\n",
        "    \"\"\"\n",
        "    Create a random input_tensor and try to pass through the model.\n",
        "    This method should be used in order to check if the model is \n",
        "    working as expected.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : keras.Model\n",
        "        The model that must be tested.\n",
        "    use_embedding_layer: depending on this flag the shape of the input must be \n",
        "        treated differently.\n",
        "\n",
        "    \"\"\"\n",
        "    print(f'Sanity check for the model with name: {model.name}')\n",
        "    # Model sanity check for seeing if it runs correctly\n",
        "    if use_embedding_layer:\n",
        "        input_tensor = np.random.uniform(\n",
        "            size=(BATCH_SIZE, MAX_SEQUENCE_SIZE)\n",
        "            )\n",
        "    else:\n",
        "        input_tensor = np.random.uniform(\n",
        "            size=(BATCH_SIZE, MAX_SEQUENCE_SIZE, EMBEDDING_SIZE)\n",
        "            )\n",
        "    print(f'Input tensor shape: {input_tensor.shape}')\n",
        "    output_tensor = model(input_tensor)\n",
        "    print(f'Output tensor shape: {output_tensor.shape}')"
      ],
      "execution_count": 479,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPDiCsbl9ea"
      },
      "source": [
        "Define utility methods for **creating layers** in order to: \n",
        "* reduce the code verbosity.\n",
        "* be sure to always create different architectures with the same layer structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rCmJJgpmBRY"
      },
      "source": [
        "# EMBEDDING\n",
        "# NOTE: Actually this layer has not been used in the final models, \n",
        "# but it has been used for some experimentations\n",
        "def embedding_layer(embedding_weights: np.array,\n",
        "                    layer_name: str='embedding') -> layers.Embedding:\n",
        "    \"\"\"\n",
        "    Create an embedding layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embedding_weights : np.array\n",
        "        The weights for the embedding layer.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Embedding\n",
        "        The created embedding layer.\n",
        "    \"\"\"\n",
        "    layer = layers.Embedding(\n",
        "        input_dim=VOCABULARY_SIZE, \n",
        "        output_dim=EMBEDDING_SIZE, \n",
        "        input_length=MAX_SEQUENCE_SIZE,\n",
        "        weights=[embedding_weights],\n",
        "        mask_zero=True,\n",
        "        name=layer_name\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# MASKING\n",
        "def masking_layer(input_shape: tuple,\n",
        "                  mask_value: float=0.0,\n",
        "                  layer_name: str='masking') -> layers.Masking:\n",
        "    \"\"\"\n",
        "    Create a masking layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_shape : tuple\n",
        "        The weights for the embedding layer.\n",
        "    mask_value : the value to mask because it represents the padding \n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Masking\n",
        "        The created masking layer.\n",
        "    \"\"\"\n",
        "    layer = layers.Masking(\n",
        "        input_shape=input_shape, \n",
        "        mask_value=mask_value,\n",
        "        name=layer_name\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# RNN (LSTM and GRU)\n",
        "def _rnn_size(layer_depth: int) -> int:\n",
        "    \"\"\"\n",
        "    Simple logic used for assigning the number of units \n",
        "    to the rnn layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    size : int\n",
        "        The number units.\n",
        "    \"\"\"\n",
        "    size = 64\n",
        "    if layer_depth > 1:\n",
        "        size = 128\n",
        "    return size\n",
        "\n",
        "def bilstm_layer(layer_depth: int,\n",
        "                 layer_name: str='bi-lstm') -> layers.Bidirectional:\n",
        "    \"\"\"\n",
        "    Create a bidirectional lstm layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Bidirectional\n",
        "        The created bidirectional lstm layer.\n",
        "    \"\"\"\n",
        "    size = _rnn_size(layer_depth)\n",
        "    layer = layers.Bidirectional(\n",
        "        layers.LSTM(size, \n",
        "                    return_sequences=True, \n",
        "                    activation='relu'),\n",
        "                    name=layer_name,\n",
        "                    \n",
        "        )\n",
        "    return layer\n",
        "\n",
        "def bigru_layer(layer_depth: int,\n",
        "                layer_name: str='bi-gru') -> layers.Bidirectional:\n",
        "    \"\"\"\n",
        "    Create a bidirectional gru layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layer_depth : int\n",
        "        The depth of the layer.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Bidirectional\n",
        "        The created bidirectional gru layer.\n",
        "    \"\"\"\n",
        "    size = _rnn_size(layer_depth)\n",
        "    layer = layers.Bidirectional(\n",
        "        layers.GRU(size, \n",
        "                   return_sequences=True, \n",
        "                   activation='relu'),\n",
        "                   name=layer_name\n",
        "        )\n",
        "    return layer\n",
        "\n",
        "# DENSE\n",
        "def _dense_size(last_layer:bool) -> int:\n",
        "    \"\"\"\n",
        "    Simple logic for assigning the size of the dense layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    size : int\n",
        "        The size of the dense layer.\n",
        "    \"\"\"\n",
        "    size = N_CLASSES\n",
        "    if not last_layer:\n",
        "        size = 256\n",
        "    return size\n",
        "\n",
        "def _dense_activation(last_layer:bool) -> str:\n",
        "    \"\"\"\n",
        "    Simple logic for assigning the activation function of the dense layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    activation : str\n",
        "        The activation function of the layer.\n",
        "    \"\"\"\n",
        "    activation = 'relu'\n",
        "    if last_layer:\n",
        "        activation = 'softmax'\n",
        "    return activation\n",
        "\n",
        "def dense_layer(last_layer:bool,\n",
        "                layer_name: str='dense') -> layers.Dense:\n",
        "    \"\"\"\n",
        "    Create a dense layer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    last_layer : bool\n",
        "        Indicates if the layer that must be created is the last\n",
        "        one of the network.\n",
        "    layer_name : str\n",
        "        The name of the layer\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    layer : layers.Dense\n",
        "        The created dense layer.\n",
        "    \"\"\"\n",
        "    size = _dense_size(last_layer)\n",
        "    activation = _dense_activation(last_layer)\n",
        "    \n",
        "    return layers.Dense(size, \n",
        "                        activation=activation, \n",
        "                        name=layer_name)"
      ],
      "execution_count": 480,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOhK3xp2FCUu"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoxVUVMap5Ma",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5625b12-5c6c-46fc-e034-8785b52302d9"
      },
      "source": [
        "# Create layers\n",
        "baseline_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE), \n",
        "                              layer_name='masking_0'),\n",
        "                bilstm_layer(layer_depth=1, \n",
        "                             layer_name='bi-lstm_0'),\n",
        "                dense_layer(last_layer=True, \n",
        "                            layer_name='dense_0')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_model = create_model('baseline', \n",
        "                              baseline_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_model)"
      ],
      "execution_count": 481,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_0 (Masking)         (None, 100, 50)           0         \n",
            "                                                                 \n",
            " bi-lstm_0 (Bidirectional)   (None, 100, 128)          58880     \n",
            "                                                                 \n",
            " dense_0 (Dense)             (None, 100, 20)           2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,460\n",
            "Trainable params: 61,460\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline\n",
            "Input tensor shape: (128, 100, 50)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LGdswmdFg6x"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4l5n888vLql"
      },
      "source": [
        "## Variations\n",
        "What follows is the implementation of small variations to the baseline architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMvk3AQFukOh"
      },
      "source": [
        "### GRU\n",
        "Change the LSTM layer with the GRU layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "856hGaR9eDRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9e9e60-5ca6-466c-e39c-151808d1dfca"
      },
      "source": [
        "# Create layers\n",
        "baseline_var1_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE),\n",
        "                              layer_name='masking_0'),\n",
        "                bigru_layer(layer_depth=1,\n",
        "                            layer_name='bi-gru_0'),\n",
        "                dense_layer(last_layer=True,\n",
        "                            layer_name='dense_0')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var1_model = create_model('baseline_var1', \n",
        "                              baseline_var1_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var1_model)"
      ],
      "execution_count": 482,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline_var1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_0 (Masking)         (None, 100, 50)           0         \n",
            "                                                                 \n",
            " bi-gru_0 (Bidirectional)    (None, 100, 128)          44544     \n",
            "                                                                 \n",
            " dense_0 (Dense)             (None, 100, 20)           2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 47,124\n",
            "Trainable params: 47,124\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline_var1\n",
            "Input tensor shape: (128, 100, 50)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9We6j_USupzN"
      },
      "source": [
        "### Additional LSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLHH94CtqZ7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68fae039-bacd-46fe-eb1c-c27d14c32a43"
      },
      "source": [
        "# Create layers\n",
        "baseline_var2_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE),\n",
        "                              layer_name='masking_0'),\n",
        "                bilstm_layer(layer_depth=1,\n",
        "                             layer_name='bi-lstm_0'),\n",
        "                bilstm_layer(layer_depth=2,\n",
        "                             layer_name='bi-lstm_1'),\n",
        "                dense_layer(last_layer=True,\n",
        "                            layer_name='dense_0')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var2_model = create_model('baseline_var2', \n",
        "                              baseline_var2_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var2_model)"
      ],
      "execution_count": 483,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline_var2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_0 (Masking)         (None, 100, 50)           0         \n",
            "                                                                 \n",
            " bi-lstm_0 (Bidirectional)   (None, 100, 128)          58880     \n",
            "                                                                 \n",
            " bi-lstm_1 (Bidirectional)   (None, 100, 256)          263168    \n",
            "                                                                 \n",
            " dense_0 (Dense)             (None, 100, 20)           5140      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 327,188\n",
            "Trainable params: 327,188\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline_var2\n",
            "Input tensor shape: (128, 100, 50)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RN6ySWyu13J"
      },
      "source": [
        "### Additional Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujfM5mekqma8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d85e70d-a675-4c27-ab72-09751ed1de42"
      },
      "source": [
        "# Create layers\n",
        "baseline_var3_layers = [\n",
        "                # embedding_layer(embedding_weights=embedding_weights),\n",
        "                masking_layer(input_shape=(MAX_SEQUENCE_SIZE, EMBEDDING_SIZE),\n",
        "                              layer_name='masking_0'),\n",
        "                bilstm_layer(layer_depth=1,\n",
        "                             layer_name='bi-lstm_0'),\n",
        "                dense_layer(last_layer=False,\n",
        "                            layer_name='dense_0'),\n",
        "                dense_layer(last_layer=True,\n",
        "                            layer_name='dense_1')\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "baseline_var3_model = create_model('baseline_var3', \n",
        "                              baseline_var3_layers, \n",
        "                              model_compile_info)\n",
        "\n",
        "# Check if the model can actually run\n",
        "model_sanity_check(baseline_var3_model)"
      ],
      "execution_count": 484,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline_var3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking_0 (Masking)         (None, 100, 50)           0         \n",
            "                                                                 \n",
            " bi-lstm_0 (Bidirectional)   (None, 100, 128)          58880     \n",
            "                                                                 \n",
            " dense_0 (Dense)             (None, 100, 256)          33024     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100, 20)           5140      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 97,044\n",
            "Trainable params: 97,044\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Sanity check for the model with name: baseline_var3\n",
            "Input tensor shape: (128, 100, 50)\n",
            "Output tensor shape: (128, 100, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwOShFdwvaHN"
      },
      "source": [
        "# Training and Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1SYhjS1vvzF"
      },
      "source": [
        "# Disussion and Error Analysis"
      ]
    }
  ]
}
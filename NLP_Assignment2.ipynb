{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1LWnzR6u13"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from functools import reduce\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# typing\n",
        "from typing import List, Callable, Dict"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xBYfRb6cjF"
      },
      "source": [
        "# Data pipeline\n",
        "The goal of this section is to convert initial textual input into a numerical format that is compatible with our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQIk9xwZ8jd"
      },
      "source": [
        "## Data Loading\n",
        "Download the dataset and save it to file system.\n",
        "The dataset is composed by 3 csv files: train, validation and test. These 3 csv files will be loaded directly into 3 different dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdVaaLhS6bNc"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "download_data('dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZeR75ZaabyS"
      },
      "source": [
        "Load the files into different data frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxE3Zp2I6y80"
      },
      "source": [
        "# Be sure all columns have a name\n",
        "column_names = ['Row', 'Claim', 'Evidence', 'ID', 'Label']\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv ('dataset/train_pairs.csv', names=column_names, header=0)\n",
        "df_val = pd.read_csv ('dataset/val_pairs.csv', names=column_names, header=0)\n",
        "df_test = pd.read_csv ('dataset/test_pairs.csv', names=column_names, header=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "lGaexishbQFT",
        "outputId": "56e99c6d-0334-49c8-a039-e39b72e7b1e6"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row</th>\n",
              "      <th>Claim</th>\n",
              "      <th>Evidence</th>\n",
              "      <th>ID</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Chris Hemsworth appeared in A Perfect Getaway.</td>\n",
              "      <td>2\\tHemsworth has also appeared in the science ...</td>\n",
              "      <td>3</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Roald Dahl is a writer.</td>\n",
              "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...</td>\n",
              "      <td>7</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Roald Dahl is a governor.</td>\n",
              "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...</td>\n",
              "      <td>8</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Ireland has relatively low-lying mountains.</td>\n",
              "      <td>10\\tThe island 's geography comprises relative...</td>\n",
              "      <td>9</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Ireland does not have relatively low-lying mou...</td>\n",
              "      <td>10\\tThe island 's geography comprises relative...</td>\n",
              "      <td>10</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Row                                              Claim  ...  ID     Label\n",
              "0    0     Chris Hemsworth appeared in A Perfect Getaway.  ...   3  SUPPORTS\n",
              "1    1                            Roald Dahl is a writer.  ...   7  SUPPORTS\n",
              "2    2                          Roald Dahl is a governor.  ...   8   REFUTES\n",
              "3    3        Ireland has relatively low-lying mountains.  ...   9  SUPPORTS\n",
              "4    4  Ireland does not have relatively low-lying mou...  ...  10   REFUTES\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMfITa8PbVR6"
      },
      "source": [
        "## Data Pre-processing\n",
        "Perform text cleaning and tokenization operations.\n",
        "Start by creating some utility methods that will be used for cleaning the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB4obklU6-jP"
      },
      "source": [
        "# Special characters to remove: /(){}[]|@,;\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "\n",
        "# Accepted symbols:\n",
        "# - numbers between 0-9\n",
        "# - all lower cased letters\n",
        "# - whitespace, #, + _\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "# - ^ begnning of a string\n",
        "# - \\d any digit\n",
        "# - \\s whitespaces and tabs\n",
        "BEGINNING_IDS_RE = re.compile('^\\d*\\s*')\n",
        "\n",
        "# The stopwords are a list of words that are very very common but don’t \n",
        "# provide useful information for most text analysis procedures.\n",
        "# Therefore, they will be removed from the dataset\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def filter_out_uncommon_symbols(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Method used for removing most common words\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def replace_ids(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Method used for removing ids and some whitespaces that could appear\n",
        "    at the beginning of the text.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    return BEGINNING_IDS_RE.sub('', text)\n",
        "\n",
        "GENERIC_PREPROCESSING_PIPELINE = [\n",
        "                                  lower,\n",
        "                                  replace_special_characters,\n",
        "                                  filter_out_uncommon_symbols,\n",
        "                                  remove_stopwords,\n",
        "                                  strip_text\n",
        "                                  ]\n",
        "\n",
        "EVIDENCES_PREPROCESSING_PIPELINE = GENERIC_PREPROCESSING_PIPELINE\n",
        "EVIDENCES_PREPROCESSING_PIPELINE.insert(0, replace_ids)\n",
        "    \n",
        "\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "    filter_methods = \\\n",
        "        filter_methods if filter_methods is not None else GENERIC_PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKct7tDs0o3W"
      },
      "source": [
        "Now we are ready to pre-process the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ochx9GctfdkX"
      },
      "source": [
        "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Replace each sentence with its pre-processed version\n",
        "    df['Evidence'] = df['Evidence'].apply(\n",
        "        lambda txt: text_prepare(txt, filter_methods=EVIDENCES_PREPROCESSING_PIPELINE)\n",
        "        )\n",
        "    df['Claim'] = df['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_train = preprocess_dataset(df_train)\n",
        "df_val = preprocess_dataset(df_val)\n",
        "df_test = preprocess_dataset(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "d-SLZC-5nytP",
        "outputId": "7e83af6e-fc91-4534-af61-b6c5306aa654"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row</th>\n",
              "      <th>Claim</th>\n",
              "      <th>Evidence</th>\n",
              "      <th>ID</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>chris hemsworth appeared perfect getaway</td>\n",
              "      <td>hemsworth also appeared science fiction action...</td>\n",
              "      <td>3</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>roald dahl writer</td>\n",
              "      <td>roald dahl lrb lsb langpronrold _ dl rsb lsb u...</td>\n",
              "      <td>7</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>roald dahl governor</td>\n",
              "      <td>roald dahl lrb lsb langpronrold _ dl rsb lsb u...</td>\n",
              "      <td>8</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>ireland relatively lowlying mountains</td>\n",
              "      <td>island geography comprises relatively lowlying...</td>\n",
              "      <td>9</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>ireland relatively lowlying mountains</td>\n",
              "      <td>island geography comprises relatively lowlying...</td>\n",
              "      <td>10</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Row                                     Claim  ...  ID     Label\n",
              "0    0  chris hemsworth appeared perfect getaway  ...   3  SUPPORTS\n",
              "1    1                         roald dahl writer  ...   7  SUPPORTS\n",
              "2    2                       roald dahl governor  ...   8   REFUTES\n",
              "3    3     ireland relatively lowlying mountains  ...   9  SUPPORTS\n",
              "4    4     ireland relatively lowlying mountains  ...  10   REFUTES\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "kfIWefidXvm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Constants and utilities"
      ],
      "metadata": {
        "id": "-HxWGraEYKAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample values\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_SIZE = 64\n",
        "MAX_TOKENS = 100\n",
        "EPOCHS = 5\n",
        "VOCABULARY_SIZE = 1000\n",
        "\n",
        "RNN_UNITS = 128\n",
        "DENSE_UNITS = 256\n",
        "MLP_LAYERS = 3\n",
        "DENSE_CLASSIFICATION_LAYERS = 3\n",
        "\n",
        "EMBEDDING_MODE = \"Simple\" #This must be one between \"Simple\", \"GloVe static\" and \"GloVe dynamic\"\n",
        "SENTENCE_EMBEDDING_MODE = \"RNN last\" #This must be one between \"RNN last\", \"RNN mean\", \"Bag of vectors\", \"MLP\"\n",
        "RNN_MODEL = \"GRU\" #This must be one between \"GRU\" and \"LSTM\"\n",
        "MERGE_MODE = \"Concatenate\" #This must be one between \"Concatenate\", \"Sum\" and \"Mean\"\n",
        "\n",
        "model_compile_info = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': 'sparse_categorical_crossentropy',\n",
        "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
        "}\n",
        "\n",
        "# Model common training information\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience=10,\n",
        "                                                restore_best_weights=True)]\n",
        "}\n",
        "\n",
        "# Model common prediction information\n",
        "prediction_info = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'verbose': 1\n",
        "}"
      ],
      "metadata": {
        "id": "1w41RoSZYako"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inputs"
      ],
      "metadata": {
        "id": "ESKdNELYZJKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "claims_input = keras.Input(shape=(MAX_TOKENS,EMBEDDING_SIZE), name=\"claims\")\n",
        "evidences_input = keras.Input(shape=(MAX_TOKENS,EMBEDDING_SIZE), name=\"evidences\")"
      ],
      "metadata": {
        "id": "JaU_BEJyZLOO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Embedding"
      ],
      "metadata": {
        "id": "CFWnFX0AX0Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_embedding_layer(vocab_size,\n",
        "                    embedding_size,\n",
        "                    max_length,\n",
        "                    layer_name,\n",
        "                    pre_trained_weights=None,\n",
        "                    train=True):\n",
        "  \n",
        "  if pre_trained_weights==None:\n",
        "    layer = layers.Embedding(\n",
        "        input_dim=vocab_size, \n",
        "        output_dim=embedding_size, \n",
        "        input_length=max_length,\n",
        "        mask_zero=True,\n",
        "        trainable=train,\n",
        "        name=layer_name\n",
        "        )\n",
        "  \n",
        "  else:\n",
        "    layer = layers.Embedding(\n",
        "          input_dim=vocab_size, \n",
        "          output_dim=embedding_size, \n",
        "          input_length=max_length,\n",
        "          weights=[pre_trained_weights],\n",
        "          mask_zero=True,\n",
        "          trainable=train,\n",
        "          name=layer_name\n",
        "          )\n",
        "  \n",
        "  return layer\n",
        "\n",
        "#TODO: implement the other embedding approaches"
      ],
      "metadata": {
        "id": "ae_meKiMX5Co"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence embedding"
      ],
      "metadata": {
        "id": "sjCzqXstfH91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, rnn_size, layer_name, mode, rnn_model):\n",
        "    super(RNN_Last, self).__init__()\n",
        "\n",
        "    self.name = layer_name\n",
        "    self.mode = mode\n",
        "\n",
        "    rnn_params= {units:rnn_size,\n",
        "                 return_sequences:True,\n",
        "                 return_state:True,\n",
        "                 activation:\"tanh\"}\n",
        "\n",
        "    if rnn_model==\"GRU\":\n",
        "      layer = layers.GRU(rnn_params)\n",
        "    elif rnn_model==\"LSTM\":\n",
        "      layer = layers.LSTM(rnn_params)\n",
        "    else:\n",
        "      raise \"Invalid RNN model. Use 'GRU' or 'LSTM'\"\n",
        "\n",
        "    self.rnn = layers.Bidirectional(layer)\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    whole_seq_output, final_memory_state, final_carry_state = self.rnn(inputs)\n",
        "\n",
        "    if mode==\"RNN last\":\n",
        "      return final_memory_state\n",
        "    elif mode==\"RNN mean\":\n",
        "      return tf.reduce_mean(whole_seq_output,axis=0) #axis=0 ->mean on max_tokens dim\n",
        "    else:\n",
        "      raise \"Invalid Mode. Use 'RNN Last' or 'RNN Mean'\"\n",
        "\n",
        "class Bag_Of_Vectors_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, layer_name):\n",
        "    super(Bag_Of_Vectors_Sentence_Embedding, self).__init__()\n",
        "    self.name=layer_name\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.reduce_mean(inputs, axis=0)  #axis=0 ->mean on max_tokens dim\n",
        "\n",
        "class MLP_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, dense_units, intermediate_dense_activation, last_dense_activation, n_layers, layer_name):\n",
        "    super(MLP_Sentence_Embedding,self).__init__()\n",
        "\n",
        "    self.name=layer_name\n",
        "    self.n_layers = n_layers\n",
        "    self.last_dense_activation = last_dense_activation\n",
        "\n",
        "    self.intermediate_dense_layer = layers.Dense(units=dense_units,\n",
        "                                    activation=intermediate_dense_activation)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    inputs_shape = tf.shape(inputs).numpy()\n",
        "    max_tokens = inputs_shape[0]\n",
        "    embedding_size = inputs_shape[1]\n",
        "\n",
        "    x = tf.reshape(inputs,[max_tokens*embedding_size])\n",
        "\n",
        "    for i in range(self.n_layers-1):\n",
        "      x = self.intermediate_dense_layer(x)\n",
        "    \n",
        "    last_dense_layer = layers.Dense(units=embedding_size, activation=self.last_dense_activation)\n",
        "    return last_dense_layer(x)\n",
        "\n",
        "def sentence_embedding_layer(mode, rnn_units, rnn_model, dense_units, mlp_layers, layer_name):\n",
        "  if mode==\"RNN last\" or mode==\"RNN mean\":\n",
        "    layer = RNN_Sentence_Embedding(rnn_units,layer_name,mode,rnn_model)\n",
        "  elif mode==\"Bag of vectors\":\n",
        "    layer = Bag_Of_Vectors_Sentence_Embedding(layer_name)\n",
        "  elif mode==\"MLP\":\n",
        "    layer = MLP_Sentence_Embedding(dense_units,\"relu\",\"tanh\",mlp_layers,layer_name)\n",
        "  else:\n",
        "    raise \"Invalid Mode.\"\n",
        "\n",
        "  return layer"
      ],
      "metadata": {
        "id": "UYD2ZsiPfJn7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merge inputs"
      ],
      "metadata": {
        "id": "_lOhQ2Jd1G_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate(layer_name):\n",
        "\n",
        "  return layers.Concatenate(name=layer_name)\n",
        "\n",
        "def sum(layer_name):\n",
        "\n",
        "  return layers.Add(name=layer_name)\n",
        "\n",
        "def mean(layer_name):\n",
        "\n",
        "  return layers.Average(name=layer_name)\n",
        "\n",
        "def merge_layer(merge_mode, layer_name):\n",
        "  if merge_mode==\"Concatenate\":\n",
        "    return concatenate\n",
        "  elif merge_mode==\"Sum\":\n",
        "    return sum(layer_name)\n",
        "  elif merge_mode==\"Mean\":\n",
        "    return mean(layer_name)\n",
        "  else:\n",
        "    raise \"Invalid merge mode.\""
      ],
      "metadata": {
        "id": "wY3cQUZW1KlK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification"
      ],
      "metadata": {
        "id": "g3zTpOcK4Qdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_classification_layer(dense_units, activation_function, layer_name, last):\n",
        "\n",
        "  if last:\n",
        "    layer= layers.Dense(units=1,\n",
        "                        activation=\"softmax\",\n",
        "                        name=layer_name)\n",
        "    \n",
        "  else:\n",
        "    layer= layers.Dense(units=dense_units,\n",
        "                        activation=dense_activation,\n",
        "                        name=layer_name)\n",
        "  \n",
        "  return layer\n",
        "\n",
        "#TODO: add other classification architectures"
      ],
      "metadata": {
        "id": "rJd89dcZ4UNv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build model"
      ],
      "metadata": {
        "id": "bHGeioKS5soW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building layers\n",
        "layer_embedded_tokens = token_embedding_layer(VOCABULARY_SIZE,\n",
        "                                         EMBEDDING_SIZE,\n",
        "                                         MAX_TOKENS,\n",
        "                                         \"token embedding\")\n",
        "\n",
        "layer_embedded_sentences = sentence_embedding_layer(SENTENCE_EMBEDDING_MODE,\n",
        "                                                           RNN_UNITS,\n",
        "                                                           RNN_MODEL,\n",
        "                                                           DENSE_UNITS,\n",
        "                                                           MLP_LAYERS,\n",
        "                                                           \"sentences embedding\")\n",
        "\n",
        "layer_merge = merge_layer(\"merge\")\n",
        "\n",
        "layer_classification = dense_classification_layer(DENSE_UNITS,\n",
        "                                                  \"relu\",\n",
        "                                                  \"intermidiate classification\",\n",
        "                                                  False)\n",
        "layer_output = dense_classification_layer(None,\n",
        "                                          None,\n",
        "                                          \"output classification\",\n",
        "                                          True)\n",
        "\n",
        "#Building model\n",
        "claims_tokens_embedded = layer_embedded_tokens(claims_input)\n",
        "evidences_tokens_embedded = layer_embedded_tokens(evidences_input)\n",
        "\n",
        "claims_sentences_embedded = layer_embedded_sentences(claims_tokens_embedded)\n",
        "evidences_sentences_embedded = layer_embedded_sentences(evidences_tokens_embedded)\n",
        "\n",
        "classification_input = layer_merge(claims_sentences_embedded,evidences_sentences_embedded)\n",
        "\n",
        "for i in range(DENSE_CLASSIFICATION_LAYERS-1):\n",
        "  classification_input = layer_classification(classification_input)\n",
        "\n",
        "classification_output = layer_output(classification_input)\n",
        "\n",
        "# Instantiate an end-to-end model\n",
        "model = keras.Model(\n",
        "    inputs=[claims_input, evidences_input],\n",
        "    outputs=[classification_output]\n",
        ")"
      ],
      "metadata": {
        "id": "CAuCaRkP5viv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1LWnzR6u13"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from functools import reduce, partial\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize,WhitespaceTokenizer\n",
        "\n",
        "# typing\n",
        "from typing import List, Callable, Dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xBYfRb6cjF"
      },
      "source": [
        "# Data pipeline\n",
        "The goal of this section is to convert initial textual input into a numerical format that is compatible with our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQIk9xwZ8jd"
      },
      "source": [
        "## Data Loading\n",
        "Download the dataset and save it to file system.\n",
        "The dataset is composed by 3 csv files: train, validation and test. These 3 csv files will be loaded directly into 3 different dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdVaaLhS6bNc"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "download_data('dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZeR75ZaabyS"
      },
      "source": [
        "Load the files into different data frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxE3Zp2I6y80"
      },
      "source": [
        "# Be sure all columns have a name\n",
        "column_names = ['Row', 'Claim', 'Evidence', 'ID', 'Label']\n",
        "\n",
        "# Load and split the datasets\n",
        "df_train = pd.read_csv ('dataset/train_pairs.csv', names=column_names, header=0)\n",
        "df_val = pd.read_csv ('dataset/val_pairs.csv', names=column_names, header=0)\n",
        "df_test = pd.read_csv ('dataset/test_pairs.csv', names=column_names, header=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGaexishbQFT"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMfITa8PbVR6"
      },
      "source": [
        "## Data Pre-processing\n",
        "Perform text cleaning and tokenization operations.\n",
        "Start by creating some utility methods that will be used for cleaning the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSRATkFJeOgk"
      },
      "source": [
        "# Special characters to remove: /(){}[]|@,;\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "\n",
        "# Accepted symbols:\n",
        "# - numbers between 0-9\n",
        "# - all lower cased letters\n",
        "# - whitespace, #, + _\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "# - ^ begnning of a string\n",
        "# - \\d any digit\n",
        "# - \\s whitespaces and tabs\n",
        "BEGINNING_IDS_RE = re.compile('^\\d*\\s*')\n",
        "\n",
        "# Remove multiple whitespaces, tabs and newlines\n",
        "EXTRA_WHITE_SPACE_RE = re.compile('/\\s\\s+/g')\n",
        "\n",
        "TAGS_TO_REMOVE = ['-LCB-', '-RCB-', '-LSB-', '-RSB-', '-RRB', '-LRB-']\n",
        "\n",
        "\n",
        "# The stopwords are a list of words that are very very common but donâ€™t \n",
        "# provide useful information for most text analysis procedures.\n",
        "# Therefore, they will be removed from the dataset\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "nltk.download('punkt') # necessary for being able to tokenize\n",
        "nltk.download('wordnet') \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def filter_out_uncommon_symbols(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Method used for removing most common words\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def replace_ids(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Method used for removing ids and some whitespaces that could appear\n",
        "    at the beginning of the text.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    return BEGINNING_IDS_RE.sub('', text)\n",
        "\n",
        "def lemsent(sentence):\n",
        "    \"\"\"\n",
        "    Method used for lemmatize text.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    #words = [lemmatizer.lemmatize(word) for word in tokenizer.tokenize(str(sentence))]\n",
        "    words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]\n",
        "    return \" \".join(words)\n",
        "\n",
        "def remove_pos_tags(text: str) -> str:\n",
        "  '''\n",
        "  Remove POS tags from the text\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text: str\n",
        "    Text where tags should be removed.\n",
        "\n",
        "  Returns\n",
        "  --------\n",
        "  text: str\n",
        "    Input text without POS tags\n",
        "  '''\n",
        "\n",
        "  for tag in TAGS_TO_REMOVE:\n",
        "    text = re.sub(tag, '', text)\n",
        "  return text\n",
        "\n",
        "def remove_wikipedia_tags(text: str) -> str:\n",
        "  '''\n",
        "  Remove Wikipedia tags at the end of a sentence.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text: str\n",
        "    Text where tags should be removed.\n",
        "\n",
        "  Returns\n",
        "  --------\n",
        "  text: str\n",
        "    Input text without Wikipedia tags\n",
        "  '''\n",
        "\n",
        "  return text.split('.\\t')[0]\n",
        "\n",
        "#List of all the preprocessing methods to be called\n",
        "GENERIC_PREPROCESSING_PIPELINE = [\n",
        "                                  remove_wikipedia_tags,\n",
        "                                  remove_pos_tags,\n",
        "                                  lower,\n",
        "                                  replace_special_characters,\n",
        "                                  filter_out_uncommon_symbols,\n",
        "                                  #remove_stopwords,\n",
        "                                  strip_text,\n",
        "                                  lemsent\n",
        "                                  ]\n",
        "\n",
        "EVIDENCES_PREPROCESSING_PIPELINE = GENERIC_PREPROCESSING_PIPELINE\n",
        "EVIDENCES_PREPROCESSING_PIPELINE.insert(0, replace_ids)\n",
        "    \n",
        "\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "    filter_methods = \\\n",
        "        filter_methods if filter_methods is not None else GENERIC_PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKct7tDs0o3W"
      },
      "source": [
        "Now we are ready to pre-process the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ochx9GctfdkX"
      },
      "source": [
        "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
        "  '''\n",
        "  Apply the preprocessing operations to the dataframe.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  df: pd.DataFrame\n",
        "    Dataframe to be preprocessed\n",
        "\n",
        "  Returns\n",
        "  --------\n",
        "  df: pd.DataFrame\n",
        "    Preprocessed dataframe\n",
        "  '''\n",
        "\n",
        "  # Replace each sentence with its pre-processed version\n",
        "  df['Evidence'] = df['Evidence'].apply(\n",
        "      lambda txt: text_prepare(txt, filter_methods=EVIDENCES_PREPROCESSING_PIPELINE)\n",
        "      )\n",
        "  df['Claim'] = df['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "  \n",
        "  return df\n",
        "\n",
        "df_train = preprocess_dataset(df_train)\n",
        "df_val = preprocess_dataset(df_val)\n",
        "df_test = preprocess_dataset(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates \n",
        "df_train = df_train[~df_train.duplicated(subset=['Claim','Evidence'], keep='first')]\n",
        "df_val = df_val[~df_val.duplicated(subset=['Claim','Evidence'], keep='first')]\n",
        "df_test = df_test[~df_test.duplicated(subset=['Claim','Evidence'], keep='first')]"
      ],
      "metadata": {
        "id": "UBW8x2mIyAmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell used for testing purposes to look for a class imbalance\n",
        "label_count = df_train['Label'].value_counts()\n",
        "pos = label_count['SUPPORTS']\n",
        "neg = label_count['REFUTES']\n",
        "print(pos, neg)\n",
        "\n",
        "initial_bias = np.log([pos/neg])\n",
        "print(initial_bias)"
      ],
      "metadata": {
        "id": "ow1qiisKHAW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and vocabularies\n",
        "Here each set is passed through a tokenization process, which also allows to define the vocabulary of each set and also their vocabulary size. Furthermore the maximum length of a token sequence is defined and the labels are extracted from the sets."
      ],
      "metadata": {
        "id": "l7_gqoiCd3tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STARTING_TOKEN = 1 #First value to start the tokenization on (0 is already used as padding value)\n",
        "QUANTILE = 0.99 #Quantile used to reduce the max number of tokens and save space\n",
        "\n",
        "def get_tokenizer(corpus: List[str])->Dict[str,int]:\n",
        "  '''\n",
        "  Create a tokenization dictionary that associates an integer to each word.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  corpus: List[str]\n",
        "    Text to examine searching for new words to add into the dictionary.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    A filled dictionary that associates an integer to each word\n",
        "  '''\n",
        "\n",
        "  #Initialize the dictionary\n",
        "  words_to_tokens = {}\n",
        "\n",
        "  #Fill dictionary with new words\n",
        "  for text in corpus:\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "      if not word in words_to_tokens:\n",
        "        #Add a new word with incremental token number\n",
        "        words_to_tokens[word] = len(words_to_tokens)+STARTING_TOKEN\n",
        "\n",
        "  return words_to_tokens\n",
        "\n",
        "def tokenize(word: str,\n",
        "             words_to_tokens: Dict[str,int])->int:\n",
        "  '''\n",
        "  Get the integer value of a given token.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  word: str\n",
        "    Token\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  int:\n",
        "    Value associated to the token\n",
        "  '''\n",
        "\n",
        "  return words_to_tokens[word]\n",
        "\n",
        "def detokenize(token:int,\n",
        "               words_to_tokens: Dict[str,int])->str:\n",
        "  '''\n",
        "  Get the token-word of a given token-value.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  token: int\n",
        "    Tokenized word\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  str:\n",
        "    Word associated to the token-value\n",
        "  '''\n",
        "\n",
        "  val_list = list(words_to_tokens.values())\n",
        "  key_list = list(words_to_tokens.keys())\n",
        "\n",
        "  position = val_list.index(token)\n",
        "\n",
        "  return key_list[position]\n",
        "\n",
        "def tokenize_string(string: str,\n",
        "                    words_to_tokens: Dict[str,int],\n",
        "                    max_length: int)->List[int]:\n",
        "\n",
        "  '''\n",
        "  Get the tokenized sequence of a string of separated tokens (document/sentence).\n",
        "\n",
        "  Parameters:\n",
        "  string: str\n",
        "    String of separated tokens (document or sentence)\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "  max_length: int\n",
        "    Tokenization length\n",
        "\n",
        "  Returns:\n",
        "    List[int]:\n",
        "      A list of token-values where each one is the tokenized value of a token\n",
        "      int the input-string.\n",
        "      The list is padded if its length is below the max_length.\n",
        "      The list is truncated if its length is above the max_length.\n",
        "  '''\n",
        "\n",
        "  tokens = string.split()\n",
        "  tokenized_sequence = [tokenize(token, words_to_tokens)  for token in tokens]\n",
        "  length_diff = max_length-len(tokenized_sequence)\n",
        "\n",
        "  if length_diff==0: # Return the same sequence if it has the requested size\n",
        "    return tokenized_sequence\n",
        "  elif length_diff<0: # Return the truncated sequence if it exceeds the requested size\n",
        "    return tokenized_sequence[0:max_length]\n",
        "  else: # Return the padded sequence if it has an inferior size than the expected one\n",
        "    return np.pad(tokenized_sequence, (0, length_diff), 'constant').tolist()\n",
        "\n",
        "def label_to_binary(label: str) ->int:\n",
        "  '''\n",
        "  Convert a label into a binary value (0 or 1).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  label: str\n",
        "    Label to be converted.\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  int\n",
        "    Binary conversion of the label.\n",
        "  '''\n",
        "\n",
        "  if label==\"SUPPORTS\":\n",
        "    return 1\n",
        "  elif label==\"REFUTES\":\n",
        "    return 0\n",
        "  else:\n",
        "    raise \"Invalid label.\"\n",
        "\n",
        "#Define corpus\n",
        "train_text_claim = df_train[\"Claim\"].tolist()\n",
        "train_text_evidence = df_train[\"Evidence\"].tolist()\n",
        "val_text_claim = df_val[\"Claim\"].tolist()\n",
        "val_text_evidence = df_val[\"Evidence\"].tolist()\n",
        "test_text_claim = df_test[\"Claim\"].tolist()\n",
        "test_text_evidence = df_test[\"Evidence\"].tolist()\n",
        "\n",
        "#Define labels\n",
        "train_labels = df_train[\"Label\"].tolist()\n",
        "val_labels = df_val[\"Label\"].tolist()\n",
        "test_labels = df_test[\"Label\"].tolist()\n",
        "\n",
        "#Token dictionary\n",
        "corpus = train_text_claim+train_text_evidence+val_text_claim+val_text_evidence+test_text_claim+test_text_evidence\n",
        "tokens_dictionary = get_tokenizer(corpus)\n",
        "\n",
        "#Vocabulary\n",
        "tokens_vocabulary = tokens_dictionary.keys()\n",
        "\n",
        "#Vocab size\n",
        "vocabulary_size = len(tokens_vocabulary)+STARTING_TOKEN #+1 to include padding value\n",
        "\n",
        "#Max length of a token sequence\n",
        "n_tokens = [len(doc.split()) for doc in corpus]\n",
        "max_length = int(np.quantile(n_tokens,QUANTILE))\n",
        "\n",
        "#Tokenized sets\n",
        "train_claims_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),train_text_claim)))\n",
        "train_evidences_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),train_text_evidence)))\n",
        "\n",
        "val_claims_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),val_text_claim)))\n",
        "val_evidences_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),val_text_evidence)))\n",
        "\n",
        "test_claims_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),test_text_claim)))\n",
        "test_evidences_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),test_text_evidence)))\n",
        "\n",
        "#Tokenized labels\n",
        "train_labels_tokenized = np.array(list(map(label_to_binary,train_labels)))\n",
        "val_labels_tokenized = np.array(list(map(label_to_binary,val_labels)))\n",
        "test_labels_tokenized = np.array(list(map(label_to_binary,test_labels)))"
      ],
      "metadata": {
        "id": "vT8NNqzXCMkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models\n",
        "\n",
        "This macro section is related with everything that concerns creating, training, testing and evaluating the NN models."
      ],
      "metadata": {
        "id": "kfIWefidXvm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Constants and utilities\n",
        "\n",
        "Defining a set of variables, whose values are fixed, that determines how the model will be created, trained, tested and evaluated. "
      ],
      "metadata": {
        "id": "-HxWGraEYKAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample values\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_SIZE = 64 #Embedding size for a token and subsequently an entire sentence\n",
        "EPOCHS = 50 #Number of training epochs\n",
        "\n",
        "DENSE_UNITS = 64 #Max number of units to use in Dense layers\n",
        "MLP_LAYERS = 2 #Number of Dense layers used in the MLP sentence embedding; one will be a dropout layer\n",
        "DENSE_CLASSIFICATION_LAYERS = 2 #Number of Dense layers used in the classification; one will be a dropout layer\n",
        "\n",
        "L2_RATE = 0.01 #Value used in L2 regularization\n",
        "DROPOUT_RATE = 0.4 #Dropout value for Dropout layers\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "SENTENCE_EMBEDDING_MODE = \"RNN mean\" #This must be one between \"RNN last\", \"RNN mean\", \"Bag of vectors\", \"MLP\"\n",
        "RNN_MODEL = \"LSTM\" #This must be one between \"GRU\" and \"LSTM\"\n",
        "MERGE_MODE = \"Concatenate\" #This must be one between \"Concatenate\", \"Sum\" and \"Mean\"\n",
        "APPLY_COSINE_SIMILARITY = True #Simple extension task using cosine similarity\n",
        "CLAIM_VERIFICATION_EVALUATION = True #Evaluate using the claim verification evaluation method (if True)\n",
        "\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "#Binary cross entropy and binary accuracy are adopted since this is a binary classification task\n",
        "LOSS = tf.keras.losses.BinaryCrossentropy()\n",
        "METRICS = [tf.keras.metrics.BinaryAccuracy()]\n",
        "\n",
        "# Model common training information, early stopping is used in case of overfitting\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience=10,\n",
        "                                                restore_best_weights=True)]\n",
        "}\n",
        "\n",
        "# Model common prediction information\n",
        "prediction_info = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'verbose': 1\n",
        "}"
      ],
      "metadata": {
        "id": "1w41RoSZYako"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inputs\n",
        "\n",
        "Defining the Input layers for keras."
      ],
      "metadata": {
        "id": "ESKdNELYZJKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "claims_input = keras.Input(shape=(max_length), name=\"claims\")\n",
        "evidences_input = keras.Input(shape=(max_length), name=\"evidences\")"
      ],
      "metadata": {
        "id": "JaU_BEJyZLOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Token embedding\n",
        "\n",
        "Defining the building of the token embedding layer, the one responsible for token embedding."
      ],
      "metadata": {
        "id": "CFWnFX0AX0Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_embedding_layer(vocab_size: int,\n",
        "                    embedding_size: int,\n",
        "                    max_length: int,\n",
        "                    layer_name: str,\n",
        "                    pre_trained_weights=None,\n",
        "                    train=True) ->keras.layers.Layer:\n",
        "  \n",
        "  '''\n",
        "  Define a keras layer for embedding.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  vocab_size: int\n",
        "    Vocabulary size\n",
        "  embedding_size: int\n",
        "    Embedding size\n",
        "  max_length: int\n",
        "    Max number of tokens\n",
        "  layer_name: str\n",
        "    Name to be assigned to the embedding layer\n",
        "  pre_trained_weights\n",
        "    Pre-trained weights to initialize the layer (optional)\n",
        "  train: bool\n",
        "    Decide if training or not the weights (optional)\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  layer: keras.layers.Layer\n",
        "    The generated embedding layer\n",
        "  '''\n",
        "\n",
        "#Use pre-trained weights for the embedding (not used)\n",
        "  if pre_trained_weights is None:\n",
        "    layer = layers.Embedding(\n",
        "        input_dim=vocab_size, \n",
        "        output_dim=embedding_size, \n",
        "        input_length=max_length,\n",
        "        mask_zero=True, #Apply mask to padding\n",
        "        trainable=train,\n",
        "        name=layer_name\n",
        "        )\n",
        "  \n",
        "  else:\n",
        "    layer = layers.Embedding(\n",
        "          input_dim=vocab_size, \n",
        "          output_dim=embedding_size, \n",
        "          input_length=max_length,\n",
        "          weights=[pre_trained_weights],\n",
        "          mask_zero=True, #Apply mask to padding\n",
        "          trainable=train,\n",
        "          name=layer_name\n",
        "          )\n",
        "  \n",
        "  return layer"
      ],
      "metadata": {
        "id": "ae_meKiMX5Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence embedding\n",
        "\n",
        "Here all possible methods for the sentence embedding step are implemented. They are: \n",
        "\n",
        "1. \"RNN last\" (implemented inside the \"RNN_Sentence_Embedding\" class)\n",
        "2. \"RNN mean\" (implemented inside the \"RNN_Sentence_Embedding\" class)\n",
        "3. \"Bag of vectors\" (implemented inside the \"Bag_of_vectors_Sentence_Embedding\" class)\n",
        "4. \"MLP\" (implemented inside the MLP_Sentence_Embedding class)"
      ],
      "metadata": {
        "id": "sjCzqXstfH91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The embedding of a sentence is derived through a RNN layer\n",
        "class RNN_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               rnn_size: int,\n",
        "               layer_name: str,\n",
        "               mode: str,\n",
        "               rnn_model: str,\n",
        "               dropout: float,\n",
        "               l2: keras.regularizers.l2):\n",
        "    '''\n",
        "    Parameters:\n",
        "    -----------\n",
        "    rnn_size: int\n",
        "      Number of units for RNN layers\n",
        "    layer_name: str\n",
        "      Name to assign to current layer\n",
        "    mode: str\n",
        "      Mode (one between 'RNN last' and 'RNN mean')\n",
        "    rnn_model: str\n",
        "      RNN model to use inside the Bidirectional layer (one between 'GRU' and 'LSTM')\n",
        "    dropout: float\n",
        "      Dropout rate\n",
        "    l2: keras.regularizers.l2\n",
        "      L2 regularizer\n",
        "    '''\n",
        "\n",
        "    super(RNN_Sentence_Embedding, self).__init__(name=layer_name)\n",
        "\n",
        "    self.mode = mode #One between \"RNN last\" and \"RNN mean\"\n",
        "\n",
        "    # TODO: TEMPORARY - TO FIX!\n",
        "    dropout = None\n",
        "    l2 = None\n",
        "    \n",
        "    dropout_value = dropout\n",
        "    if dropout is None:\n",
        "      dropout_value = 0\n",
        "\n",
        "    #Common RNN parameters\n",
        "    rnn_params= {\"units\":rnn_size,\n",
        "                 \"return_sequences\":True,\n",
        "                 \"return_state\":True,\n",
        "                 \"activation\":\"tanh\",\n",
        "                 \"kernel_regularizer\": l2,\n",
        "                 \"dropout\": dropout_value\n",
        "                 }\n",
        "\n",
        "    #Define a GRU or LSTM layer\n",
        "    if rnn_model==\"GRU\":\n",
        "      layer = layers.GRU(**rnn_params)\n",
        "    elif rnn_model==\"LSTM\":\n",
        "      layer = layers.LSTM(**rnn_params)\n",
        "    else:\n",
        "      raise Exception(\"Invalid RNN model. Use 'GRU' or 'LSTM'\")\n",
        "\n",
        "    #Define a Bidirectional layer containing the RNN model defined above\n",
        "    self.rnn = layers.Bidirectional(layer,merge_mode=\"ave\") #Take the average of correspondent forward and backward hidden states\n",
        "    print(f'\\tAdding bidirectional rnn with {rnn_size} units')\n",
        "\n",
        "    #Define an average layer\n",
        "    if self.mode==\"RNN last\":\n",
        "      self.average_layer = layers.Average()\n",
        "      print(f'\\tAdding average layer')\n",
        "    elif self.mode==\"RNN mean\":\n",
        "      self.average_layer = layers.GlobalAveragePooling1D()\n",
        "      print(f'\\tAdding global average pooling layer')\n",
        "    else:\n",
        "      raise Exception(\"Invalid Mode. Use 'RNN Last' or 'RNN Mean'\")\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    #Overriding: return a null mask after this sentence embedding layer\n",
        "    return None\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "\n",
        "    #Apply the Bidirectional layer and take the outputs\n",
        "    whole_seq_output, forward_h, forward_c, backward_h, backward_c = self.rnn(inputs,mask=mask)\n",
        "\n",
        "    if self.mode==\"RNN last\":\n",
        "      #Take the average between last forward and backward hidden states\n",
        "      return self.average_layer([forward_h, backward_h])\n",
        "    elif self.mode==\"RNN mean\":\n",
        "      #Take the average between all the hidden states\n",
        "      return self.average_layer(whole_seq_output,mask=mask)\n",
        "\n",
        "#The embedding of a sentence is defined as a mean of all the embeddings of its tokens\n",
        "class Bag_Of_Vectors_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, layer_name: str):\n",
        "    '''\n",
        "    Parameters:\n",
        "    ----------\n",
        "    layer_name: str\n",
        "      Name to assign to current layer\n",
        "    '''\n",
        "\n",
        "    super(Bag_Of_Vectors_Sentence_Embedding, self).__init__(name=layer_name)\n",
        "\n",
        "    #Define a layer for computing the mean\n",
        "    self.average_pooling_layer = layers.GlobalAveragePooling1D()\n",
        "    print(f'\\tAdding global average pooling layer')\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    #Overriding: return a null mask after this sentence embedding layer\n",
        "    return None\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    #Compute the mean of all the embedding values\n",
        "    return self.average_pooling_layer(inputs,mask=mask)\n",
        "\n",
        "#The embedding of a sentence is derived through a series of Dense layers\n",
        "class MLP_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, \n",
        "               max_dense_units: int,\n",
        "               n_layers: int, \n",
        "               layer_name: str, \n",
        "               max_tokens: int, \n",
        "               embedding_size: int, \n",
        "               dropout=None,\n",
        "               l2=None):\n",
        "    '''\n",
        "    Parameters:\n",
        "    -----------\n",
        "    max_dense_units: int\n",
        "      Max number of units to use in a Dense layer\n",
        "    n_layers: int\n",
        "      Number of Dense layers\n",
        "    layer_name: str\n",
        "      Name to assign to current layer\n",
        "    max_tokens: int \n",
        "      Max number of tokens\n",
        "    embedding_size: int\n",
        "      Embedding size\n",
        "    dropout: float\n",
        "      Dropout rate\n",
        "    l2: keras.regularizers.l2\n",
        "      L2 regularizer\n",
        "    '''\n",
        "\n",
        "    super(MLP_Sentence_Embedding,self).__init__(name=layer_name)\n",
        "\n",
        "    #A list of layers to be applied\n",
        "    self.layer_list = []\n",
        "    \n",
        "    #Add a reshape layer to reshape input from (batch_size, max_tokens, embedding_size) to (batch_size, max_tokens*embedding_size)\n",
        "    reshape_layer = layers.Reshape((max_tokens*embedding_size,))\n",
        "    self.layer_list.append(reshape_layer)\n",
        "    \n",
        "    #Add a series of Dense layers\n",
        "    for i in range(n_layers):\n",
        "      n_units = self.number_of_units(i, n_layers, max_dense_units, embedding_size)\n",
        "      dense_layer = layers.Dense(\n",
        "          units=n_units,\n",
        "          activation='tanh',\n",
        "          kernel_regularizer=l2\n",
        "          )\n",
        "      self.layer_list.append(dense_layer)\n",
        "      print(f'\\tAdding dense layer with {n_units} units')\n",
        "\n",
        "      #Add dropout layer if requested\n",
        "      if dropout is not None:\n",
        "        dropout_layer = layers.Dropout(dropout)\n",
        "        self.layer_list.append(dropout_layer)\n",
        "        print(f'\\tAdding dropout layer')\n",
        "      \n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    #Overriding: return a null mask after this sentence embedding layer\n",
        "    return None\n",
        "  \n",
        "  def number_of_units(self, \n",
        "                      layer_number: int, \n",
        "                      max_layer_number: int,\n",
        "                      max_dense_units: int,\n",
        "                      embedding_size: int)->int:\n",
        "    '''\n",
        "    Determine the number of units to use inside a Dense layer.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    layer_number: int\n",
        "      Number of the Dense layer in the sequence\n",
        "    max_layer_number: int\n",
        "      Max number of Dense layer in the sequence\n",
        "    max_dense_units: int\n",
        "      Max number of units accepted\n",
        "    embedding_size: int\n",
        "      Embedding size\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    n: int\n",
        "      Number of units to use.\n",
        "    '''\n",
        "\n",
        "    #Last layer must have a number of units equal to the embedding size\n",
        "    if layer_number == max_layer_number - 1:\n",
        "      n = embedding_size\n",
        "    #Each layer has half of the units of the one that preceeds it\n",
        "    else:\n",
        "      n = max_dense_units / 2**layer_number\n",
        "      if(n < embedding_size):\n",
        "        #The number of units cannot be less than the embedding size\n",
        "        n = embedding_size\n",
        "    \n",
        "    return n\n",
        "\n",
        "    return n\n",
        "\n",
        "  def call(self, x, mask=None):\n",
        "\n",
        "    #Apply all the layers defined in the initialization\n",
        "    for layer in self.layer_list:\n",
        "      x = layer(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def sentence_embedding_layer(mode: str, \n",
        "                             rnn_model: str, \n",
        "                             dense_units: int, \n",
        "                             mlp_layers: int, \n",
        "                             layer_name: str, \n",
        "                             max_tokens: int, \n",
        "                             embedding_size: int, \n",
        "                             dropout: float, \n",
        "                             l2: keras.regularizers.l2) ->keras.layers.Layer:\n",
        "  '''\n",
        "  Build the sentence embedding layer.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  mode: str\n",
        "    Sentence embedding mode\n",
        "  rnn_model: str\n",
        "    RNN model to use in 'RNN last' or 'RNN mean' modes\n",
        "  dense_units: int\n",
        "    Max number of units to adopt in 'MLP' sentence embedding mode\n",
        "  mlp_layers: int\n",
        "    Number of layers to adopt in 'MLP' sentence embedding mode\n",
        "  layer_name: str\n",
        "    Name to assign to the current layer\n",
        "  max_tokens: int\n",
        "    Max number of tokens\n",
        "  embedding_size: int\n",
        "    Embedding size\n",
        "  dropout: float\n",
        "    Dropout rate\n",
        "  l2: keras.regularizers.l2\n",
        "    L2 regularizer\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  layer: keras.layers.Layer\n",
        "    The sentence embedding layer.\n",
        "  ''' \n",
        "\n",
        "  print('Sentence Embedding layers creation started')\n",
        "\n",
        "  if mode==\"RNN last\" or mode==\"RNN mean\":\n",
        "    layer = RNN_Sentence_Embedding(embedding_size,layer_name,mode,rnn_model,dropout,l2)\n",
        "  elif mode==\"Bag of vectors\":\n",
        "    layer = Bag_Of_Vectors_Sentence_Embedding(layer_name)\n",
        "  elif mode==\"MLP\":\n",
        "    layer = MLP_Sentence_Embedding(dense_units, mlp_layers, layer_name, max_tokens, embedding_size, dropout, l2)\n",
        "  else:\n",
        "    raise Exception(\"Invalid Mode.\")\n",
        "  \n",
        "  print('Sentence Embedding layers creation completed')\n",
        "\n",
        "  return layer"
      ],
      "metadata": {
        "id": "UYD2ZsiPfJn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merge \n",
        "\n",
        "This section defines all the possible layers to handle the \"merge\" step of the model, where the embedded sentences of the claims and the evidences must be merged in one of the possible way:\n",
        "\n",
        "1. Using concatenation;\n",
        "2. Summing their values;\n",
        "3. Taking a mean of their values."
      ],
      "metadata": {
        "id": "_lOhQ2Jd1G_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate(layer_name: str)->keras.layers.Layer:\n",
        "\n",
        "  '''\n",
        "  Return a layer for inputs concatenation.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  layer_name:str\n",
        "    Name to assign to current layer\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  layer: keras.layers.Layer\n",
        "    Concatenation layer\n",
        "  '''\n",
        "\n",
        "  return layers.Concatenate(name=layer_name)\n",
        "\n",
        "def sum(layer_name: str)->keras.layers.Layer:\n",
        "  '''\n",
        "  Return a layer for summing the inputs.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  layer_name:str\n",
        "    Name to assign to current layer\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  layer: keras.layers.Layer\n",
        "    Add layer\n",
        "  '''\n",
        "\n",
        "  return layers.Add(name=layer_name)\n",
        "\n",
        "def mean(layer_name: str)->keras.layers.Layer:\n",
        "  '''\n",
        "  Return a layer for averaging the inputs.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  layer_name:str\n",
        "    Name to assign to current layer\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  layer: keras.layers.Layer\n",
        "    Average layer\n",
        "  '''\n",
        "\n",
        "  return layers.Average(name=layer_name)\n",
        "\n",
        "def merge_layer(merge_mode: str, layer_name:str)->keras.layers.Layer:\n",
        "  '''\n",
        "  Return a layer for merging inputs.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  merge_mode: str\n",
        "    A merge mode between 'Concatenate', 'Sum' or 'Mean'\n",
        "  layer_name: str\n",
        "    Name to assign to current layer\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  layer: keras.layers.Layer\n",
        "    Merge layer\n",
        "  '''\n",
        "\n",
        "  if merge_mode==\"Concatenate\":\n",
        "    return concatenate(layer_name)\n",
        "  elif merge_mode==\"Sum\":\n",
        "    return sum(layer_name)\n",
        "  elif merge_mode==\"Mean\":\n",
        "    return mean(layer_name)\n",
        "  else:\n",
        "    raise Exception(\"Invalid merge mode.\")"
      ],
      "metadata": {
        "id": "wY3cQUZW1KlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification\n",
        "\n",
        "Defining basic layer (Dense) for the classification step."
      ],
      "metadata": {
        "id": "g3zTpOcK4Qdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_classification_layer(dense_units: int,\n",
        "                               activation_function: str,\n",
        "                               layer_name: str,\n",
        "                               last: bool,\n",
        "                               l2=None) ->keras.layers.Layer:\n",
        "  '''\n",
        "  Return a Dense layer for the classification task.\n",
        "\n",
        "  Parameters:\n",
        "  ------------\n",
        "  dense_units: int\n",
        "    Number of units to use in Dense layers\n",
        "  activation_function: str\n",
        "    Activation function to adopt\n",
        "  layer_name: str\n",
        "    Name to assign to current layer\n",
        "  last: bool\n",
        "    Whether the current layer is the last one or not\n",
        "  l2: keras.regularizers.l2\n",
        "    L2 regularizer\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  layer: keras.layers.Layer\n",
        "    Dense classification layer.\n",
        "  '''\n",
        "\n",
        "  #Use sigmoid activation function and a single unit if it is the last Dense layer to allow binary classification\n",
        "  if last:\n",
        "    layer= layers.Dense(1,\n",
        "                        activation=\"sigmoid\",\n",
        "                        name=layer_name)\n",
        "    \n",
        "  else:\n",
        "    layer= layers.Dense(units=dense_units,\n",
        "                        activation=activation_function,\n",
        "                        kernel_regularizer=l2,\n",
        "                        name=layer_name)\n",
        "  \n",
        "  return layer"
      ],
      "metadata": {
        "id": "rJd89dcZ4UNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build model"
      ],
      "metadata": {
        "id": "bHGeioKS5soW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building layers"
      ],
      "metadata": {
        "id": "Bxd8XBImfE8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_layers(vocab_size: int,\n",
        "                 embedding_size: int,\n",
        "                 max_tokens: int,\n",
        "                 sentence_embedding_mode: str,\n",
        "                 dense_units: int,\n",
        "                 mlp_layers: int,\n",
        "                 dense_classification_layers: int,\n",
        "                 rnn_model: str,\n",
        "                 dropout=None,\n",
        "                 l2=None):\n",
        "  \n",
        "  '''\n",
        "  Build and return all the layers used to build the entire model.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  vocab_size: int\n",
        "    Vocabulary size\n",
        "  embedding_size: int\n",
        "    Embedding size\n",
        "  max_tokens: int\n",
        "    Max number of tokens\n",
        "  sentence_embedding_mode: str\n",
        "    Mode used for sentence embedding\n",
        "  dense_units: int\n",
        "    Max number of units to use in Dense layers,\n",
        "    in case of \"MLP\" mode for sentence embedding\n",
        "  mlp_layers: int\n",
        "    Number of Dense layers,\n",
        "    in case of \"MLP\" mode for sentence embedding\n",
        "  dense_classification_layers: int\n",
        "    Number of Dense layers for the classification step\n",
        "  rnn_model: str\n",
        "    RNN model, in case of of RNN sentence embedding mode\n",
        "  dropout: float\n",
        "    Dropout rate\n",
        "  l2: keras.regularizers.l2\n",
        "    L2 regularizer\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  Tuple containing the requested layers of the model.\n",
        "  '''\n",
        "\n",
        "  #Build the token embedding layer\n",
        "  layer_embedded_tokens = token_embedding_layer(vocab_size,\n",
        "                                                embedding_size,\n",
        "                                                max_tokens,\n",
        "                                                \"token_embedding\")\n",
        "  #Build the sentence embedding layer\n",
        "  layer_embedded_sentences = sentence_embedding_layer(sentence_embedding_mode,\n",
        "                                                      rnn_model,\n",
        "                                                      dense_units,\n",
        "                                                      mlp_layers,\n",
        "                                                      \"sentences_embedding\",\n",
        "                                                      max_tokens,\n",
        "                                                      embedding_size,\n",
        "                                                      dropout,\n",
        "                                                      l2)\n",
        "  #Build the merge layer\n",
        "  layer_merge = merge_layer(MERGE_MODE,\"merge\")\n",
        "\n",
        "  #Build classification layers\n",
        "  classification_layers = []\n",
        "  \n",
        "  for i in range(dense_classification_layers):\n",
        "    units = dense_units/2**i #Each layer has half of the units of the one that preceeds it\n",
        "    is_last_layer = False\n",
        "    layer_name = \"intermediate_classification_\"+str(i+1)\n",
        "\n",
        "    if i == (dense_classification_layers - 1):\n",
        "      is_last_layer = True\n",
        "      layer_name = \"final_classification\"\n",
        "    \n",
        "    classification_layers.append(\n",
        "        dense_classification_layer(units,\n",
        "                                   \"tanh\",\n",
        "                                   layer_name,\n",
        "                                   is_last_layer,\n",
        "                                   l2)\n",
        "        )\n",
        "    \n",
        "    #Add dropout layers if requested\n",
        "    if dropout is not None and not is_last_layer:\n",
        "      classification_layers.append(layers.Dropout(dropout))\n",
        "  \n",
        "  return (layer_embedded_tokens, layer_embedded_sentences, layer_merge, classification_layers)"
      ],
      "metadata": {
        "id": "FHLWacnIfI-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###End-to-end model\n",
        "\n",
        "In this part all the layers are combined to create the final model."
      ],
      "metadata": {
        "id": "QuBjVYc7fMcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(built_layers,\n",
        "                claims_input,\n",
        "                evidences_input,\n",
        "                dense_classification_layers):\n",
        "\n",
        "  layer_embedded_tokens, layer_embedded_sentences, layer_merge, classification_layers = built_layers\n",
        "\n",
        "  claims_tokens_embedded = layer_embedded_tokens(claims_input)\n",
        "  evidences_tokens_embedded = layer_embedded_tokens(evidences_input)\n",
        "\n",
        "  claims_sentences_embedded = layer_embedded_sentences(claims_tokens_embedded)\n",
        "  evidences_sentences_embedded = layer_embedded_sentences(evidences_tokens_embedded)\n",
        "\n",
        "  classification_input = layer_merge([claims_sentences_embedded,evidences_sentences_embedded])\n",
        "\n",
        "  #Cosine similarity extension step:\n",
        "  if APPLY_COSINE_SIMILARITY:\n",
        "    layer_cosine_similarity = layers.Dot(axes=(1), normalize=True,name=\"cosine_similarity\") #Normalize=True will compute the cosine similarity (see documentation)\n",
        "    layer_concatenation = layers.Concatenate(name=\"cosine_similarity_concat\")\n",
        "    cosine_similarity = layer_cosine_similarity([claims_sentences_embedded,evidences_sentences_embedded])\n",
        "    classification_output = layer_concatenation([classification_input,cosine_similarity])\n",
        "  \n",
        "  \n",
        "\n",
        "  for layer in classification_layers:\n",
        "    classification_output = layer(classification_output)\n",
        "\n",
        "  # Instantiate an end-to-end model\n",
        "  model = keras.Model(\n",
        "      inputs=[claims_input, evidences_input],\n",
        "      outputs=[classification_output]\n",
        "  )\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "BEoLxSGbfTJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Build and compile model\n",
        "\n",
        "The following cell makes use of the previous methods to build and compile a full model, based on the variables set in the \"Constants and utilities\" section."
      ],
      "metadata": {
        "id": "C4wjuecBfWo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_and_compile(vocab_size: int,\n",
        "                      embedding_size: int,\n",
        "                      max_tokens: int,\n",
        "                      sentence_embedding_mode: str,\n",
        "                      dense_units: int,\n",
        "                      mlp_layers: int,\n",
        "                      rnn_model: str,\n",
        "                      claims_input: keras.layers.Layer,\n",
        "                      evidences_input: keras.layers.Layer,\n",
        "                      dense_classification_layers: int,\n",
        "                      optimizer: keras.optimizers,\n",
        "                      loss: tf.keras.losses,\n",
        "                      metrics: list,\n",
        "                      dropout=None,\n",
        "                      l2=None) ->keras.Model:\n",
        "\n",
        "  '''\n",
        "  Build and compile a new model.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  vocab_size: int\n",
        "    Vocabulary size\n",
        "  embedding_size: int\n",
        "    Embedding size\n",
        "  max_tokens: int\n",
        "    Max number of tokens\n",
        "  sentence_embedding_mode: str\n",
        "    Mode used for sentence embedding\n",
        "  dense_units: int\n",
        "    Max number of units to use in Dense layers,\n",
        "    in case of \"MLP\" mode for sentence embedding\n",
        "  mlp_layers: int\n",
        "    Number of Dense layers,\n",
        "    in case of \"MLP\" mode for sentence embedding\n",
        "  rnn_model: str\n",
        "    RNN model, in case of of RNN sentence embedding mode\n",
        "  claims_input: keras.layers.Layer\n",
        "    Claims input layer\n",
        "  evidences_input: keras.layers.Layer\n",
        "    Evidences input layer\n",
        "  dense_classification_layers: int\n",
        "    Number of Dense layers for the classification step\n",
        "  optimizer: keras.optimizers\n",
        "    Optimizer of the model\n",
        "  loss: tf.keras.losses\n",
        "    Loss function to train the model\n",
        "  metrics: List\n",
        "    List of metrics to track model's progression\n",
        "  dropout: float\n",
        "    Dropout rate\n",
        "  l2: keras.regularizers.l2\n",
        "    L2 regularizer\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  model: keras.Model\n",
        "    The built and compiled model.\n",
        "  '''\n",
        "\n",
        "  #Build the required layers\n",
        "  built_layers = build_layers(vocab_size,\n",
        "                              embedding_size,\n",
        "                              max_tokens,\n",
        "                              sentence_embedding_mode,\n",
        "                              dense_units,\n",
        "                              mlp_layers,\n",
        "                              dense_classification_layers,\n",
        "                              rnn_model,\n",
        "                              dropout,\n",
        "                              l2)\n",
        "  \n",
        "  #Combine the layers into a full model\n",
        "  model = build_model(built_layers,\n",
        "                      claims_input,\n",
        "                      evidences_input,\n",
        "                      dense_classification_layers)\n",
        "\n",
        "  model_compile_info = {\n",
        "      'optimizer': optimizer,\n",
        "      'loss': loss,\n",
        "      'metrics': metrics,\n",
        "  }\n",
        "\n",
        "  #Compile the model\n",
        "  model.compile(**model_compile_info)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "CAuCaRkP5viv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build and compile the model\n",
        "model = build_and_compile(vocabulary_size,\n",
        "                          EMBEDDING_SIZE,\n",
        "                          max_length,\n",
        "                          SENTENCE_EMBEDDING_MODE,\n",
        "                          DENSE_UNITS,\n",
        "                          MLP_LAYERS,\n",
        "                          RNN_MODEL,\n",
        "                          claims_input,\n",
        "                          evidences_input,\n",
        "                          DENSE_CLASSIFICATION_LAYERS,\n",
        "                          OPTIMIZER,\n",
        "                          LOSS,\n",
        "                          METRICS,\n",
        "                          DROPOUT_RATE,#None, DROPOUT_RATE\n",
        "                          keras.regularizers.l2(L2_RATE))#None, keras.regularizers.l2(L2_RATE)\n",
        "\n",
        "#Show architecture\n",
        "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True, expand_nested=True)"
      ],
      "metadata": {
        "id": "bvOaYxwyp5Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train model\n",
        "\n",
        "All the methods revolving around the training of the model are implemented in this section."
      ],
      "metadata": {
        "id": "vmCzR930eFVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_history(history: keras.callbacks.History):\n",
        "    \"\"\"\n",
        "    Shows training history data stored by the History Keras callback\n",
        "\n",
        "    :param history: History Keras callback\n",
        "    \"\"\"\n",
        "    print(history.history)\n",
        "    history_data = history.history\n",
        "    print(\"Displaying the following history keys: \", history_data.keys())\n",
        "\n",
        "    for key, value in history_data.items():\n",
        "        if not key.startswith('val'):\n",
        "            fig, ax = plt.subplots(1, 1)\n",
        "            ax.set_title(key)\n",
        "            ax.plot(value)\n",
        "            if 'val_{}'.format(key) in history_data:\n",
        "                ax.plot(history_data['val_{}'.format(key)])\n",
        "            else:\n",
        "                print(\"Couldn't find validation values for metric: \", key)\n",
        "\n",
        "            ax.set_ylabel(key)\n",
        "            ax.set_xlabel('epoch')\n",
        "            ax.legend(['train', 'val'], loc='best')\n",
        "\n",
        "    val_accuracies = history_data['val_binary_accuracy']\n",
        "    best_val_epoch = np.argmax(val_accuracies)\n",
        "    best_val_acc = val_accuracies[best_val_epoch]\n",
        "\n",
        "    print(f'Best validation accuracy: {best_val_acc} obtained at epoch: {best_val_epoch}')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_model(model: keras.Model,\n",
        "                claims_train: np.ndarray,\n",
        "                evidences_train: np.ndarray,\n",
        "                labels_train: np.ndarray,\n",
        "                claims_val: np.ndarray,\n",
        "                evidences_val: np.ndarray,\n",
        "                labels_val: np.ndarray,\n",
        "                training_info: dict) -> keras.Model:\n",
        "\n",
        "    '''\n",
        "    Train the model and plot valuable information.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model: keras.Model\n",
        "      Model to be trained\n",
        "    claims_train: np.ndarray\n",
        "      Training set of the claims\n",
        "    evidences_train: np.ndarray\n",
        "      Training set of the evidences\n",
        "    labels_train: np.ndarray\n",
        "      Training set of the labels\n",
        "    claims_val: np.ndarray\n",
        "      Validation set of the claims\n",
        "    evidences_val: np.ndarray\n",
        "      Validation set of the evidences\n",
        "    labels_val: np.ndarray\n",
        "      Validation set of the labels\n",
        "    training_info: dict\n",
        "      Training data\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model: keras.Model\n",
        "      Trained model\n",
        "    '''\n",
        "\n",
        "    print(\"Start training! \\nParameters: {}\".format(training_info))\n",
        "\n",
        "    history = model.fit(x={\"claims\":claims_train,\"evidences\":evidences_train},\n",
        "                        y=labels_train,\n",
        "                        validation_data=({\"claims\":claims_val,\"evidences\":evidences_val},labels_val),\n",
        "                        **training_info)\n",
        "    \n",
        "    print(\"Training completed! Showing history...\")\n",
        "\n",
        "    show_history(history)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "uk7WucTbeG8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute training on the generated model\n",
        "model = train_model(model,\n",
        "            train_claims_tokenized,\n",
        "            train_evidences_tokenized,\n",
        "            train_labels_tokenized,\n",
        "            val_claims_tokenized,\n",
        "            val_evidences_tokenized,\n",
        "            val_labels_tokenized,\n",
        "            training_info)"
      ],
      "metadata": {
        "id": "lOB8mLxtkF-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test model"
      ],
      "metadata": {
        "id": "yRycTGdMgErq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def consecutive_claims(tokenized_claims_list: np.ndarray)->list:\n",
        "\n",
        "  '''\n",
        "  Generate a list containing the numbers of consecutive element of the list\n",
        "  given in input that are equal.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  tokenized_claims_list: np.ndarray\n",
        "    List of claims\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  consecutive_claims_list: list\n",
        "    Number of consecutive equal claims\n",
        "\n",
        "  Example:\n",
        "  -------\n",
        "  Input:\n",
        "    [\"Claim 1\", \"Claim 1\", \"Claim 2\", \"Claim 2\", \"Claim 2\", \"Claim 3]\n",
        "  Output:\n",
        "    [2,3,1]\n",
        "  '''\n",
        "\n",
        "  n_claims = tokenized_claims_list.shape[0]\n",
        "\n",
        "  consecutive_claims_list = []\n",
        "\n",
        "  current_claim = tokenized_claims_list[0]\n",
        "  current_sum = 1\n",
        "\n",
        "  for i in range(1,n_claims):\n",
        "    if np.array_equal(current_claim, tokenized_claims_list[i]):\n",
        "      current_sum+=1\n",
        "    else:\n",
        "      consecutive_claims_list.append(current_sum)\n",
        "\n",
        "      current_claim = tokenized_claims_list[i]\n",
        "      current_sum=1\n",
        "  consecutive_claims_list.append(current_sum)\n",
        "\n",
        "  return consecutive_claims_list\n",
        "\n",
        "def majority_vote(votes:np.ndarray)->int:\n",
        "  '''\n",
        "  Return a majority vote given a list of votes.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  votes: np.ndarray\n",
        "    List of binary votes\n",
        "  \n",
        "  Returns:\n",
        "  -------\n",
        "  value: int\n",
        "    Majority binary vote\n",
        "  '''\n",
        "\n",
        "  zeros = (votes==0).sum()\n",
        "  ones = (votes==1).sum()\n",
        "\n",
        "  return 0 if zeros>=ones else 1\n",
        "\n",
        "def check_labels_consistency(labels: np.ndarray)->bool:\n",
        "  '''\n",
        "  Check if a set of labels is consistent (all equal)\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  labels: np.ndarray\n",
        "    List of binary labels\n",
        "  \n",
        "  Returns:\n",
        "  -------\n",
        "  bool\n",
        "    Consistency\n",
        "  '''\n",
        "\n",
        "  return np.max(labels) == np.min(labels)\n",
        "\n",
        "def claim_verification_selection(claims_test: np.ndarray,\n",
        "                                 labels_test: np.ndarray,\n",
        "                                 test_predictions: np.ndarray):\n",
        "\n",
        "  '''\n",
        "  Select predictions by applying the majority vote to those ones sharing\n",
        "  the same initial claim.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  claims_test: np.ndarray\n",
        "    Test set of the claims\n",
        "  labels_test: np.ndarray\n",
        "    Test set of the labels\n",
        "  test_predictions: np.ndarray\n",
        "    Predicted labels\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  reduced_predictions: np.ndarray\n",
        "    Selected predictions using majority vote\n",
        "  reduced_labels: np.ndarray\n",
        "    Selected true labels\n",
        "  '''\n",
        "\n",
        "  consecutive_claims_list = consecutive_claims(claims_test)\n",
        "\n",
        "  reduced_predictions = np.zeros(len(consecutive_claims_list))\n",
        "  reduced_labels = np.zeros(len(consecutive_claims_list))\n",
        "\n",
        "  start=0\n",
        "  for index, n_consecutives in enumerate(consecutive_claims_list):\n",
        "\n",
        "    labels = labels_test[start:start+n_consecutives]\n",
        "    if check_labels_consistency(labels):\n",
        "      reduced_labels[index] = labels[0]\n",
        "    else:\n",
        "      print(\"Inconsistency found!\")\n",
        "      for j in range(start,start+n_consecutives):\n",
        "        print(\"Claim {}: {}\".format(j-start+1,test_text_claim[j]))\n",
        "        print(\"Evidence {}: {}\".format(j-start+1,test_text_evidence[j]))\n",
        "        print(\"Label {}: {}\".format(j-start+1,test_labels[j]))\n",
        "        print(\"\\n\")\n",
        "      \n",
        "      start+=n_consecutives\n",
        "      continue\n",
        "\n",
        "    votes = test_predictions[start:start+n_consecutives]\n",
        "    vote = majority_vote(votes)\n",
        "    reduced_predictions[index]=vote\n",
        "\n",
        "    start+=n_consecutives\n",
        "\n",
        "  return reduced_predictions, reduced_labels\n",
        "\n",
        "#Threshold is higher than 0.5 to tackle class imbalance in the training set,\n",
        "#where 1 values are three times more common than 0 values.\n",
        "def round_to_label(prediction: float,\n",
        "                   threshold=0.65)->int:\n",
        "  '''\n",
        "  Round a prediction from [0,1] to {0,1}.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  prediction: float\n",
        "    Predicted value\n",
        "  threshold: float\n",
        "    Rounding threshold\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  int\n",
        "    Predicted label (0 or 1)    \n",
        "  '''\n",
        "\n",
        "  if prediction<threshold:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "def predict_data(model: keras.Model,\n",
        "                 claims_test: np.ndarray,\n",
        "                 evidences_test: np.ndarray,\n",
        "                 prediction_info: dict) -> np.ndarray:\n",
        "\n",
        "    '''\n",
        "    Predict labels on the test set.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model: keras.Model\n",
        "      Trained model\n",
        "    claims_test: np.ndarray\n",
        "      Test set of the claims\n",
        "    evidences_test: np.ndarray\n",
        "      Test set of the evidences\n",
        "    prediction_info: dict\n",
        "      Predictiond data info\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    predictions: np.ndarray\n",
        "      Predicted values\n",
        "    '''\n",
        "\n",
        "    print('Starting prediction: \\n{}'.format(prediction_info))\n",
        "\n",
        "    predictions = model.predict(x={\"claims\":claims_test,\"evidences\":evidences_test}, **prediction_info)\n",
        "    return predictions\n",
        "\n",
        "def evaluate_predictions(predictions: np.ndarray,\n",
        "                         y: np.ndarray,\n",
        "                         metrics: List[Callable],\n",
        "                         metric_names: List[str]):\n",
        "  \n",
        "    '''\n",
        "    Evaluate metrics on the predictions.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    predictions: np.ndarray\n",
        "      Predicted values\n",
        "    y: np.ndarray\n",
        "      True labels\n",
        "    metrics: List[Callable]\n",
        "      Metrics to compute\n",
        "    metric_names: List[str]\n",
        "      Names of the metrics to compute\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    Metric info of all computed metrics.\n",
        "    '''\n",
        "    \n",
        "    assert len(metrics) == len(metric_names)\n",
        "    \n",
        "    print(f'Prediction evaluation started...')\n",
        "\n",
        "    metric_info = {}\n",
        "    for metric, metric_name in zip(metrics, metric_names):\n",
        "        \n",
        "        metric_value = metric(y_pred=predictions, y_true=y)\n",
        "        metric_info[metric_name] = metric_value\n",
        "\n",
        "    return metric_info\n",
        "\n",
        "def test_model(model: keras.Model,\n",
        "               claims_test: np.array,\n",
        "               evidences_test: np.array,\n",
        "               labels_test: np.array,\n",
        "               prediction_info: dict,\n",
        "               claim_verification_evaluation: bool):\n",
        "  \n",
        "    '''\n",
        "    Test the model and compute the metrics.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    model: keras.Model\n",
        "      Trained model\n",
        "    claims_test: np.array\n",
        "      Test set of the claims\n",
        "    evidences_test: np.array\n",
        "      Test set of the evidences\n",
        "    labels_test: np.array\n",
        "      Test set of the labels\n",
        "    prediction_info: dict\n",
        "      Predictiond data info\n",
        "    claim_verification_evaluation: bool\n",
        "      Whether to use the evaluation with majority vote or not\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Metric info of all computed metrics;\n",
        "    Predicted labels.\n",
        "    '''\n",
        "        \n",
        "    predictions = predict_data(model,\n",
        "                               claims_test,\n",
        "                               evidences_test,\n",
        "                               prediction_info)\n",
        "\n",
        "    #Reshape predictions from (Batch,1) to (Batch)\n",
        "    predictions = np.reshape(predictions,(predictions.size))\n",
        "\n",
        "    #Round each probability into a label (0 or 1)\n",
        "    test_predictions = np.array(list(map(round_to_label,predictions)))\n",
        "\n",
        "    # Evaluation\n",
        "    metrics = [\n",
        "        partial(f1_score, average='macro'),\n",
        "        partial(f1_score, average='micro'),\n",
        "        partial(f1_score, average='weighted'),\n",
        "        partial(accuracy_score)\n",
        "    ]\n",
        "\n",
        "    metric_names = [\n",
        "        \"macro_f1\",\n",
        "        \"micro_f1\",\n",
        "        \"weighted_f1\",\n",
        "        \"accuracy\"\n",
        "    ]\n",
        "\n",
        "    #Use majority vote system\n",
        "    if claim_verification_evaluation:\n",
        "      reduced_predictions, reduced_labels = claim_verification_selection(claims_test, labels_test,test_predictions)\n",
        "      metric_info = evaluate_predictions(predictions=reduced_predictions,\n",
        "                                        y=reduced_labels,\n",
        "                                        metrics=metrics,\n",
        "                                        metric_names=metric_names)\n",
        "\n",
        "    else:\n",
        "      metric_info = evaluate_predictions(predictions=test_predictions,\n",
        "                                        y=labels_test,\n",
        "                                        metrics=metrics,\n",
        "                                        metric_names=metric_names)\n",
        "\n",
        "    return metric_info, test_predictions"
      ],
      "metadata": {
        "id": "Shu8BCOMgHJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute predictions and metrics info\n",
        "metric_info, y_pred = test_model(model,\n",
        "                                 test_claims_tokenized,\n",
        "                                 test_evidences_tokenized,\n",
        "                                 test_labels_tokenized,\n",
        "                                 prediction_info,\n",
        "                                 CLAIM_VERIFICATION_EVALUATION)"
      ],
      "metadata": {
        "id": "qOeTt-f0r6Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate model\n",
        "\n",
        "This part contains methods to evaluate and plot model's performances."
      ],
      "metadata": {
        "id": "UbI0uX-VvSW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_f1_scores(metric_info):\n",
        "    \"\"\"\n",
        "    Method for creating a list of labels that will be used for testing.\n",
        "    \n",
        "    Parameters\n",
        "    -------\n",
        "    metric_info : dict\n",
        "        Dictionary that contains the f1 scores\n",
        "    \n",
        "    \"\"\"\n",
        "    print()\n",
        "    print('F1 SCORES:')\n",
        "    print(f'  macro: {metric_info[\"macro_f1\"]}')\n",
        "    print(f'  micro: {metric_info[\"micro_f1\"]}')\n",
        "    print(f'  weighted: {metric_info[\"weighted_f1\"]}')\n",
        "    print()\n",
        "\n",
        "def show_accuracy_score(metric_info):\n",
        "  '''\n",
        "  Method to show the accuracy score of the predicted labels.\n",
        "\n",
        "  Parameters\n",
        "  -------\n",
        "  metric_info : dict\n",
        "      Dictionary that contains the accuracy_score\n",
        "  '''\n",
        "\n",
        "  print(\"Accuracy score:\")\n",
        "  print(metric_info[\"accuracy\"])\n",
        "  print()\n",
        "\n",
        "def show_classification_report(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Method that prints the classification report\n",
        "    \n",
        "    Parameters\n",
        "    -------\n",
        "    y_true : np.array\n",
        "        Array of true labels\n",
        "    y_pred : np.array\n",
        "        Array of predicted labels\n",
        "    \"\"\"\n",
        "\n",
        "    print(classification_report(\n",
        "        y_true, \n",
        "        y_pred,\n",
        "        ))\n",
        "\n",
        "def show_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Method that shows the confusion matrix.\n",
        "    \n",
        "    Parameters\n",
        "    -------\n",
        "    y_true : np.array\n",
        "        Array of true labels\n",
        "    y_pred : np.array\n",
        "        Array of predicted labels\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(5,5))\n",
        "    ConfusionMatrixDisplay.from_predictions(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        normalize='true', \n",
        "        cmap=plt.cm.Blues, \n",
        "        values_format=\".2f\",\n",
        "        xticks_rotation='vertical',\n",
        "        ax=ax)"
      ],
      "metadata": {
        "id": "EudZ04CsvUzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show scores\n",
        "show_f1_scores(metric_info)\n",
        "show_classification_report(test_labels_tokenized, y_pred)\n",
        "show_accuracy_score(metric_info)\n",
        "show_confusion_matrix(test_labels_tokenized, y_pred)"
      ],
      "metadata": {
        "id": "s_hbKdb_v31i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion and Error Analysis\n",
        "In this part there is a small summary of the tests we have done, and the explanation of some choices that we made.\n",
        "\n",
        "Finally, there are some considerations about the models that we selected for the final testing, and a comparison with the results obtained on the validation set.\n",
        "\n",
        "* how we pre-processed the data and why\n",
        "* observe that the train set is unbalanced and contains some inconsistencies\n",
        "* model creation and parameter tuning\n",
        "* results discussion\n",
        "* possible improvements\n"
      ],
      "metadata": {
        "id": "Q5zHCzQXBklE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing\n",
        "We noticed that the original datasets were relatively dirty, and they required some cleaning before using them as inputs of the models.\n",
        "\n",
        "**Firstly**, we made some standard operations, e.g., removed unwanted punctuation, lower cased everything, removed special characters, removed some not useful POS tags, etc....\n",
        "\n",
        "**Secondly**, we applied specific actions for datasets cluttering removal. In particular:\n",
        "* we noticed that each **evidence** was starting with a number (most likely the ID of the evidence) and a tabulation character. We removed all of these occurences in all the dataset evidences. Example:\n",
        "\n",
        " **Original Sentence**\n",
        "~~~\n",
        "14\tThe series finale aired August 28 , 2013 .\n",
        "~~~\n",
        " **Processed Sentence**\n",
        "~~~\n",
        "The series finale aired August 28 , 2013 .\n",
        "~~~\n",
        "* almost all the evidences terminate with a series of tags that are not really relevant for the classification task, and we decided to remove them. Example:\n",
        " **Original Sentence**\n",
        "~~~\n",
        "5\tIt stars Ray Winstone in his first role in a costume drama .\tRay Winstone\tRay Winstone\n",
        "~~~\n",
        " **Processed Sentence**\n",
        "~~~\n",
        "It stars Ray Winstone in his first role in a costume drama .\n",
        "~~~\n",
        "\n",
        "At the beginning, we also removed **stopwords** from the dataset.\n",
        "After a lot of testing, we discovered that this was causing 2 major problems:\n",
        "* **inconsistences**: after the pre-processing, some claims had the **same description**, but **different labels**. \n",
        "For example, in the original dataset we have the following claims:\n",
        "~~~\n",
        "SZA's music combines elements from different genres. (SUPPORT)\n",
        "SZA's music does not combine elements from different genres. (REFUTES)\n",
        "~~~\n",
        "After the pre-processing, which was also consisting in removing stopwords, the results were the same, but with different labels:\n",
        "~~~\n",
        "szas music combine element different genre (SUPPORT)\n",
        "szas music combine element different genre (REFUTES)\n",
        "~~~\n",
        "* **duplicates**: for the same reasons explained above, some rows were duplicated (e.g., same claim and same evidence).\n",
        "\n",
        "Therefore, we eventually decided to discard the stopwords removal, and we remarked an important difference in the validation accuracy during the training phase (~+5-10%).\n"
      ],
      "metadata": {
        "id": "p13f93lhCfYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9BkWxVYyFXoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1LWnzR6u13"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from functools import reduce\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize,WhitespaceTokenizer\n",
        "\n",
        "\n",
        "# typing\n",
        "from typing import List, Callable, Dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xBYfRb6cjF"
      },
      "source": [
        "# Data pipeline\n",
        "The goal of this section is to convert initial textual input into a numerical format that is compatible with our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQIk9xwZ8jd"
      },
      "source": [
        "## Data Loading\n",
        "Download the dataset and save it to file system.\n",
        "The dataset is composed by 3 csv files: train, validation and test. These 3 csv files will be loaded directly into 3 different dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdVaaLhS6bNc"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "download_data('dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZeR75ZaabyS"
      },
      "source": [
        "Load the files into different data frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxE3Zp2I6y80"
      },
      "source": [
        "# Be sure all columns have a name\n",
        "column_names = ['Row', 'Claim', 'Evidence', 'ID', 'Label']\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv ('dataset/train_pairs.csv', names=column_names, header=0)\n",
        "df_val = pd.read_csv ('dataset/val_pairs.csv', names=column_names, header=0)\n",
        "df_test = pd.read_csv ('dataset/test_pairs.csv', names=column_names, header=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lGaexishbQFT",
        "outputId": "ee1d0331-091e-4a15-ee53-ecc5e7ef73f6"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row</th>\n",
              "      <th>Claim</th>\n",
              "      <th>Evidence</th>\n",
              "      <th>ID</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Chris Hemsworth appeared in A Perfect Getaway.</td>\n",
              "      <td>2\\tHemsworth has also appeared in the science ...</td>\n",
              "      <td>3</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Roald Dahl is a writer.</td>\n",
              "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...</td>\n",
              "      <td>7</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Roald Dahl is a governor.</td>\n",
              "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...</td>\n",
              "      <td>8</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Ireland has relatively low-lying mountains.</td>\n",
              "      <td>10\\tThe island 's geography comprises relative...</td>\n",
              "      <td>9</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Ireland does not have relatively low-lying mou...</td>\n",
              "      <td>10\\tThe island 's geography comprises relative...</td>\n",
              "      <td>10</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Row                                              Claim  ...  ID     Label\n",
              "0    0     Chris Hemsworth appeared in A Perfect Getaway.  ...   3  SUPPORTS\n",
              "1    1                            Roald Dahl is a writer.  ...   7  SUPPORTS\n",
              "2    2                          Roald Dahl is a governor.  ...   8   REFUTES\n",
              "3    3        Ireland has relatively low-lying mountains.  ...   9  SUPPORTS\n",
              "4    4  Ireland does not have relatively low-lying mou...  ...  10   REFUTES\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB4obklU6-jP",
        "outputId": "401bdc4b-e7d3-481c-f81a-f8a3d601796a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Special characters to remove: /(){}[]|@,;\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "\n",
        "# Accepted symbols:\n",
        "# - numbers between 0-9\n",
        "# - all lower cased letters\n",
        "# - whitespace, #, + _\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "# - ^ begnning of a string\n",
        "# - \\d any digit\n",
        "# - \\s whitespaces and tabs\n",
        "BEGINNING_IDS_RE = re.compile('^\\d*\\s*')\n",
        "\n",
        "# The stopwords are a list of words that are very very common but don’t \n",
        "# provide useful information for most text analysis procedures.\n",
        "# Therefore, they will be removed from the dataset\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def filter_out_uncommon_symbols(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Method used for removing most common words\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def replace_ids(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Method used for removing ids and some whitespaces that could appear\n",
        "    at the beginning of the text.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    return BEGINNING_IDS_RE.sub('', text)\n",
        "\n",
        "nltk.download('punkt') # necessary for being able to tokenize\n",
        "nltk.download('wordnet') \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "def lemsent(sentence):\n",
        "    \"\"\"\n",
        "    Method used for lemmatize text.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    words = [lemmatizer.lemmatize(word) for word in tokenizer.tokenize(str(sentence))]\n",
        "    return \" \".join(words)\n",
        "\n",
        "GENERIC_PREPROCESSING_PIPELINE = [\n",
        "                                  lower,\n",
        "                                  replace_special_characters,\n",
        "                                  filter_out_uncommon_symbols,\n",
        "                                  remove_stopwords,\n",
        "                                  strip_text,\n",
        "                                  lemsent\n",
        "                                  ]\n",
        "\n",
        "EVIDENCES_PREPROCESSING_PIPELINE = GENERIC_PREPROCESSING_PIPELINE\n",
        "EVIDENCES_PREPROCESSING_PIPELINE.insert(0, replace_ids)\n",
        "    \n",
        "\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "    filter_methods = \\\n",
        "        filter_methods if filter_methods is not None else GENERIC_PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKct7tDs0o3W"
      },
      "source": [
        "Now we are ready to pre-process the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ochx9GctfdkX"
      },
      "source": [
        "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Replace each sentence with its pre-processed version\n",
        "    df['Evidence'] = df['Evidence'].apply(\n",
        "        lambda txt: text_prepare(txt, filter_methods=EVIDENCES_PREPROCESSING_PIPELINE)\n",
        "        )\n",
        "    df['Claim'] = df['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_train = preprocess_dataset(df_train)\n",
        "df_val = preprocess_dataset(df_val)\n",
        "df_test = preprocess_dataset(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "d-SLZC-5nytP",
        "outputId": "e197f652-ca29-4f88-9964-a0ba9c70ae33"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row</th>\n",
              "      <th>Claim</th>\n",
              "      <th>Evidence</th>\n",
              "      <th>ID</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>chris hemsworth appeared perfect getaway</td>\n",
              "      <td>hemsworth also appeared science fiction action...</td>\n",
              "      <td>3</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>roald dahl writer</td>\n",
              "      <td>roald dahl lrb lsb langpronrold _ dl rsb lsb u...</td>\n",
              "      <td>7</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>roald dahl governor</td>\n",
              "      <td>roald dahl lrb lsb langpronrold _ dl rsb lsb u...</td>\n",
              "      <td>8</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>ireland relatively lowlying mountain</td>\n",
              "      <td>island geography comprises relatively lowlying...</td>\n",
              "      <td>9</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>ireland relatively lowlying mountain</td>\n",
              "      <td>island geography comprises relatively lowlying...</td>\n",
              "      <td>10</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Row                                     Claim  ...  ID     Label\n",
              "0    0  chris hemsworth appeared perfect getaway  ...   3  SUPPORTS\n",
              "1    1                         roald dahl writer  ...   7  SUPPORTS\n",
              "2    2                       roald dahl governor  ...   8   REFUTES\n",
              "3    3      ireland relatively lowlying mountain  ...   9  SUPPORTS\n",
              "4    4      ireland relatively lowlying mountain  ...  10   REFUTES\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMfITa8PbVR6"
      },
      "source": [
        "## Tokenization and vocabularies\n",
        "Here each set is passed through a tokenization process, which also allows to define the vocabulary of each set and also their vocabulary size. Furthermore the maximum length of a token sequence is defined and the labels are extracted from the sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STARTING_TOKEN = 1 #First value to start the tokenization on (0 is already used as padding value)\n",
        "\n",
        "def get_tokenizer(corpus: List[str],\n",
        "                  starting_dict=None)->Dict[str,int]:\n",
        "  '''\n",
        "  Create or expand (given an existing dictionary) a tokenization dictionary\n",
        "  that associates an integer to each word.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  corpus: List[str]\n",
        "    Text to examine searching for new words to add into the dictionary.\n",
        "  starting_dict: Dict[str,int]\n",
        "    An already filled dictionary to further expand (optional).\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    1. A filled dictionary that associates an integer to each word (if starting_dict=None);\n",
        "    2. An expanded dictionary that associates an integer to each new word (if starting_dict is not None)\n",
        "  '''\n",
        "\n",
        "  #Copy the original dictionary to keep it save from updates\n",
        "  words_to_tokens = {} if starting_dict==None else starting_dict.copy()\n",
        "\n",
        "  for text in corpus:\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "      if not word in words_to_tokens:\n",
        "        words_to_tokens[word] = len(words_to_tokens)+STARTING_TOKEN\n",
        "\n",
        "  return words_to_tokens\n",
        "\n",
        "def tokenize(word: str,\n",
        "             words_to_tokens: Dict[str,int])->int:\n",
        "  '''\n",
        "  Get the integer value of a given token.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  word: str\n",
        "    Token\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  int:\n",
        "    Value associated to the token\n",
        "  '''\n",
        "\n",
        "  return words_to_tokens[word]\n",
        "\n",
        "def detokenize(token:int,\n",
        "               words_to_tokens: Dict[str,int])->str:\n",
        "  '''\n",
        "  Get the token-word of a given token-value.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  token: int\n",
        "    Tokenized word\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  str:\n",
        "    Word associated to the token-value\n",
        "  '''\n",
        "\n",
        "  val_list = list(words_to_tokens.values())\n",
        "  key_list = list(words_to_tokens.keys())\n",
        "\n",
        "  position = val_list.index(token)\n",
        "\n",
        "  return key_list[position]\n",
        "\n",
        "def tokenize_string(string: str,\n",
        "                    words_to_tokens: Dict[str,int],\n",
        "                    max_length: int)->List[int]:\n",
        "\n",
        "  '''\n",
        "  Get the tokenized sequence of a string of separated tokens (document/sentence).\n",
        "\n",
        "  Parameters:\n",
        "  string: str\n",
        "    String of separated tokens (document or sentence)\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "  max_length: int\n",
        "    Tokenization length\n",
        "\n",
        "  Returns:\n",
        "    List[int]:\n",
        "      A list of token-values where each one is the tokenized value of a token\n",
        "      int the input-string.\n",
        "      The list is padded if its length is below the max_length.\n",
        "      The list is truncated if its length is above the max_length.\n",
        "  '''\n",
        "\n",
        "  tokens = string.split()\n",
        "  tokenized_sequence = [tokenize(token, words_to_tokens)  for token in tokens]\n",
        "  length_diff = max_length-len(tokenized_sequence)\n",
        "\n",
        "  if length_diff==0: # Return the same sequence if it has the requested size\n",
        "    return tokenized_sequence\n",
        "  elif length_diff<0: # Return the truncated sequence if it exceeds the requested size\n",
        "    return tokenized_sequence[0:max_length]\n",
        "  else: # Return the padded sequence if it has an inferior size than the expected one\n",
        "    return np.pad(tokenized_sequence, (0, length_diff), 'constant').tolist()\n",
        "\n",
        "#Define corpus\n",
        "train_text_claim = df_train[\"Claim\"].tolist()\n",
        "train_text_evidence = df_train[\"Evidence\"].tolist()\n",
        "val_text_claim = df_val[\"Claim\"].tolist()\n",
        "val_text_evidence = df_val[\"Evidence\"].tolist()\n",
        "test_text_claim = df_test[\"Claim\"].tolist()\n",
        "test_text_evidence = df_test[\"Evidence\"].tolist()\n",
        "print(train_text_claim[0])\n",
        "print(train_text_evidence[0])\n",
        "#Define labels\n",
        "train_labels = df_train[\"Label\"].tolist()\n",
        "val_labels = df_val[\"Label\"].tolist()\n",
        "test_labels = df_test[\"Label\"].tolist()\n",
        "\n",
        "#Token dictionary\n",
        "corpus = train_text_claim+train_text_evidence+val_text_claim+val_text_evidence+test_text_claim+test_text_evidence\n",
        "tokens_dictionary = get_tokenizer(corpus)\n",
        "#train_tokens_claim = get_tokenizer(train_text_claim)\n",
        "#train_tokens_evidence = get_tokenizer(train_text_evidence, starting_dict = train_tokens_claim)\n",
        "#val_tokens_claim = get_tokenizer(val_text_claim, starting_dict = train_tokens_evidence)\n",
        "#val_tokens_evidence = get_tokenizer(val_text_evidence, starting_dict = val_tokens_claim)\n",
        "#test_tokens_claim = get_tokenizer(test_text_claim, starting_dict = val_tokens_evidence)\n",
        "#test_tokens_evidence = get_tokenizer(test_text_evidence, starting_dict = val_tokens_claim)\n",
        "\n",
        "#Vocabulary\n",
        "tokens_vocabulary = tokens_dictionary.keys()\n",
        "#train_claim_vocab = train_tokens_claim.keys()\n",
        "#train_evidence_vocab = train_tokens_evidence.keys()\n",
        "#val_claim_vocab = val_tokens_claim.keys()\n",
        "#val_evidence_vocab = val_tokens_evidence.keys()\n",
        "#test_claim_vocab = test_tokens_claim.keys()\n",
        "#test_evidence_vocab = test_tokens_evidence.keys()\n",
        "\n",
        "#Vocab size\n",
        "vocabulary_size = len(tokens_vocabulary)\n",
        "#train_claim_vocab_size = len(train_claim_vocab)\n",
        "#train_evidence_vocab_size = len(train_evidence_vocab)\n",
        "#val_claim_vocab_size = len(val_claim_vocab)\n",
        "#val_evidence_vocab_size = len(val_evidence_vocab)\n",
        "#test_claim_vocab_size = len(test_claim_vocab)\n",
        "#test_evidence_vocab_size = len(test_evidence_vocab)\n",
        "\n",
        "#Max length of a token sequence\n",
        "n_tokens = [len(doc.split()) for doc in corpus]\n",
        "max_length = int(np.quantile(n_tokens,0.99))\n",
        "print(max_length)\n",
        "\n",
        "#Tokenized sets\n",
        "train_claim_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),train_text_claim)))\n",
        "train_evidence_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),train_text_evidence)))\n",
        "val_claim_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),val_text_claim)))\n",
        "val_evidence_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),val_text_evidence)))\n",
        "test_claim_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),test_text_claim)))\n",
        "test_evidence_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),test_text_evidence)))\n",
        "print(train_claim_tokenized[0])"
      ],
      "metadata": {
        "id": "vT8NNqzXCMkt",
        "outputId": "1fc61684-77b6-4c35-a0e2-b8a7ed10d4bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chris hemsworth appeared perfect getaway\n",
            "hemsworth also appeared science fiction action film star trek lrb 2009 rrb thriller adventure perfect getaway lrb 2009 rrb horror comedy cabin wood lrb 2012 rrb darkfantasy action film snow white huntsman lrb 2012 rrb war film red dawn lrb 2012 rrb biographical sport drama film rush lrb 2013 rrb star trekstar trek film perfect getawaya perfect getawaythe cabin woodsthe cabin woodssnow white huntsmansnow white huntsmanred dawnred dawn 2012 film rushrush 2013 film\n",
            "70\n",
            "[1 2 3 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "kfIWefidXvm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Constants and utilities"
      ],
      "metadata": {
        "id": "-HxWGraEYKAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample values\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_SIZE = 64\n",
        "MAX_TOKENS = 100\n",
        "EPOCHS = 5\n",
        "VOCABULARY_SIZE = 1000\n",
        "\n",
        "RNN_UNITS = 128\n",
        "DENSE_UNITS = 256\n",
        "MLP_LAYERS = 3\n",
        "DENSE_CLASSIFICATION_LAYERS = 3\n",
        "\n",
        "EMBEDDING_MODE = \"Simple\" #This must be one between \"Simple\", \"GloVe static\" and \"GloVe dynamic\"\n",
        "SENTENCE_EMBEDDING_MODE = \"RNN last\" #This must be one between \"RNN last\", \"RNN mean\", \"Bag of vectors\", \"MLP\"\n",
        "RNN_MODEL = \"GRU\" #This must be one between \"GRU\" and \"LSTM\"\n",
        "MERGE_MODE = \"Concatenate\" #This must be one between \"Concatenate\", \"Sum\" and \"Mean\"\n",
        "\n",
        "model_compile_info = {\n",
        "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    'loss': 'sparse_categorical_crossentropy',\n",
        "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
        "}\n",
        "\n",
        "# Model common training information\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience=10,\n",
        "                                                restore_best_weights=True)]\n",
        "}\n",
        "\n",
        "# Model common prediction information\n",
        "prediction_info = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'verbose': 1\n",
        "}"
      ],
      "metadata": {
        "id": "1w41RoSZYako"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inputs"
      ],
      "metadata": {
        "id": "ESKdNELYZJKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "claims_input = keras.Input(shape=(MAX_TOKENS,EMBEDDING_SIZE), name=\"claims\")\n",
        "evidences_input = keras.Input(shape=(MAX_TOKENS,EMBEDDING_SIZE), name=\"evidences\")"
      ],
      "metadata": {
        "id": "JaU_BEJyZLOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Embedding"
      ],
      "metadata": {
        "id": "CFWnFX0AX0Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_embedding_layer(vocab_size,\n",
        "                    embedding_size,\n",
        "                    max_length,\n",
        "                    layer_name,\n",
        "                    pre_trained_weights=None,\n",
        "                    train=True):\n",
        "  \n",
        "  if pre_trained_weights==None:\n",
        "    layer = layers.Embedding(\n",
        "        input_dim=vocab_size, \n",
        "        output_dim=embedding_size, \n",
        "        input_length=max_length,\n",
        "        mask_zero=True,\n",
        "        trainable=train,\n",
        "        name=layer_name\n",
        "        )\n",
        "  \n",
        "  else:\n",
        "    layer = layers.Embedding(\n",
        "          input_dim=vocab_size, \n",
        "          output_dim=embedding_size, \n",
        "          input_length=max_length,\n",
        "          weights=[pre_trained_weights],\n",
        "          mask_zero=True,\n",
        "          trainable=train,\n",
        "          name=layer_name\n",
        "          )\n",
        "  \n",
        "  return layer\n",
        "\n",
        "#TODO: implement the other embedding approaches"
      ],
      "metadata": {
        "id": "ae_meKiMX5Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence embedding"
      ],
      "metadata": {
        "id": "sjCzqXstfH91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, rnn_size, layer_name, mode, rnn_model):\n",
        "    super(RNN_Last, self).__init__()\n",
        "\n",
        "    self.name = layer_name\n",
        "    self.mode = mode\n",
        "\n",
        "    rnn_params= {units:rnn_size,\n",
        "                 return_sequences:True,\n",
        "                 return_state:True,\n",
        "                 activation:\"tanh\"}\n",
        "\n",
        "    if rnn_model==\"GRU\":\n",
        "      layer = layers.GRU(rnn_params)\n",
        "    elif rnn_model==\"LSTM\":\n",
        "      layer = layers.LSTM(rnn_params)\n",
        "    else:\n",
        "      raise \"Invalid RNN model. Use 'GRU' or 'LSTM'\"\n",
        "\n",
        "    self.rnn = layers.Bidirectional(layer)\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    whole_seq_output, final_memory_state, final_carry_state = self.rnn(inputs)\n",
        "\n",
        "    if mode==\"RNN last\":\n",
        "      return final_memory_state\n",
        "    elif mode==\"RNN mean\":\n",
        "      return tf.reduce_mean(whole_seq_output,axis=0) #axis=0 ->mean on max_tokens dim\n",
        "    else:\n",
        "      raise \"Invalid Mode. Use 'RNN Last' or 'RNN Mean'\"\n",
        "\n",
        "class Bag_Of_Vectors_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, layer_name):\n",
        "    super(Bag_Of_Vectors_Sentence_Embedding, self).__init__()\n",
        "    self.name=layer_name\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.reduce_mean(inputs, axis=0)  #axis=0 ->mean on max_tokens dim\n",
        "\n",
        "class MLP_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, dense_units, intermediate_dense_activation, last_dense_activation, n_layers, layer_name):\n",
        "    super(MLP_Sentence_Embedding,self).__init__()\n",
        "\n",
        "    self.name=layer_name\n",
        "    self.n_layers = n_layers\n",
        "    self.last_dense_activation = last_dense_activation\n",
        "\n",
        "    self.intermediate_dense_layer = layers.Dense(units=dense_units,\n",
        "                                    activation=intermediate_dense_activation)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    inputs_shape = tf.shape(inputs).numpy()\n",
        "    max_tokens = inputs_shape[0]\n",
        "    embedding_size = inputs_shape[1]\n",
        "\n",
        "    x = tf.reshape(inputs,[max_tokens*embedding_size])\n",
        "\n",
        "    for i in range(self.n_layers-1):\n",
        "      x = self.intermediate_dense_layer(x)\n",
        "    \n",
        "    last_dense_layer = layers.Dense(units=embedding_size, activation=self.last_dense_activation)\n",
        "    return last_dense_layer(x)\n",
        "\n",
        "def sentence_embedding_layer(mode, rnn_units, rnn_model, dense_units, mlp_layers, layer_name):\n",
        "  if mode==\"RNN last\" or mode==\"RNN mean\":\n",
        "    layer = RNN_Sentence_Embedding(rnn_units,layer_name,mode,rnn_model)\n",
        "  elif mode==\"Bag of vectors\":\n",
        "    layer = Bag_Of_Vectors_Sentence_Embedding(layer_name)\n",
        "  elif mode==\"MLP\":\n",
        "    layer = MLP_Sentence_Embedding(dense_units,\"relu\",\"tanh\",mlp_layers,layer_name)\n",
        "  else:\n",
        "    raise \"Invalid Mode.\"\n",
        "\n",
        "  return layer"
      ],
      "metadata": {
        "id": "UYD2ZsiPfJn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merge inputs"
      ],
      "metadata": {
        "id": "_lOhQ2Jd1G_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate(layer_name):\n",
        "\n",
        "  return layers.Concatenate(name=layer_name)\n",
        "\n",
        "def sum(layer_name):\n",
        "\n",
        "  return layers.Add(name=layer_name)\n",
        "\n",
        "def mean(layer_name):\n",
        "\n",
        "  return layers.Average(name=layer_name)\n",
        "\n",
        "def merge_layer(merge_mode, layer_name):\n",
        "  if merge_mode==\"Concatenate\":\n",
        "    return concatenate\n",
        "  elif merge_mode==\"Sum\":\n",
        "    return sum(layer_name)\n",
        "  elif merge_mode==\"Mean\":\n",
        "    return mean(layer_name)\n",
        "  else:\n",
        "    raise \"Invalid merge mode.\""
      ],
      "metadata": {
        "id": "wY3cQUZW1KlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification"
      ],
      "metadata": {
        "id": "g3zTpOcK4Qdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_classification_layer(dense_units, activation_function, layer_name, last):\n",
        "\n",
        "  if last:\n",
        "    layer= layers.Dense(units=1,\n",
        "                        activation=\"softmax\",\n",
        "                        name=layer_name)\n",
        "    \n",
        "  else:\n",
        "    layer= layers.Dense(units=dense_units,\n",
        "                        activation=dense_activation,\n",
        "                        name=layer_name)\n",
        "  \n",
        "  return layer\n",
        "\n",
        "#TODO: add other classification architectures"
      ],
      "metadata": {
        "id": "rJd89dcZ4UNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build model"
      ],
      "metadata": {
        "id": "bHGeioKS5soW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building layers\n",
        "layer_embedded_tokens = token_embedding_layer(VOCABULARY_SIZE,\n",
        "                                         EMBEDDING_SIZE,\n",
        "                                         MAX_TOKENS,\n",
        "                                         \"token embedding\")\n",
        "\n",
        "layer_embedded_sentences = sentence_embedding_layer(SENTENCE_EMBEDDING_MODE,\n",
        "                                                           RNN_UNITS,\n",
        "                                                           RNN_MODEL,\n",
        "                                                           DENSE_UNITS,\n",
        "                                                           MLP_LAYERS,\n",
        "                                                           \"sentences embedding\")\n",
        "\n",
        "layer_merge = merge_layer(\"merge\")\n",
        "\n",
        "layer_classification = dense_classification_layer(DENSE_UNITS,\n",
        "                                                  \"relu\",\n",
        "                                                  \"intermidiate classification\",\n",
        "                                                  False)\n",
        "layer_output = dense_classification_layer(None,\n",
        "                                          None,\n",
        "                                          \"output classification\",\n",
        "                                          True)\n",
        "\n",
        "#Building model\n",
        "claims_tokens_embedded = layer_embedded_tokens(claims_input)\n",
        "evidences_tokens_embedded = layer_embedded_tokens(evidences_input)\n",
        "\n",
        "claims_sentences_embedded = layer_embedded_sentences(claims_tokens_embedded)\n",
        "evidences_sentences_embedded = layer_embedded_sentences(evidences_tokens_embedded)\n",
        "\n",
        "classification_input = layer_merge(claims_sentences_embedded,evidences_sentences_embedded)\n",
        "\n",
        "for i in range(DENSE_CLASSIFICATION_LAYERS-1):\n",
        "  classification_input = layer_classification(classification_input)\n",
        "\n",
        "classification_output = layer_output(classification_input)\n",
        "\n",
        "# Instantiate an end-to-end model\n",
        "model = keras.Model(\n",
        "    inputs=[claims_input, evidences_input],\n",
        "    outputs=[classification_output]\n",
        ")"
      ],
      "metadata": {
        "id": "CAuCaRkP5viv",
        "outputId": "8ef9ec34-4161-4616-acdc-3f272860184c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-183-d42174c43f47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                            \u001b[0mDENSE_UNITS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                            \u001b[0mMLP_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                                            \"sentences embedding\")\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlayer_merge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-180-e9f5f1aea0b4>\u001b[0m in \u001b[0;36msentence_embedding_layer\u001b[0;34m(mode, rnn_units, rnn_model, dense_units, mlp_layers, layer_name)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msentence_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"RNN last\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"RNN mean\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_Sentence_Embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"Bag of vectors\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBag_Of_Vectors_Sentence_Embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-180-e9f5f1aea0b4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rnn_size, layer_name, mode, rnn_model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRNN_Sentence_Embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN_Last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RNN_Last' is not defined"
          ]
        }
      ]
    }
  ]
}
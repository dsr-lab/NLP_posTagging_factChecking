{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1LWnzR6u13"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from functools import reduce, partial\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize,WhitespaceTokenizer\n",
        "\n",
        "# typing\n",
        "from typing import List, Callable, Dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9xBYfRb6cjF"
      },
      "source": [
        "# Data pipeline\n",
        "The goal of this section is to convert initial textual input into a numerical format that is compatible with our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQIk9xwZ8jd"
      },
      "source": [
        "## Data Loading\n",
        "Download the dataset and save it to file system.\n",
        "The dataset is composed by 3 csv files: train, validation and test. These 3 csv files will be loaded directly into 3 different dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdVaaLhS6bNc"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "download_data('dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9GLMF0k2aLfO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZeR75ZaabyS"
      },
      "source": [
        "Load the files into different data frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxE3Zp2I6y80"
      },
      "source": [
        "# Be sure all columns have a name\n",
        "column_names = ['Row', 'Claim', 'Evidence', 'ID', 'Label']\n",
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv ('dataset/train_pairs.csv', names=column_names, header=0)\n",
        "df_val = pd.read_csv ('dataset/val_pairs.csv', names=column_names, header=0)\n",
        "df_test = pd.read_csv ('dataset/test_pairs.csv', names=column_names, header=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bTxsfaQsnEfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGaexishbQFT"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMfITa8PbVR6"
      },
      "source": [
        "## Data Pre-processing\n",
        "Perform text cleaning and tokenization operations.\n",
        "Start by creating some utility methods that will be used for cleaning the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSRATkFJeOgk"
      },
      "source": [
        "# Special characters to remove: /(){}[]|@,;\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "\n",
        "# Accepted symbols:\n",
        "# - numbers between 0-9\n",
        "# - all lower cased letters\n",
        "# - whitespace, #, + _\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "# - ^ begnning of a string\n",
        "# - \\d any digit\n",
        "# - \\s whitespaces and tabs\n",
        "BEGINNING_IDS_RE = re.compile('^\\d*\\s*')\n",
        "\n",
        "# Remove multiple whitespaces, tabs and newlines\n",
        "EXTRA_WHITE_SPACE_RE = re.compile('/\\s\\s+/g')\n",
        "\n",
        "TAGS_TO_REMOVE = ['-LCB-', '-RCB-', '-LSB-', '-RSB-', '-RRB', '-LRB-']\n",
        "\n",
        "\n",
        "# The stopwords are a list of words that are very very common but donâ€™t \n",
        "# provide useful information for most text analysis procedures.\n",
        "# Therefore, they will be removed from the dataset\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "nltk.download('punkt') # necessary for being able to tokenize\n",
        "nltk.download('wordnet') \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def filter_out_uncommon_symbols(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Method used for removing most common words\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def replace_ids(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Method used for removing ids and some whitespaces that could appear\n",
        "    at the beginning of the text.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    return BEGINNING_IDS_RE.sub('', text)\n",
        "\n",
        "def lemsent(sentence):\n",
        "    \"\"\"\n",
        "    Method used for lemmatize text.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        The text to process.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        The processed text.\n",
        "    \"\"\"\n",
        "    #words = [lemmatizer.lemmatize(word) for word in tokenizer.tokenize(str(sentence))]\n",
        "    words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]\n",
        "    return \" \".join(words)\n",
        "\n",
        "def remove_pos_tags(text: str) -> str:\n",
        "  for tag in TAGS_TO_REMOVE:\n",
        "    text = re.sub(tag, '', text)\n",
        "  return text\n",
        "\n",
        "def remove_wikipedia_tags(text: str) -> str:\n",
        "  return text.split('.\\t')[0]\n",
        "\n",
        "GENERIC_PREPROCESSING_PIPELINE = [\n",
        "                                  remove_wikipedia_tags,\n",
        "                                  remove_pos_tags,\n",
        "                                  lower,\n",
        "                                  replace_special_characters,\n",
        "                                  filter_out_uncommon_symbols,\n",
        "                                  #remove_stopwords,\n",
        "                                  strip_text,\n",
        "                                  lemsent\n",
        "                                  ]\n",
        "\n",
        "EVIDENCES_PREPROCESSING_PIPELINE = GENERIC_PREPROCESSING_PIPELINE\n",
        "EVIDENCES_PREPROCESSING_PIPELINE.insert(0, replace_ids)\n",
        "    \n",
        "\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "    filter_methods = \\\n",
        "        filter_methods if filter_methods is not None else GENERIC_PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKct7tDs0o3W"
      },
      "source": [
        "Now we are ready to pre-process the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ochx9GctfdkX"
      },
      "source": [
        "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Replace each sentence with its pre-processed version\n",
        "    df['Evidence'] = df['Evidence'].apply(\n",
        "        lambda txt: text_prepare(txt, filter_methods=EVIDENCES_PREPROCESSING_PIPELINE)\n",
        "        )\n",
        "    df['Claim'] = df['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_train = preprocess_dataset(df_train)\n",
        "df_val = preprocess_dataset(df_val)\n",
        "df_test = preprocess_dataset(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: comments that we tested this, but actually not used\n",
        "label_count = df_train['Label'].value_counts()\n",
        "pos = label_count['SUPPORTS']\n",
        "neg = label_count['REFUTES']\n",
        "print(pos, neg)\n",
        "\n",
        "initial_bias = np.log([pos/neg])\n",
        "print(initial_bias)"
      ],
      "metadata": {
        "id": "ow1qiisKHAW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nPBGyv8am8-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and vocabularies\n",
        "Here each set is passed through a tokenization process, which also allows to define the vocabulary of each set and also their vocabulary size. Furthermore the maximum length of a token sequence is defined and the labels are extracted from the sets."
      ],
      "metadata": {
        "id": "l7_gqoiCd3tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STARTING_TOKEN = 1 #First value to start the tokenization on (0 is already used as padding value)\n",
        "QUANTILE = 0.99\n",
        "\n",
        "def get_tokenizer(corpus: List[str],\n",
        "                  starting_dict=None)->Dict[str,int]:\n",
        "  '''\n",
        "  Create or expand (given an existing dictionary) a tokenization dictionary\n",
        "  that associates an integer to each word.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  corpus: List[str]\n",
        "    Text to examine searching for new words to add into the dictionary.\n",
        "  starting_dict: Dict[str,int]\n",
        "    An already filled dictionary to further expand (optional).\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    1. A filled dictionary that associates an integer to each word (if starting_dict=None);\n",
        "    2. An expanded dictionary that associates an integer to each new word (if starting_dict is not None)\n",
        "  '''\n",
        "\n",
        "  #Copy the original dictionary to keep it save from updates\n",
        "  words_to_tokens = {} if starting_dict==None else starting_dict.copy()\n",
        "\n",
        "  for text in corpus:\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "      if not word in words_to_tokens:\n",
        "        words_to_tokens[word] = len(words_to_tokens)+STARTING_TOKEN\n",
        "\n",
        "  return words_to_tokens\n",
        "\n",
        "def tokenize(word: str,\n",
        "             words_to_tokens: Dict[str,int])->int:\n",
        "  '''\n",
        "  Get the integer value of a given token.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  word: str\n",
        "    Token\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  int:\n",
        "    Value associated to the token\n",
        "  '''\n",
        "\n",
        "  return words_to_tokens[word]\n",
        "\n",
        "def detokenize(token:int,\n",
        "               words_to_tokens: Dict[str,int])->str:\n",
        "  '''\n",
        "  Get the token-word of a given token-value.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  token: int\n",
        "    Tokenized word\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  str:\n",
        "    Word associated to the token-value\n",
        "  '''\n",
        "\n",
        "  val_list = list(words_to_tokens.values())\n",
        "  key_list = list(words_to_tokens.keys())\n",
        "\n",
        "  position = val_list.index(token)\n",
        "\n",
        "  return key_list[position]\n",
        "\n",
        "def tokenize_string(string: str,\n",
        "                    words_to_tokens: Dict[str,int],\n",
        "                    max_length: int)->List[int]:\n",
        "\n",
        "  '''\n",
        "  Get the tokenized sequence of a string of separated tokens (document/sentence).\n",
        "\n",
        "  Parameters:\n",
        "  string: str\n",
        "    String of separated tokens (document or sentence)\n",
        "  words_to_tokens: Dict[str,int]\n",
        "    Tokenization dictionary\n",
        "  max_length: int\n",
        "    Tokenization length\n",
        "\n",
        "  Returns:\n",
        "    List[int]:\n",
        "      A list of token-values where each one is the tokenized value of a token\n",
        "      int the input-string.\n",
        "      The list is padded if its length is below the max_length.\n",
        "      The list is truncated if its length is above the max_length.\n",
        "  '''\n",
        "\n",
        "  tokens = string.split()\n",
        "  tokenized_sequence = [tokenize(token, words_to_tokens)  for token in tokens]\n",
        "  length_diff = max_length-len(tokenized_sequence)\n",
        "\n",
        "  if length_diff==0: # Return the same sequence if it has the requested size\n",
        "    return tokenized_sequence\n",
        "  elif length_diff<0: # Return the truncated sequence if it exceeds the requested size\n",
        "    return tokenized_sequence[0:max_length]\n",
        "  else: # Return the padded sequence if it has an inferior size than the expected one\n",
        "    return np.pad(tokenized_sequence, (0, length_diff), 'constant').tolist()\n",
        "\n",
        "def label_to_binary(label):\n",
        "  if label==\"SUPPORTS\":\n",
        "    return 1\n",
        "  elif label==\"REFUTES\":\n",
        "    return 0\n",
        "  else:\n",
        "    raise \"Invalid label.\"\n",
        "\n",
        "#Define corpus\n",
        "train_text_claim = df_train[\"Claim\"].tolist()\n",
        "train_text_evidence = df_train[\"Evidence\"].tolist()\n",
        "val_text_claim = df_val[\"Claim\"].tolist()\n",
        "val_text_evidence = df_val[\"Evidence\"].tolist()\n",
        "test_text_claim = df_test[\"Claim\"].tolist()\n",
        "test_text_evidence = df_test[\"Evidence\"].tolist()\n",
        "\n",
        "#Define labels\n",
        "train_labels = df_train[\"Label\"].tolist()\n",
        "val_labels = df_val[\"Label\"].tolist()\n",
        "test_labels = df_test[\"Label\"].tolist()\n",
        "\n",
        "#Token dictionary\n",
        "corpus = train_text_claim+train_text_evidence+val_text_claim+val_text_evidence+test_text_claim+test_text_evidence\n",
        "tokens_dictionary = get_tokenizer(corpus)\n",
        "\n",
        "#Vocabulary\n",
        "tokens_vocabulary = tokens_dictionary.keys()\n",
        "\n",
        "#Vocab size\n",
        "vocabulary_size = len(tokens_vocabulary)+STARTING_TOKEN #+1 to include padding value\n",
        "\n",
        "#Max length of a token sequence\n",
        "n_tokens = [len(doc.split()) for doc in corpus]\n",
        "max_length = int(np.quantile(n_tokens,QUANTILE))\n",
        "\n",
        "#Tokenized sets\n",
        "train_claims_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),train_text_claim)))\n",
        "train_evidences_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),train_text_evidence)))\n",
        "\n",
        "val_claims_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),val_text_claim)))\n",
        "val_evidences_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),val_text_evidence)))\n",
        "\n",
        "test_claims_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),test_text_claim)))\n",
        "test_evidences_tokenized = np.array(list(map(lambda string: tokenize_string(string, tokens_dictionary,max_length),test_text_evidence)))\n",
        "\n",
        "#Tokenized labels\n",
        "train_labels_tokenized = np.array(list(map(label_to_binary,train_labels)))\n",
        "val_labels_tokenized = np.array(list(map(label_to_binary,val_labels)))\n",
        "test_labels_tokenized = np.array(list(map(label_to_binary,test_labels)))"
      ],
      "metadata": {
        "id": "vT8NNqzXCMkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "kfIWefidXvm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Constants and utilities"
      ],
      "metadata": {
        "id": "-HxWGraEYKAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample values\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_SIZE = 64\n",
        "EPOCHS = 50\n",
        "\n",
        "DENSE_UNITS = 64\n",
        "MLP_LAYERS = 2 #One will be a dropout layer\n",
        "DENSE_CLASSIFICATION_LAYERS = 3 #One will be a dropout layer\n",
        "\n",
        "L2_RATE = 0.01\n",
        "DROPOUT_RATE = 0.4\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "TOKEN_EMBEDDING_MODE = \"Simple\" #This must be one between \"Simple\", \"GloVe static\" and \"GloVe dynamic\"\n",
        "SENTENCE_EMBEDDING_MODE = \"RNN mean\" #This must be one between \"RNN last\", \"RNN mean\", \"Bag of vectors\", \"MLP\"\n",
        "RNN_MODEL = \"LSTM\" #This must be one between \"GRU\" and \"LSTM\"\n",
        "MERGE_MODE = \"Concatenate\" #This must be one between \"Concatenate\", \"Sum\" and \"Mean\"\n",
        "APPLY_COSINE_SIMILARITY = True #Simple extension task\n",
        "CLAIM_VERIFICATION_EVALUATION = True #Evaluates using the claim verification evaluation method\n",
        "\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "LOSS = tf.keras.losses.BinaryCrossentropy()\n",
        "METRICS = [tf.keras.metrics.BinaryAccuracy()]\n",
        "\n",
        "# Model common training information\n",
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                patience=10,\n",
        "                                                restore_best_weights=True)]\n",
        "}\n",
        "\n",
        "# Model common prediction information\n",
        "prediction_info = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'verbose': 1\n",
        "}"
      ],
      "metadata": {
        "id": "1w41RoSZYako"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inputs"
      ],
      "metadata": {
        "id": "ESKdNELYZJKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "claims_input = keras.Input(shape=(max_length), name=\"claims\")\n",
        "evidences_input = keras.Input(shape=(max_length), name=\"evidences\")"
      ],
      "metadata": {
        "id": "JaU_BEJyZLOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Token embedding"
      ],
      "metadata": {
        "id": "CFWnFX0AX0Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_embedding_layer(vocab_size,\n",
        "                    embedding_size,\n",
        "                    max_length,\n",
        "                    layer_name,\n",
        "                    pre_trained_weights=None,\n",
        "                    train=True):\n",
        "\n",
        "  if pre_trained_weights is None:\n",
        "    layer = layers.Embedding(\n",
        "        input_dim=vocab_size, \n",
        "        output_dim=embedding_size, \n",
        "        input_length=max_length,\n",
        "        mask_zero=True,\n",
        "        trainable=train,\n",
        "        name=layer_name\n",
        "        )\n",
        "  \n",
        "  else:\n",
        "    layer = layers.Embedding(\n",
        "          input_dim=vocab_size, \n",
        "          output_dim=embedding_size, \n",
        "          input_length=max_length,\n",
        "          weights=[pre_trained_weights],\n",
        "          mask_zero=True,\n",
        "          trainable=train,\n",
        "          name=layer_name\n",
        "          )\n",
        "  \n",
        "  return layer"
      ],
      "metadata": {
        "id": "ae_meKiMX5Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentence embedding"
      ],
      "metadata": {
        "id": "sjCzqXstfH91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, rnn_size, layer_name, mode, rnn_model,dropout,l2):\n",
        "    super(RNN_Sentence_Embedding, self).__init__(name=layer_name)\n",
        "\n",
        "    self.mode = mode\n",
        "\n",
        "    # TODO: TEMP!\n",
        "    dropout = None\n",
        "    l2 = None\n",
        "    \n",
        "    dropout_value = dropout\n",
        "    if dropout is None:\n",
        "      dropout_value = 0\n",
        "\n",
        "    print(f'recurrent_dropout: {dropout_value}')\n",
        "    \n",
        "\n",
        "    rnn_params= {\"units\":rnn_size,\n",
        "                 \"return_sequences\":True,\n",
        "                 \"return_state\":True,\n",
        "                 \"activation\":\"tanh\",\n",
        "                 \"kernel_regularizer\": l2,\n",
        "                 \"dropout\": dropout_value\n",
        "                 }\n",
        "\n",
        "    if rnn_model==\"GRU\":\n",
        "      layer = layers.GRU(**rnn_params)\n",
        "    elif rnn_model==\"LSTM\":\n",
        "      layer = layers.LSTM(**rnn_params)\n",
        "    else:\n",
        "      raise \"Invalid RNN model. Use 'GRU' or 'LSTM'\"\n",
        "\n",
        "    self.rnn = layers.Bidirectional(layer,merge_mode=\"ave\")\n",
        "    print(f'Adding bidirectional rnn with {rnn_size} units')\n",
        "\n",
        "    if self.mode==\"RNN last\":\n",
        "      self.average_layer = layers.Average()\n",
        "      print(f'Adding average layer')\n",
        "    elif self.mode==\"RNN mean\":\n",
        "      self.average_layer = layers.GlobalAveragePooling1D()\n",
        "      print(f'Adding global average pooling layer')\n",
        "    else:\n",
        "      raise \"Invalid Mode. Use 'RNN Last' or 'RNN Mean'\"\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return None\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "\n",
        "    whole_seq_output, forward_h, forward_c, backward_h, backward_c = self.rnn(inputs,mask=mask)\n",
        "\n",
        "    if self.mode==\"RNN last\":\n",
        "      return self.average_layer([forward_h, backward_h])\n",
        "    elif self.mode==\"RNN mean\":\n",
        "      return self.average_layer(whole_seq_output,mask=mask)\n",
        "\n",
        "class Bag_Of_Vectors_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, layer_name):\n",
        "    super(Bag_Of_Vectors_Sentence_Embedding, self).__init__(name=layer_name)\n",
        "\n",
        "    self.average_pooling_layer = layers.GlobalAveragePooling1D()\n",
        "    print(f'Adding global average pooling layer')\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return None\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    #masked_inputs = tf.ragged.boolean_mask(inputs,mask)\n",
        "    #return tf.reduce_mean(masked_inputs, axis=1)\n",
        "    return self.average_pooling_layer(inputs,mask=mask)\n",
        "\n",
        "class MLP_Sentence_Embedding(keras.layers.Layer):\n",
        "  def __init__(self, \n",
        "               max_dense_units,\n",
        "               n_layers, \n",
        "               layer_name, \n",
        "               max_tokens, \n",
        "               embedding_size, \n",
        "               dropout=None,\n",
        "               l2=None):\n",
        "    \n",
        "\n",
        "    super(MLP_Sentence_Embedding,self).__init__(name=layer_name)\n",
        "\n",
        "    self.layer_list = []\n",
        "    \n",
        "    reshape_layer = layers.Reshape((max_tokens*embedding_size,))\n",
        "    self.layer_list.append(reshape_layer)\n",
        "    \n",
        "    for i in range(n_layers):\n",
        "      n_units = self.number_of_units(i, n_layers, max_dense_units, embedding_size)\n",
        "      dense_layer = layers.Dense(\n",
        "          units=n_units,\n",
        "          activation='tanh',\n",
        "          kernel_regularizer=l2\n",
        "          )\n",
        "      self.layer_list.append(dense_layer)\n",
        "      print(f'Adding dense layer with {n_units} units')\n",
        "\n",
        "      if dropout is not None:\n",
        "        dropout_layer = layers.Dropout(dropout)\n",
        "        self.layer_list.append(dropout_layer)\n",
        "        print(f'Adding dropout layer')\n",
        "      \n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return None\n",
        "  \n",
        "  def number_of_units(self, \n",
        "                      layer_number: int, \n",
        "                      max_layer_number: int,\n",
        "                      max_dense_units: int,\n",
        "                      embedding_size: int):\n",
        "    \n",
        "    if layer_number == max_layer_number - 1:\n",
        "      n = embedding_size\n",
        "    else:\n",
        "      n = max_dense_units / 2**layer_number\n",
        "      if(n < embedding_size):\n",
        "        n = embedding_size\n",
        "    \n",
        "    return n\n",
        "\n",
        "  def call(self, x, mask=None):\n",
        "\n",
        "    for layer in self.layer_list:\n",
        "      x = layer(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def sentence_embedding_layer(mode, \n",
        "                             rnn_model, \n",
        "                             dense_units, \n",
        "                             mlp_layers, \n",
        "                             layer_name, \n",
        "                             max_tokens, \n",
        "                             embedding_size, \n",
        "                             dropout, \n",
        "                             l2):\n",
        "  print('Sentence Embedding layers creation started')\n",
        "\n",
        "  if mode==\"RNN last\" or mode==\"RNN mean\":\n",
        "    layer = RNN_Sentence_Embedding(embedding_size,layer_name,mode,rnn_model,dropout,l2)\n",
        "  elif mode==\"Bag of vectors\":\n",
        "    layer = Bag_Of_Vectors_Sentence_Embedding(layer_name)\n",
        "  elif mode==\"MLP\":\n",
        "    layer = MLP_Sentence_Embedding(dense_units, mlp_layers, layer_name, max_tokens, embedding_size, dropout, l2)\n",
        "  else:\n",
        "    raise Exception(\"Invalid Mode.\")\n",
        "  \n",
        "  print('Sentence Embedding layers creation completed')\n",
        "\n",
        "  return layer"
      ],
      "metadata": {
        "id": "UYD2ZsiPfJn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merge inputs"
      ],
      "metadata": {
        "id": "_lOhQ2Jd1G_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate(layer_name):\n",
        "\n",
        "  return layers.Concatenate(name=layer_name)\n",
        "\n",
        "def sum(layer_name):\n",
        "\n",
        "  return layers.Add(name=layer_name)\n",
        "\n",
        "def mean(layer_name):\n",
        "\n",
        "  return layers.Average(name=layer_name)\n",
        "\n",
        "def merge_layer(merge_mode, layer_name):\n",
        "  if merge_mode==\"Concatenate\":\n",
        "    return concatenate(layer_name)\n",
        "  elif merge_mode==\"Sum\":\n",
        "    return sum(layer_name)\n",
        "  elif merge_mode==\"Mean\":\n",
        "    return mean(layer_name)\n",
        "  else:\n",
        "    raise Exception(\"Invalid merge mode.\")"
      ],
      "metadata": {
        "id": "wY3cQUZW1KlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification"
      ],
      "metadata": {
        "id": "g3zTpOcK4Qdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_classification_layer(dense_units, activation_function, layer_name, last, l2=None):\n",
        "\n",
        "  if last:\n",
        "    layer= layers.Dense(1,\n",
        "                        activation=\"sigmoid\",\n",
        "                        name=layer_name)\n",
        "    \n",
        "  else:\n",
        "    layer= layers.Dense(units=dense_units,\n",
        "                        activation=activation_function,\n",
        "                        kernel_regularizer=l2,\n",
        "                        name=layer_name)\n",
        "  \n",
        "  return layer"
      ],
      "metadata": {
        "id": "rJd89dcZ4UNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build model"
      ],
      "metadata": {
        "id": "bHGeioKS5soW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building layers"
      ],
      "metadata": {
        "id": "Bxd8XBImfE8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_layers(vocab_size,\n",
        "                 embedding_size,\n",
        "                 max_tokens,\n",
        "                 token_embedding_mode,\n",
        "                 sentence_embedding_mode,\n",
        "                 dense_units,\n",
        "                 mlp_layers,\n",
        "                 dense_classification_layers,\n",
        "                 rnn_model,\n",
        "                 dropout=None,\n",
        "                 l2=None):\n",
        "\n",
        "  layer_embedded_tokens = token_embedding_layer(vocab_size,\n",
        "                                                embedding_size,\n",
        "                                                max_tokens,\n",
        "                                                \"token_embedding\")\n",
        "\n",
        "  layer_embedded_sentences = sentence_embedding_layer(sentence_embedding_mode,\n",
        "                                                      rnn_model,\n",
        "                                                      dense_units,\n",
        "                                                      mlp_layers,\n",
        "                                                      \"sentences_embedding\",\n",
        "                                                      max_tokens,\n",
        "                                                      embedding_size,\n",
        "                                                      dropout,\n",
        "                                                      l2)\n",
        "\n",
        "  layer_merge = merge_layer(MERGE_MODE,\"merge\")\n",
        "\n",
        "  classification_layers = []\n",
        "  \n",
        "  for i in range(dense_classification_layers):\n",
        "    units = dense_units/2**i\n",
        "    is_last_layer = False\n",
        "    layer_name = \"intermediate_classification_\"+str(i+1)\n",
        "\n",
        "    if i == (dense_classification_layers - 1):\n",
        "      is_last_layer = True\n",
        "      layer_name = \"final_classification\"\n",
        "    \n",
        "    classification_layers.append(\n",
        "        dense_classification_layer(units,\n",
        "                                   \"tanh\",\n",
        "                                   layer_name,\n",
        "                                   is_last_layer,\n",
        "                                   l2)\n",
        "        )\n",
        "    \n",
        "    if dropout is not None and not is_last_layer:\n",
        "      classification_layers.append(layers.Dropout(dropout))\n",
        "  \n",
        "  return (layer_embedded_tokens, layer_embedded_sentences, layer_merge, classification_layers)"
      ],
      "metadata": {
        "id": "FHLWacnIfI-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###End-to-end model"
      ],
      "metadata": {
        "id": "QuBjVYc7fMcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(built_layers,\n",
        "                claims_input,\n",
        "                evidences_input,\n",
        "                dense_classification_layers):\n",
        "\n",
        "  layer_embedded_tokens, layer_embedded_sentences, layer_merge, classification_layers = built_layers\n",
        "\n",
        "  claims_tokens_embedded = layer_embedded_tokens(claims_input)\n",
        "  evidences_tokens_embedded = layer_embedded_tokens(evidences_input)\n",
        "\n",
        "  claims_sentences_embedded = layer_embedded_sentences(claims_tokens_embedded)\n",
        "  evidences_sentences_embedded = layer_embedded_sentences(evidences_tokens_embedded)\n",
        "\n",
        "  classification_input = layer_merge([claims_sentences_embedded,evidences_sentences_embedded])\n",
        "\n",
        "  #Cosine similarity extension step:\n",
        "  if APPLY_COSINE_SIMILARITY:\n",
        "    layer_cosine_similarity = layers.Dot(axes=(1), normalize=True,name=\"cosine_similarity\") #Normalize=True will compute the cosine similarity (see documentation)\n",
        "    layer_concatenation = layers.Concatenate(name=\"cosine_similarity_concat\")\n",
        "    cosine_similarity = layer_cosine_similarity([claims_sentences_embedded,evidences_sentences_embedded])\n",
        "    classification_output = layer_concatenation([classification_input,cosine_similarity])\n",
        "  \n",
        "  \n",
        "\n",
        "  for layer in classification_layers:\n",
        "    classification_output = layer(classification_output)\n",
        "\n",
        "  # Instantiate an end-to-end model\n",
        "  model = keras.Model(\n",
        "      inputs=[claims_input, evidences_input],\n",
        "      outputs=[classification_output]\n",
        "  )\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "BEoLxSGbfTJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Build and compile model"
      ],
      "metadata": {
        "id": "C4wjuecBfWo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_and_compile(vocab_size,\n",
        "                      embedding_size,\n",
        "                      max_tokens,\n",
        "                      token_embedding_mode,\n",
        "                      sentence_embedding_mode,\n",
        "                      dense_units,\n",
        "                      mlp_layers,\n",
        "                      rnn_model,\n",
        "                      claims_input,\n",
        "                      evidences_input,\n",
        "                      dense_classification_layers,\n",
        "                      optimizer,\n",
        "                      loss,\n",
        "                      metrics,\n",
        "                      dropout=None,\n",
        "                      l2=None):\n",
        "\n",
        "  built_layers = build_layers(vocab_size,\n",
        "                              embedding_size,\n",
        "                              max_tokens,\n",
        "                              token_embedding_mode,\n",
        "                              sentence_embedding_mode,\n",
        "                              dense_units,\n",
        "                              mlp_layers,\n",
        "                              dense_classification_layers,\n",
        "                              rnn_model,\n",
        "                              dropout,\n",
        "                              l2)\n",
        "  \n",
        "  model = build_model(built_layers,\n",
        "                      claims_input,\n",
        "                      evidences_input,\n",
        "                      dense_classification_layers)\n",
        "\n",
        "  model_compile_info = {\n",
        "      'optimizer': optimizer,\n",
        "      'loss': loss,\n",
        "      'metrics': metrics,\n",
        "  }\n",
        "\n",
        "  model.compile(**model_compile_info)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "CAuCaRkP5viv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_and_compile(vocabulary_size,\n",
        "                          EMBEDDING_SIZE,\n",
        "                          max_length,\n",
        "                          TOKEN_EMBEDDING_MODE,\n",
        "                          SENTENCE_EMBEDDING_MODE,\n",
        "                          DENSE_UNITS,\n",
        "                          MLP_LAYERS,\n",
        "                          RNN_MODEL,\n",
        "                          claims_input,\n",
        "                          evidences_input,\n",
        "                          DENSE_CLASSIFICATION_LAYERS,\n",
        "                          OPTIMIZER,\n",
        "                          LOSS,\n",
        "                          METRICS,\n",
        "                          DROPOUT_RATE,#None, DROPOUT_RATE\n",
        "                          keras.regularizers.l2(L2_RATE))#None, keras.regularizers.l2(L2_RATE)\n",
        "\n",
        "#Show architecture\n",
        "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True, expand_nested=True)"
      ],
      "metadata": {
        "id": "bvOaYxwyp5Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train model"
      ],
      "metadata": {
        "id": "vmCzR930eFVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_history(history: keras.callbacks.History):\n",
        "    \"\"\"\n",
        "    Shows training history data stored by the History Keras callback\n",
        "\n",
        "    :param history: History Keras callback\n",
        "    \"\"\"\n",
        "    print(history.history)\n",
        "    history_data = history.history\n",
        "    print(\"Displaying the following history keys: \", history_data.keys())\n",
        "\n",
        "    for key, value in history_data.items():\n",
        "        if not key.startswith('val'):\n",
        "            fig, ax = plt.subplots(1, 1)\n",
        "            ax.set_title(key)\n",
        "            ax.plot(value)\n",
        "            if 'val_{}'.format(key) in history_data:\n",
        "                ax.plot(history_data['val_{}'.format(key)])\n",
        "            else:\n",
        "                print(\"Couldn't find validation values for metric: \", key)\n",
        "\n",
        "            ax.set_ylabel(key)\n",
        "            ax.set_xlabel('epoch')\n",
        "            ax.legend(['train', 'val'], loc='best')\n",
        "\n",
        "    val_accuracies = history_data['val_binary_accuracy']\n",
        "    best_val_epoch = np.argmax(val_accuracies)\n",
        "    best_val_acc = val_accuracies[best_val_epoch]\n",
        "\n",
        "    print(f'Best validation accuracy: {best_val_acc} obtained at epoch: {best_val_epoch}')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_model(model: keras.Model,\n",
        "                claims_train: np.ndarray,\n",
        "                evidences_train: np.ndarray,\n",
        "                labels_train: np.ndarray,\n",
        "                claims_val: np.ndarray,\n",
        "                evidences_val: np.ndarray,\n",
        "                labels_val: np.ndarray,\n",
        "                training_info: dict) -> keras.Model:\n",
        "\n",
        "    print(\"Start training! \\nParameters: {}\".format(training_info))\n",
        "\n",
        "    history = model.fit(x={\"claims\":claims_train,\"evidences\":evidences_train},\n",
        "                        y=labels_train,\n",
        "                        validation_data=({\"claims\":claims_val,\"evidences\":evidences_val},labels_val),\n",
        "                        **training_info)\n",
        "    \n",
        "    print(\"Training completed! Showing history...\")\n",
        "\n",
        "    show_history(history)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "uk7WucTbeG8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model,\n",
        "            train_claims_tokenized,\n",
        "            train_evidences_tokenized,\n",
        "            train_labels_tokenized,\n",
        "            val_claims_tokenized,\n",
        "            val_evidences_tokenized,\n",
        "            val_labels_tokenized,\n",
        "            training_info)"
      ],
      "metadata": {
        "id": "lOB8mLxtkF-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test model"
      ],
      "metadata": {
        "id": "yRycTGdMgErq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def consecutive_claims(tokenized_claims_list):\n",
        "\n",
        "  n_claims = tokenized_claims_list.shape[0]\n",
        "\n",
        "  consecutive_claims_list = []\n",
        "\n",
        "  current_claim = tokenized_claims_list[0]\n",
        "  current_sum = 1\n",
        "\n",
        "  for i in range(1,n_claims):\n",
        "    if np.array_equal(current_claim, tokenized_claims_list[i]):\n",
        "      current_sum+=1\n",
        "    else:\n",
        "      consecutive_claims_list.append(current_sum)\n",
        "\n",
        "      current_claim = tokenized_claims_list[i]\n",
        "      current_sum=1\n",
        "  consecutive_claims_list.append(current_sum)\n",
        "\n",
        "  return consecutive_claims_list\n",
        "\n",
        "def majority_vote(votes):\n",
        "  zeros = (votes==0).sum()\n",
        "  ones = (votes==1).sum()\n",
        "\n",
        "  return 0 if zeros>=ones else 1\n",
        "\n",
        "def check_labels_consistency(labels):\n",
        "  return np.max(labels) == np.min(labels)\n",
        "\n",
        "def claim_verification_selection(claims_test, labels_test, test_predictions):\n",
        "\n",
        "  consecutive_claims_list = consecutive_claims(claims_test)\n",
        "\n",
        "  reduced_predictions = np.zeros(len(consecutive_claims_list))\n",
        "  reduced_labels = np.zeros(len(consecutive_claims_list))\n",
        "\n",
        "  start=0\n",
        "  for index, n_consecutives in enumerate(consecutive_claims_list):\n",
        "\n",
        "    labels = labels_test[start:start+n_consecutives]\n",
        "    if check_labels_consistency(labels):\n",
        "      reduced_labels[index] = labels[0]\n",
        "    else:\n",
        "      print(\"Inconsistency found!\")\n",
        "      for j in range(start,start+n_consecutives):\n",
        "        print(\"Claim {}: {}\".format(j-start+1,test_text_claim[j]))\n",
        "        print(\"Evidence {}: {}\".format(j-start+1,test_text_evidence[j]))\n",
        "        print(\"Label {}: {}\".format(j-start+1,test_labels[j]))\n",
        "        print(\"\\n\")\n",
        "      \n",
        "      start+=n_consecutives\n",
        "      continue\n",
        "\n",
        "    votes = test_predictions[start:start+n_consecutives]\n",
        "    vote = majority_vote(votes)\n",
        "    reduced_predictions[index]=vote\n",
        "\n",
        "    start+=n_consecutives\n",
        "\n",
        "  return reduced_predictions, reduced_labels\n",
        "\n",
        "def round_to_label(prediction, threshold=0.65):\n",
        "  if prediction<threshold:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "def predict_data(model: keras.Model,\n",
        "                 claims_test: np.ndarray,\n",
        "                 evidences_test: np.ndarray,\n",
        "                 prediction_info: dict) -> np.ndarray:\n",
        "\n",
        "    print('Starting prediction: \\n{}'.format(prediction_info))\n",
        "\n",
        "    predictions = model.predict(x={\"claims\":claims_test,\"evidences\":evidences_test}, **prediction_info)\n",
        "    return predictions\n",
        "\n",
        "def evaluate_predictions(predictions: np.ndarray,\n",
        "                         y: np.ndarray,\n",
        "                         metrics: List[Callable],\n",
        "                         metric_names: List[str]):\n",
        "    \n",
        "    assert len(metrics) == len(metric_names)\n",
        "    \n",
        "    print(f'Prediction evaluation started...')\n",
        "\n",
        "    metric_info = {}\n",
        "    for metric, metric_name in zip(metrics, metric_names):\n",
        "        \n",
        "        metric_value = metric(y_pred=predictions, y_true=y)\n",
        "        metric_info[metric_name] = metric_value\n",
        "\n",
        "    return metric_info\n",
        "\n",
        "def test_model(model: keras.Model,\n",
        "               claims_test: np.array,\n",
        "               evidences_test: np.array,\n",
        "               labels_test: np.array,\n",
        "               prediction_info: dict,\n",
        "               claim_verification_evaluation: bool):\n",
        "        \n",
        "    predictions = predict_data(model,\n",
        "                               claims_test,\n",
        "                               evidences_test,\n",
        "                               prediction_info)\n",
        "\n",
        "    #Reshape predictions from (Batch,1) to (Batch)\n",
        "    predictions = np.reshape(predictions,(predictions.size))\n",
        "\n",
        "    #Round each probability into a label (0 or 1)\n",
        "    test_predictions = np.array(list(map(round_to_label,predictions)))\n",
        "\n",
        "    # Evaluation\n",
        "    metrics = [\n",
        "        partial(f1_score, average='macro'),\n",
        "        partial(f1_score, average='micro'),\n",
        "        partial(f1_score, average='weighted')\n",
        "    ]\n",
        "\n",
        "    metric_names = [\n",
        "        \"macro_f1\",\n",
        "        \"micro_f1\",\n",
        "        \"weighted_f1\"\n",
        "    ]\n",
        "\n",
        "    if claim_verification_evaluation:\n",
        "      reduced_predictions, reduced_labels = claim_verification_selection(claims_test, labels_test,test_predictions)\n",
        "      metric_info = evaluate_predictions(predictions=reduced_predictions,\n",
        "                                        y=reduced_labels,\n",
        "                                        metrics=metrics,\n",
        "                                        metric_names=metric_names)\n",
        "\n",
        "    else:\n",
        "      metric_info = evaluate_predictions(predictions=test_predictions,\n",
        "                                        y=labels_test,\n",
        "                                        metrics=metrics,\n",
        "                                        metric_names=metric_names)\n",
        "\n",
        "    return metric_info, test_predictions"
      ],
      "metadata": {
        "id": "Shu8BCOMgHJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_info, y_pred = test_model(model,\n",
        "                                 test_claims_tokenized,\n",
        "                                 test_evidences_tokenized,\n",
        "                                 test_labels_tokenized,\n",
        "                                 prediction_info,\n",
        "                                 CLAIM_VERIFICATION_EVALUATION)"
      ],
      "metadata": {
        "id": "qOeTt-f0r6Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate model"
      ],
      "metadata": {
        "id": "UbI0uX-VvSW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_f1_scores(metric_info):\n",
        "    \"\"\"\n",
        "    Method for creating a list of labels that will be used for testing.\n",
        "    \n",
        "    Parameters\n",
        "    -------\n",
        "    metric_info : dict\n",
        "        Dictionary that contains the f1 scores\n",
        "    \n",
        "    \"\"\"\n",
        "    print()\n",
        "    print('F1 SCORES:')\n",
        "    print(f'  macro: {metric_info[\"macro_f1\"]}')\n",
        "    print(f'  micro: {metric_info[\"micro_f1\"]}')\n",
        "    print(f'  weighted: {metric_info[\"weighted_f1\"]}')\n",
        "    print()\n",
        "\n",
        "def show_classification_report(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Method that prints the classification report\n",
        "    \n",
        "    Parameters\n",
        "    -------\n",
        "    y_true : np.array\n",
        "        Array of true labels\n",
        "    y_pred : np.array\n",
        "        Array of predicted labels\n",
        "    \"\"\"\n",
        "\n",
        "    print(classification_report(\n",
        "        y_true, \n",
        "        y_pred,\n",
        "        ))\n",
        "\n",
        "def show_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Method that shows the confusion matrix.\n",
        "    \n",
        "    Parameters\n",
        "    -------\n",
        "    y_true : np.array\n",
        "        Array of true labels\n",
        "    y_pred : np.array\n",
        "        Array of predicted labels\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(5,5))\n",
        "    ConfusionMatrixDisplay.from_predictions(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        normalize='true', \n",
        "        cmap=plt.cm.Blues, \n",
        "        values_format=\".2f\",\n",
        "        xticks_rotation='vertical',\n",
        "        ax=ax)"
      ],
      "metadata": {
        "id": "EudZ04CsvUzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_f1_scores(metric_info)\n",
        "show_classification_report(test_labels_tokenized, y_pred)\n",
        "show_confusion_matrix(test_labels_tokenized, y_pred)"
      ],
      "metadata": {
        "id": "s_hbKdb_v31i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion and Error Analysis\n",
        "In this part there is a small summary of the tests we have done, and the explanation of some choices that we made.\n",
        "\n",
        "Finally, there are some considerations about the models that we selected for the final testing, and a comparison with the results obtained on the validation set.\n",
        "\n",
        "* how we pre-processed the data and why\n",
        "* observe that the train set is unbalanced and contains some inconsistencies\n",
        "* model creation and parameter tuning\n",
        "* results discussion\n",
        "* possible improvements\n"
      ],
      "metadata": {
        "id": "Q5zHCzQXBklE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing\n",
        "We noticed that the original datasets were relatively dirty, and they required some cleaning before using them as inputs of the models.\n",
        "\n",
        "Firstly, we made some standard operations, e.g., removed unwanted punctuation, lower cased everything, removed special characters, etc....\n",
        "Secondly, we applied specific actions for datasets cluttering removal. In particular:\n",
        "* we noticed that each **evidence** was starting with a number (most likely the ID of the evidence) and a tabulation character. We removed all of these occurences in all the dataset evidences. Example:\n",
        "\n",
        " **Original Sentence**\n",
        "~~~\n",
        "14\tThe series finale aired August 28 , 2013 .\n",
        "~~~\n",
        " **Processed Sentence**\n",
        "~~~\n",
        "The series finale aired August 28 , 2013 .\n",
        "~~~\n",
        "* almost all the evidences terminate with a series of tags that are not really relevant for the classification task, and we decided to remove them. Example:\n",
        " **Original Sentence**\n",
        "~~~\n",
        "5\tIt stars Ray Winstone in his first role in a costume drama .\tRay Winstone\tRay Winstone\n",
        "~~~\n",
        " **Processed Sentence**\n",
        "~~~\n",
        "It stars Ray Winstone in his first role in a costume drama .\n",
        "~~~\n",
        "\n",
        "At the beginning, we also removed **stopwords** from the dataset.\n",
        "After a lot of testing, we discovered that this was causing 2 major problems:\n",
        "* **inconsistences**: after the pre-processing, some claims had the **same description**, but **different labels**. \n",
        "For example, in the original dataset we have the following claims:\n",
        "~~~\n",
        "SZA's music combines elements from different genres. (SUPPORT)\n",
        "SZA's music does not combine elements from different genres. (REFUTES)\n",
        "~~~\n",
        "After the pre-processing, which was also consisting in removing stopwords, the results were the same, but with different labels:\n",
        "~~~\n",
        "szas music combine element different genre (SUPPORT)\n",
        "szas music combine element different genre (REFUTES)\n",
        "~~~\n",
        "* **duplicates**: for the same reasons explained above, some rows were duplicated (e.g., same claim and same evidence).\n",
        "\n",
        "Therefore, we eventually decided to discard the stopwords removal, and we remarked an important difference in the validation accuracy during the training phase (~+5-10%).\n"
      ],
      "metadata": {
        "id": "p13f93lhCfYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9BkWxVYyFXoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}